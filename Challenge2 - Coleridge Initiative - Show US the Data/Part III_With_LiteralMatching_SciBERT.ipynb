{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.036557,
     "end_time": "2021-06-09T09:38:25.360645",
     "exception": false,
     "start_time": "2021-06-09T09:38:25.324088",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Part III (Testing)\n",
    "\n",
    "# 1. Introduction\n",
    "\n",
    "This notebook contains the prediction phase for the Kaggle challenge \"Coleridge Initative: Show US the Data\" (https://www.kaggle.com/c/coleridgeinitiative-show-us-the-data/)\n",
    "\n",
    "To recap, this challenge is about datasets used in scientific papers. In particular, we want to extract the datasets for scientific paper, with several NLP approaches. In this notebook, we test both BERT and SciBERT. The first model is introduced by Devlin, J., Chang, M. W., Lee, K., and Toutanova, K., in 2018 [1]. Source code of BERT can be fuond [here](https://github.com/google-research/bert). The second model is  introduced by Beltagy, I., Lo, K., and Cohan, A. in 2019 [2]. Source code of SciBERT can be found [here](https://github.com/allenai/scibert).\n",
    "\n",
    "\n",
    "Furthermore, we append the existing data with a specialized Corpus for dataset tagging. TDMSci is a Corpus existing of annotated data for tasks, metrices and datasets. Here, B-DATASET and I-DATASET are the NER-labels indicating a word is (part of) a dataset [3]. Source code (and annotated data) of TDMSci can be found [here](https://github.com/IBM/science-result-extractor).\n",
    "\n",
    "This boils down to exectuing Named Entity Recognition (NER), in particular token classfication.\n",
    "\n",
    "We have created three notebooks, one for **dataset creation** ([Part I]()), one for **training** ([Part IIa]() and [Part IIb]()) and this one for **testing** (Part III). This part exists of the testing phase of the model. We loaded the pre-trained model from either [part IIa()] or [part IIb()]  and executed predictions with the pre-trained model on our validation set.\n",
    "\n",
    "For part I (creating dataset), we refer to [this notebook](https://www.kaggle.com/lunaelise/fork-of-mlip-group25-scibert-dataset)\n",
    "For part II (training), we refer to [this notebook](https://www.kaggle.com/lunaelise/fork-of-mlip-group25-scibert-training).\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "[1] Devlin, J., Chang, M. W., Lee, K., & Toutanova, K. (2018). Bert: Pre-training of deep bidirectional transformers for language understanding. arXiv preprint arXiv:1810.04805.\n",
    "[2] Beltagy, I., Lo, K., & Cohan, A. (2019). SciBERT: A pretrained language model for scientific text. arXiv preprint arXiv:1903.10676.  \n",
    "[3] Hou, Y., Jochim, C., Gleize, M., Bonin, F., & Ganguly, D. (2021). TDMSci: A Specialized Corpus for Scientific Literature Entity Tagging of Tasks Datasets and Metrics. arXiv preprint arXiv:2101.10273.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.033301,
     "end_time": "2021-06-09T09:38:25.428591",
     "exception": false,
     "start_time": "2021-06-09T09:38:25.395290",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# 2. Preparing Notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-06-09T09:38:25.507415Z",
     "iopub.status.busy": "2021-06-09T09:38:25.506870Z",
     "iopub.status.idle": "2021-06-09T09:40:17.656566Z",
     "shell.execute_reply": "2021-06-09T09:40:17.655931Z",
     "shell.execute_reply.started": "2021-06-09T09:34:24.897249Z"
    },
    "papermill": {
     "duration": 112.195183,
     "end_time": "2021-06-09T09:40:17.656713",
     "exception": false,
     "start_time": "2021-06-09T09:38:25.461530",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in links: file:///kaggle/input/coleridge-packages/packages/datasets\r\n",
      "Processing /kaggle/input/coleridge-packages/packages/datasets/datasets-1.5.0-py3-none-any.whl\r\n",
      "Requirement already satisfied: dill in /opt/conda/lib/python3.7/site-packages (from datasets) (0.3.3)\r\n",
      "Requirement already satisfied: pyarrow>=0.17.1 in /opt/conda/lib/python3.7/site-packages (from datasets) (1.0.1)\r\n",
      "Requirement already satisfied: numpy>=1.17 in /opt/conda/lib/python3.7/site-packages (from datasets) (1.19.5)\r\n",
      "Requirement already satisfied: multiprocess in /opt/conda/lib/python3.7/site-packages (from datasets) (0.70.11.1)\r\n",
      "Processing /kaggle/input/coleridge-packages/packages/datasets/huggingface_hub-0.0.7-py3-none-any.whl\r\n",
      "Requirement already satisfied: requests>=2.19.0 in /opt/conda/lib/python3.7/site-packages (from datasets) (2.25.1)\r\n",
      "Requirement already satisfied: importlib-metadata in /opt/conda/lib/python3.7/site-packages (from datasets) (3.4.0)\r\n",
      "Requirement already satisfied: fsspec in /opt/conda/lib/python3.7/site-packages (from datasets) (0.8.5)\r\n",
      "Processing /kaggle/input/coleridge-packages/packages/datasets/tqdm-4.49.0-py2.py3-none-any.whl\r\n",
      "Requirement already satisfied: pandas in /opt/conda/lib/python3.7/site-packages (from datasets) (1.1.5)\r\n",
      "Processing /kaggle/input/coleridge-packages/packages/datasets/xxhash-2.0.0-cp37-cp37m-manylinux2010_x86_64.whl\r\n",
      "Requirement already satisfied: filelock in /opt/conda/lib/python3.7/site-packages (from huggingface-hub<0.1.0->datasets) (3.0.12)\r\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /opt/conda/lib/python3.7/site-packages (from requests>=2.19.0->datasets) (1.26.3)\r\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.7/site-packages (from requests>=2.19.0->datasets) (2020.12.5)\r\n",
      "Requirement already satisfied: idna<3,>=2.5 in /opt/conda/lib/python3.7/site-packages (from requests>=2.19.0->datasets) (2.10)\r\n",
      "Requirement already satisfied: chardet<5,>=3.0.2 in /opt/conda/lib/python3.7/site-packages (from requests>=2.19.0->datasets) (3.0.4)\r\n",
      "Requirement already satisfied: zipp>=0.5 in /opt/conda/lib/python3.7/site-packages (from importlib-metadata->datasets) (3.4.0)\r\n",
      "Requirement already satisfied: typing-extensions>=3.6.4 in /opt/conda/lib/python3.7/site-packages (from importlib-metadata->datasets) (3.7.4.3)\r\n",
      "Requirement already satisfied: python-dateutil>=2.7.3 in /opt/conda/lib/python3.7/site-packages (from pandas->datasets) (2.8.1)\r\n",
      "Requirement already satisfied: pytz>=2017.2 in /opt/conda/lib/python3.7/site-packages (from pandas->datasets) (2021.1)\r\n",
      "Requirement already satisfied: six>=1.5 in /opt/conda/lib/python3.7/site-packages (from python-dateutil>=2.7.3->pandas->datasets) (1.15.0)\r\n",
      "Installing collected packages: tqdm, xxhash, huggingface-hub, datasets\r\n",
      "  Attempting uninstall: tqdm\r\n",
      "    Found existing installation: tqdm 4.56.2\r\n",
      "    Uninstalling tqdm-4.56.2:\r\n",
      "      Successfully uninstalled tqdm-4.56.2\r\n",
      "Successfully installed datasets-1.5.0 huggingface-hub-0.0.7 tqdm-4.49.0 xxhash-2.0.0\r\n",
      "Processing /kaggle/input/coleridge-packages/seqeval-1.2.2-py3-none-any.whl\r\n",
      "Requirement already satisfied: numpy>=1.14.0 in /opt/conda/lib/python3.7/site-packages (from seqeval==1.2.2) (1.19.5)\r\n",
      "Requirement already satisfied: scikit-learn>=0.21.3 in /opt/conda/lib/python3.7/site-packages (from seqeval==1.2.2) (0.24.1)\r\n",
      "Requirement already satisfied: joblib>=0.11 in /opt/conda/lib/python3.7/site-packages (from scikit-learn>=0.21.3->seqeval==1.2.2) (1.0.1)\r\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in /opt/conda/lib/python3.7/site-packages (from scikit-learn>=0.21.3->seqeval==1.2.2) (2.1.0)\r\n",
      "Requirement already satisfied: scipy>=0.19.1 in /opt/conda/lib/python3.7/site-packages (from scikit-learn>=0.21.3->seqeval==1.2.2) (1.5.4)\r\n",
      "Installing collected packages: seqeval\r\n",
      "Successfully installed seqeval-1.2.2\r\n",
      "Processing /kaggle/input/coleridge-packages/tokenizers-0.10.1-cp37-cp37m-manylinux1_x86_64.whl\r\n",
      "tokenizers is already installed with the same version as the provided wheel. Use --force-reinstall to force an installation of the wheel.\r\n",
      "Processing /kaggle/input/coleridge-packages/transformers-4.5.0.dev0-py3-none-any.whl\r\n",
      "Requirement already satisfied: tokenizers<0.11,>=0.10.1 in /opt/conda/lib/python3.7/site-packages (from transformers==4.5.0.dev0) (0.10.1)\r\n",
      "Requirement already satisfied: packaging in /opt/conda/lib/python3.7/site-packages (from transformers==4.5.0.dev0) (20.9)\r\n",
      "Requirement already satisfied: numpy>=1.17 in /opt/conda/lib/python3.7/site-packages (from transformers==4.5.0.dev0) (1.19.5)\r\n",
      "Requirement already satisfied: regex!=2019.12.17 in /opt/conda/lib/python3.7/site-packages (from transformers==4.5.0.dev0) (2020.11.13)\r\n",
      "Requirement already satisfied: tqdm>=4.27 in /opt/conda/lib/python3.7/site-packages (from transformers==4.5.0.dev0) (4.49.0)\r\n",
      "Requirement already satisfied: importlib-metadata in /opt/conda/lib/python3.7/site-packages (from transformers==4.5.0.dev0) (3.4.0)\r\n",
      "Requirement already satisfied: requests in /opt/conda/lib/python3.7/site-packages (from transformers==4.5.0.dev0) (2.25.1)\r\n",
      "Requirement already satisfied: filelock in /opt/conda/lib/python3.7/site-packages (from transformers==4.5.0.dev0) (3.0.12)\r\n",
      "Requirement already satisfied: sacremoses in /opt/conda/lib/python3.7/site-packages (from transformers==4.5.0.dev0) (0.0.43)\r\n",
      "Requirement already satisfied: zipp>=0.5 in /opt/conda/lib/python3.7/site-packages (from importlib-metadata->transformers==4.5.0.dev0) (3.4.0)\r\n",
      "Requirement already satisfied: typing-extensions>=3.6.4 in /opt/conda/lib/python3.7/site-packages (from importlib-metadata->transformers==4.5.0.dev0) (3.7.4.3)\r\n",
      "Requirement already satisfied: pyparsing>=2.0.2 in /opt/conda/lib/python3.7/site-packages (from packaging->transformers==4.5.0.dev0) (2.4.7)\r\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /opt/conda/lib/python3.7/site-packages (from requests->transformers==4.5.0.dev0) (1.26.3)\r\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.7/site-packages (from requests->transformers==4.5.0.dev0) (2020.12.5)\r\n",
      "Requirement already satisfied: idna<3,>=2.5 in /opt/conda/lib/python3.7/site-packages (from requests->transformers==4.5.0.dev0) (2.10)\r\n",
      "Requirement already satisfied: chardet<5,>=3.0.2 in /opt/conda/lib/python3.7/site-packages (from requests->transformers==4.5.0.dev0) (3.0.4)\r\n",
      "Requirement already satisfied: click in /opt/conda/lib/python3.7/site-packages (from sacremoses->transformers==4.5.0.dev0) (7.1.2)\r\n",
      "Requirement already satisfied: joblib in /opt/conda/lib/python3.7/site-packages (from sacremoses->transformers==4.5.0.dev0) (1.0.1)\r\n",
      "Requirement already satisfied: six in /opt/conda/lib/python3.7/site-packages (from sacremoses->transformers==4.5.0.dev0) (1.15.0)\r\n",
      "Installing collected packages: transformers\r\n",
      "  Attempting uninstall: transformers\r\n",
      "    Found existing installation: transformers 4.4.2\r\n",
      "    Uninstalling transformers-4.4.2:\r\n",
      "      Successfully uninstalled transformers-4.4.2\r\n",
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\r\n",
      "allennlp 2.2.0 requires transformers<4.5,>=4.1, but you have transformers 4.5.0.dev0 which is incompatible.\u001b[0m\r\n",
      "Successfully installed transformers-4.5.0.dev0\r\n",
      "Requirement already satisfied: datasets in /opt/conda/lib/python3.7/site-packages (1.5.0)\r\n",
      "Requirement already satisfied: pyarrow>=0.17.1 in /opt/conda/lib/python3.7/site-packages (from datasets) (1.0.1)\r\n",
      "Requirement already satisfied: fsspec in /opt/conda/lib/python3.7/site-packages (from datasets) (0.8.5)\r\n",
      "Requirement already satisfied: dill in /opt/conda/lib/python3.7/site-packages (from datasets) (0.3.3)\r\n",
      "Requirement already satisfied: importlib-metadata in /opt/conda/lib/python3.7/site-packages (from datasets) (3.4.0)\r\n",
      "Requirement already satisfied: pandas in /opt/conda/lib/python3.7/site-packages (from datasets) (1.1.5)\r\n",
      "Requirement already satisfied: xxhash in /opt/conda/lib/python3.7/site-packages (from datasets) (2.0.0)\r\n",
      "Requirement already satisfied: huggingface-hub<0.1.0 in /opt/conda/lib/python3.7/site-packages (from datasets) (0.0.7)\r\n",
      "Requirement already satisfied: multiprocess in /opt/conda/lib/python3.7/site-packages (from datasets) (0.70.11.1)\r\n",
      "Requirement already satisfied: tqdm<4.50.0,>=4.27 in /opt/conda/lib/python3.7/site-packages (from datasets) (4.49.0)\r\n",
      "Requirement already satisfied: numpy>=1.17 in /opt/conda/lib/python3.7/site-packages (from datasets) (1.19.5)\r\n",
      "Requirement already satisfied: requests>=2.19.0 in /opt/conda/lib/python3.7/site-packages (from datasets) (2.25.1)\r\n",
      "Requirement already satisfied: filelock in /opt/conda/lib/python3.7/site-packages (from huggingface-hub<0.1.0->datasets) (3.0.12)\r\n",
      "Requirement already satisfied: chardet<5,>=3.0.2 in /opt/conda/lib/python3.7/site-packages (from requests>=2.19.0->datasets) (3.0.4)\r\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.7/site-packages (from requests>=2.19.0->datasets) (2020.12.5)\r\n",
      "Requirement already satisfied: idna<3,>=2.5 in /opt/conda/lib/python3.7/site-packages (from requests>=2.19.0->datasets) (2.10)\r\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /opt/conda/lib/python3.7/site-packages (from requests>=2.19.0->datasets) (1.26.3)\r\n",
      "Requirement already satisfied: typing-extensions>=3.6.4 in /opt/conda/lib/python3.7/site-packages (from importlib-metadata->datasets) (3.7.4.3)\r\n",
      "Requirement already satisfied: zipp>=0.5 in /opt/conda/lib/python3.7/site-packages (from importlib-metadata->datasets) (3.4.0)\r\n",
      "Requirement already satisfied: python-dateutil>=2.7.3 in /opt/conda/lib/python3.7/site-packages (from pandas->datasets) (2.8.1)\r\n",
      "Requirement already satisfied: pytz>=2017.2 in /opt/conda/lib/python3.7/site-packages (from pandas->datasets) (2021.1)\r\n",
      "Requirement already satisfied: six>=1.5 in /opt/conda/lib/python3.7/site-packages (from python-dateutil>=2.7.3->pandas->datasets) (1.15.0)\r\n"
     ]
    }
   ],
   "source": [
    "!pip install datasets --no-index --find-links=file:///kaggle/input/coleridge-packages/packages/datasets \n",
    "!pip install ../input/coleridge-packages/seqeval-1.2.2-py3-none-any.whl \n",
    "!pip install ../input/coleridge-packages/tokenizers-0.10.1-cp37-cp37m-manylinux1_x86_64.whl \n",
    "!pip install ../input/coleridge-packages/transformers-4.5.0.dev0-py3-none-any.whl \n",
    "!pip install datasets "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-06-09T09:40:17.740946Z",
     "iopub.status.busy": "2021-06-09T09:40:17.740176Z",
     "iopub.status.idle": "2021-06-09T09:40:28.390599Z",
     "shell.execute_reply": "2021-06-09T09:40:28.391347Z",
     "shell.execute_reply.started": "2021-06-09T09:36:17.159932Z"
    },
    "papermill": {
     "duration": 10.695229,
     "end_time": "2021-06-09T09:40:28.391498",
     "exception": false,
     "start_time": "2021-06-09T09:40:17.696269",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.7/site-packages/torchaudio/backend/utils.py:54: UserWarning: \"sox\" backend is being deprecated. The default backend will be changed to \"sox_io\" backend in 0.8.0 and \"sox\" backend will be removed in 0.9.0. Please migrate to \"sox_io\" backend. Please refer to https://github.com/pytorch/audio/issues/903 for the detail.\n",
      "  '\"sox\" backend is being deprecated. '\n"
     ]
    }
   ],
   "source": [
    "#Import necessary libraries\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "import nltk\n",
    "import re\n",
    "import os\n",
    "from os import listdir\n",
    "from os.path import isfile, join\n",
    "import re\n",
    "import json\n",
    "import time\n",
    "import datetime\n",
    "import random\n",
    "import glob\n",
    "import importlib\n",
    "import allennlp\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from transformers import *\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import torch\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-06-09T09:40:28.474680Z",
     "iopub.status.busy": "2021-06-09T09:40:28.474199Z",
     "iopub.status.idle": "2021-06-09T09:40:30.324318Z",
     "shell.execute_reply": "2021-06-09T09:40:30.325327Z",
     "shell.execute_reply.started": "2021-06-09T09:36:27.373476Z"
    },
    "papermill": {
     "duration": 1.894463,
     "end_time": "2021-06-09T09:40:30.325530",
     "exception": false,
     "start_time": "2021-06-09T09:40:28.431067",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 309999 rows in the training set!\n"
     ]
    }
   ],
   "source": [
    "acc = 0\n",
    "with open('/kaggle/input/tdmsci/train_ner.json') as f:\n",
    "    for row in f:\n",
    "        acc += 1\n",
    "\n",
    "print(\"There are {} rows in the training set!\".format(acc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-06-09T09:40:30.461959Z",
     "iopub.status.busy": "2021-06-09T09:40:30.461104Z",
     "iopub.status.idle": "2021-06-09T09:40:30.463292Z",
     "shell.execute_reply": "2021-06-09T09:40:30.462647Z",
     "shell.execute_reply.started": "2021-06-09T09:36:29.689129Z"
    },
    "papermill": {
     "duration": 0.073202,
     "end_time": "2021-06-09T09:40:30.463440",
     "exception": false,
     "start_time": "2021-06-09T09:40:30.390238",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "#!cp /kaggle/input/coleridge-packages/my_seqeval.py ./\n",
    "import os\n",
    "\n",
    "#Make directory in kaggle output of the model\n",
    "# path_pretrained_SciBERT = '/kaggle/input/tdmsci/output/output' #'/kaggle/working/output'\n",
    "# os.makedirs(path_pretrained_SciBERT, exist_ok=True)\n",
    "\n",
    "# os.environ[\"ModelOutputPath\"] = f\"{path_pretrained_SciBERT}\"\n",
    "# #Refer to pretrained SciBERT model\n",
    "# #path_SciBERT_model = '/kaggle/input/tdmsci/output/output' \n",
    "# for root, dirs, files in os.walk(\"/kaggle/input/mlip-group25-scibert-training\", topdown=False):\n",
    "#     for filename in files:\n",
    "#         print(os.path.join(root, filename))\n",
    "#         name = os.path.join(root, filename)\n",
    "#         os.environ[\"filename\"] = f\"{name}\"\n",
    "#         !cp \"$filename\" \"$ModelOutputPath\"\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-06-09T09:40:30.602252Z",
     "iopub.status.busy": "2021-06-09T09:40:30.599633Z",
     "iopub.status.idle": "2021-06-09T09:40:30.611262Z",
     "shell.execute_reply": "2021-06-09T09:40:30.612380Z",
     "shell.execute_reply.started": "2021-06-09T09:36:29.699204Z"
    },
    "papermill": {
     "duration": 0.084921,
     "end_time": "2021-06-09T09:40:30.612587",
     "exception": false,
     "start_time": "2021-06-09T09:40:30.527666",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "model_name = 'bert-base-cased' #'allenai/scibert_scivocab_cased'\n",
    "#model_name =  'bert-base-cased'\n",
    "#Initialize paths for data\n",
    "path_abs = '/kaggle/input/coleridgeinitiative-show-us-the-data/'\n",
    "path_test = os.path.join(path_abs, 'test/')\n",
    "path_train = os.path.join(path_abs, 'train/')\n",
    "path_metadata = os.path.join(path_abs, 'train.csv')\n",
    "path_ner_json = '/kaggle/input/tdmsci/train_ner.json'\n",
    "sample_submission_path = '../input/coleridgeinitiative-show-us-the-data/sample_submission.csv'\n",
    "sample_submission = pd.read_csv(sample_submission_path)\n",
    "# path_sample_submission = os.path.join(path_abs, 'sample_submission.csv')\n",
    "# path_abs_tdmsci = '/kaggle/input/tdmsci/'\n",
    "\n",
    "adnl_govt_labels_path = '../input/coleridge-additional-gov-datasets-22000popular/data_set_800_with10000popular.csv'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.063795,
     "end_time": "2021-06-09T09:40:30.740811",
     "exception": false,
     "start_time": "2021-06-09T09:40:30.677016",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# 3. Get to know the data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.054365,
     "end_time": "2021-06-09T09:40:30.859283",
     "exception": false,
     "start_time": "2021-06-09T09:40:30.804918",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## 3.1. Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-06-09T09:40:30.950472Z",
     "iopub.status.busy": "2021-06-09T09:40:30.949455Z",
     "iopub.status.idle": "2021-06-09T09:40:30.961807Z",
     "shell.execute_reply": "2021-06-09T09:40:30.962276Z",
     "shell.execute_reply.started": "2021-06-09T09:36:29.717119Z"
    },
    "papermill": {
     "duration": 0.062548,
     "end_time": "2021-06-09T09:40:30.962425",
     "exception": false,
     "start_time": "2021-06-09T09:40:30.899877",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# #Load metadata\n",
    "# label_df = pd.read_csv(path_train_metadata)\n",
    "# label_df = label_df.groupby('Id').agg({\n",
    "#     'pub_title': 'first',\n",
    "#     'dataset_title': '|'.join,\n",
    "#     'dataset_label': '|'.join,\n",
    "#     'cleaned_label': '|'.join\n",
    "# }).reset_index()\n",
    "\n",
    "# sample_submission = pd.read_csv(path_sample_submission)\n",
    "# #Inpsect data\n",
    "# label_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "papermill": {
     "duration": 0.071259,
     "end_time": "2021-06-09T09:40:31.109459",
     "exception": false,
     "start_time": "2021-06-09T09:40:31.038200",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.063457,
     "end_time": "2021-06-09T09:40:31.238085",
     "exception": false,
     "start_time": "2021-06-09T09:40:31.174628",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# 4. Create testdata (and basic NLP)\n",
    "\n",
    "Here, we will load our provided test data and apply basic NLP methods on this. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-06-09T09:40:31.376549Z",
     "iopub.status.busy": "2021-06-09T09:40:31.375670Z",
     "iopub.status.idle": "2021-06-09T09:40:31.381184Z",
     "shell.execute_reply": "2021-06-09T09:40:31.380160Z",
     "shell.execute_reply.started": "2021-06-09T09:36:29.725547Z"
    },
    "papermill": {
     "duration": 0.079396,
     "end_time": "2021-06-09T09:40:31.381341",
     "exception": false,
     "start_time": "2021-06-09T09:40:31.301945",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "MAX_LENGTH = 64\n",
    "OVERLAP = 20\n",
    "\n",
    "def clean_paper_sentence(s):\n",
    "    \"\"\"\n",
    "    This function is essentially clean_text without lowercasing.\n",
    "    \"\"\"\n",
    "    s = re.sub('[^A-Za-z0-9]+', ' ', str(s)).strip()\n",
    "    s = re.sub(' +', ' ', s)\n",
    "    return s\n",
    "\n",
    "def shorten_sentences(sentences):\n",
    "    \"\"\"\n",
    "    Sentences that have more than MAX_LENGTH words will be split\n",
    "    into multiple sentences with overlappings.\n",
    "    \"\"\"\n",
    "    short_sentences = []\n",
    "    for sentence in sentences:\n",
    "        words = sentence.split()\n",
    "        if len(words) > MAX_LENGTH:\n",
    "            for p in range(0, len(words), MAX_LENGTH - OVERLAP):\n",
    "                short_sentences.append(' '.join(words[p:p+MAX_LENGTH]))\n",
    "        else:\n",
    "            short_sentences.append(sentence)\n",
    "    return short_sentences\n",
    "\n",
    "#Concatenate text and split sentences, as the BERT (and SciBERT) model \n",
    "#has the contraint of maximum sequence length of 512.\n",
    "# def concatenate_text(json_dict):\n",
    "#     total_text = \"\"\n",
    "        \n",
    "#     for section_dict in json_dict:\n",
    "#         total_text += section_dict['text']+ '\\n'\n",
    "#     #sentences = nltk.tokenize.sent_tokenize(total_text) # This seems to take a lot of time?\n",
    "#     sentences = re.split('\\. ', total_text)\n",
    "#     sentences = [clean_paper_sentence(s) for s in sentences]\n",
    "   \n",
    "#     sentences = shorten_sentences(sentences)\n",
    "#     return sentences\n",
    "\n",
    "# def create_test_df(path, dataset, N_test):\n",
    "#     '''\n",
    "    \n",
    "#     '''\n",
    "#     #Initialize dictionary with right keys\n",
    "#     final_dict = dict()\n",
    "#     final_dict[\"Id\"] = []\n",
    "#     final_dict[\"Sentences\"] = []\n",
    "    \n",
    "#     max_length = N_test\n",
    "    \n",
    "#     counter = 0\n",
    "#     for root, _, files in os.walk(path):\n",
    "#         for filename in range(0,max_length):#files:\n",
    "#             id = files[filename][:-5] #Remove .json from filename to retrieve id\n",
    "#             with open(path + files[filename]) as f:\n",
    "#                 json_dict = json.load(f)\n",
    "#                 sentences = concatenate_text(json_dict)\n",
    "#                 final_dict[\"Id\"].append(id)\n",
    "#                 final_dict[\"Sentences\"].append(sentences)\n",
    "                \n",
    "#             counter += 1\n",
    "#     df = pd.DataFrame.from_dict(final_dict)\n",
    "\n",
    "#     return df\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-06-09T09:40:31.500999Z",
     "iopub.status.busy": "2021-06-09T09:40:31.499201Z",
     "iopub.status.idle": "2021-06-09T09:40:31.501589Z",
     "shell.execute_reply": "2021-06-09T09:40:31.502003Z",
     "shell.execute_reply.started": "2021-06-09T09:36:29.737006Z"
    },
    "papermill": {
     "duration": 0.056526,
     "end_time": "2021-06-09T09:40:31.502155",
     "exception": false,
     "start_time": "2021-06-09T09:40:31.445629",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# N_test = len([name for name in os.listdir(path_test)])\n",
    "# print(N_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-06-09T09:40:31.585413Z",
     "iopub.status.busy": "2021-06-09T09:40:31.584880Z",
     "iopub.status.idle": "2021-06-09T09:40:31.588825Z",
     "shell.execute_reply": "2021-06-09T09:40:31.588405Z",
     "shell.execute_reply.started": "2021-06-09T09:36:29.747240Z"
    },
    "papermill": {
     "duration": 0.047146,
     "end_time": "2021-06-09T09:40:31.588939",
     "exception": false,
     "start_time": "2021-06-09T09:40:31.541793",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "#Create dataframe with labels\n",
    "#label_df = label_df[['Id','cleaned_label']]\n",
    "\n",
    "#Create train test data for NER classification\n",
    "#test_df_ner = create_test_df(path_test, \"test\", N_test)\n",
    "\n",
    "#create test data for MLM classification\n",
    "#test_df_mlm = create_train_test_df(path_test, \"test\",0, N_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.039153,
     "end_time": "2021-06-09T09:40:31.667419",
     "exception": false,
     "start_time": "2021-06-09T09:40:31.628266",
     "status": "completed"
    },
    "tags": []
   },
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-06-09T09:40:31.750113Z",
     "iopub.status.busy": "2021-06-09T09:40:31.749564Z",
     "iopub.status.idle": "2021-06-09T09:40:31.753266Z",
     "shell.execute_reply": "2021-06-09T09:40:31.753635Z",
     "shell.execute_reply.started": "2021-06-09T09:36:29.757691Z"
    },
    "papermill": {
     "duration": 0.047106,
     "end_time": "2021-06-09T09:40:31.753793",
     "exception": false,
     "start_time": "2021-06-09T09:40:31.706687",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "#test_df_ner.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "papermill": {
     "duration": 0.039333,
     "end_time": "2021-06-09T09:40:31.832413",
     "exception": false,
     "start_time": "2021-06-09T09:40:31.793080",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.039074,
     "end_time": "2021-06-09T09:40:31.910910",
     "exception": false,
     "start_time": "2021-06-09T09:40:31.871836",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "#2. Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-06-09T09:40:31.996219Z",
     "iopub.status.busy": "2021-06-09T09:40:31.995635Z",
     "iopub.status.idle": "2021-06-09T09:41:27.321823Z",
     "shell.execute_reply": "2021-06-09T09:41:27.322241Z",
     "shell.execute_reply.started": "2021-06-09T09:36:29.766122Z"
    },
    "papermill": {
     "duration": 55.371762,
     "end_time": "2021-06-09T09:41:27.322389",
     "exception": false,
     "start_time": "2021-06-09T09:40:31.950627",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "14316\n"
     ]
    }
   ],
   "source": [
    "#Literal matching (create knowledge bank)\n",
    "train = pd.read_csv(path_metadata)\n",
    "papers = {}\n",
    "\n",
    "for paper_id in train['Id'].unique():\n",
    "    with open(f'{path_train}/{paper_id}.json', 'r') as f:\n",
    "        paper = json.load(f)\n",
    "        papers[paper_id] = paper\n",
    "\n",
    "print(len(list(papers.keys())))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-06-09T09:41:27.408739Z",
     "iopub.status.busy": "2021-06-09T09:41:27.408229Z",
     "iopub.status.idle": "2021-06-09T09:41:27.431295Z",
     "shell.execute_reply": "2021-06-09T09:41:27.430633Z",
     "shell.execute_reply.started": "2021-06-09T09:37:26.746568Z"
    },
    "papermill": {
     "duration": 0.069704,
     "end_time": "2021-06-09T09:41:27.431423",
     "exception": false,
     "start_time": "2021-06-09T09:41:27.361719",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "sample_submission_path = '../input/coleridgeinitiative-show-us-the-data/sample_submission.csv'\n",
    "sample_submission = pd.read_csv(sample_submission_path)\n",
    "paper_test_folder = '../input/coleridgeinitiative-show-us-the-data/test'\n",
    "for paper_id in sample_submission['Id']:\n",
    "    with open(f'{paper_test_folder}/{paper_id}.json', 'r') as f:\n",
    "        paper = json.load(f)\n",
    "        papers[paper_id] = paper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-06-09T09:41:27.522469Z",
     "iopub.status.busy": "2021-06-09T09:41:27.521611Z",
     "iopub.status.idle": "2021-06-09T09:41:27.740906Z",
     "shell.execute_reply": "2021-06-09T09:41:27.740452Z",
     "shell.execute_reply.started": "2021-06-09T09:37:58.124371Z"
    },
    "papermill": {
     "duration": 0.269669,
     "end_time": "2021-06-09T09:41:27.741042",
     "exception": false,
     "start_time": "2021-06-09T09:41:27.471373",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No. different labels: 12230\n"
     ]
    }
   ],
   "source": [
    "all_labels = set()\n",
    "\n",
    "for label_1, label_2, label_3 in train[['dataset_title', 'dataset_label', 'cleaned_label']].itertuples(index=False):\n",
    "    all_labels.add(str(label_1).lower())\n",
    "    all_labels.add(str(label_2).lower())\n",
    "    all_labels.add(str(label_3).lower())\n",
    "\n",
    "adnl_govt_labels = pd.read_csv(adnl_govt_labels_path)\n",
    "\n",
    "for l in adnl_govt_labels.title:\n",
    "    \n",
    "    if (len(l.split()) > 3):\n",
    "        all_labels.add(l.lower())\n",
    "        all_labels.add(l)\n",
    "        all_labels.add(clean_paper_sentence(l))\n",
    "    \n",
    "    \n",
    "all_labels = set(all_labels)\n",
    "print(f'No. different labels: {len(all_labels)}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.039697,
     "end_time": "2021-06-09T09:41:27.820771",
     "exception": false,
     "start_time": "2021-06-09T09:41:27.781074",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# 5. NER with SciBERT\n",
    "\n",
    "We first apply NER in combination with SciBERT. For this purpose, we need the pretrained [AutoModelForTokenClassification](https://huggingface.co/transformers/model_doc/auto.html#automodelfortokenclassification) from SciBERT, which we already loaded at the beginning of this notebook.\n",
    "\n",
    "We first take a look at our training data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.039529,
     "end_time": "2021-06-09T09:41:27.899792",
     "exception": false,
     "start_time": "2021-06-09T09:41:27.860263",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "We see that the training data for now exists of an Id (of a paper) with a list of corresponding sentences. Now we need to preprocess this, in order to apply NER. As we are only interest in datasets, we annotated the data just as is done by the authors introducing TDMSci [1]. We make use of the same annotation format as is used in the [CoNLL-2003 dataset](https://www.clips.uantwerpen.be/conll2003/ner/).  \n",
    "\n",
    "This means we can have two types of NER-tags: _B-types_ and _I-types_. The first type indicates the beginning of a named entity (or beginning of a phrase). The second type indicates other words in a named entity (or a phrase). When there is no entity (or word that is not part of a phrase), this word is labelled with 'O'. For this task, we use three different NER-task: **B-DATASET**, **I-DATASET** and **O**.  \n",
    "\n",
    "This boils down to labelling every word not being (part of) a dataset as **O**, whilst datasets are being tagged with either **B-DATASET**, or **I-DATASET**.\n",
    "\n",
    "[1] Hou, Y., Jochim, C., Gleize, M., Bonin, F., & Ganguly, D. (2021). TDMSci: A Specialized Corpus for Scientific Literature Entity Tagging of Tasks Datasets and Metrics. arXiv preprint arXiv:2101.10273."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-06-09T09:41:27.992449Z",
     "iopub.status.busy": "2021-06-09T09:41:27.991774Z",
     "iopub.status.idle": "2021-06-09T09:41:27.995287Z",
     "shell.execute_reply": "2021-06-09T09:41:27.994857Z",
     "shell.execute_reply.started": "2021-06-06T13:25:12.711318Z"
    },
    "papermill": {
     "duration": 0.051559,
     "end_time": "2021-06-09T09:41:27.995394",
     "exception": false,
     "start_time": "2021-06-09T09:41:27.943835",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def clean_text(txt):\n",
    "    return re.sub('[^A-Za-z0-9]+', ' ', str(txt).lower()).strip()\n",
    "\n",
    "def totally_clean_text(txt):\n",
    "    txt = clean_text(txt)\n",
    "    txt = re.sub(' +', ' ', txt)\n",
    "    return txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.039932,
     "end_time": "2021-06-09T09:41:28.075490",
     "exception": false,
     "start_time": "2021-06-09T09:41:28.035558",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## 5.1. Test SciBERT model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-06-09T09:41:28.196132Z",
     "iopub.status.busy": "2021-06-09T09:41:28.185956Z",
     "iopub.status.idle": "2021-06-09T09:41:31.255875Z",
     "shell.execute_reply": "2021-06-09T09:41:31.255404Z",
     "shell.execute_reply.started": "2021-06-06T13:33:31.258934Z"
    },
    "papermill": {
     "duration": 3.139525,
     "end_time": "2021-06-09T09:41:31.256019",
     "exception": false,
     "start_time": "2021-06-09T09:41:28.116494",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "literal_preds = []\n",
    "\n",
    "for paper_id in sample_submission['Id']:\n",
    "    paper = papers[paper_id]\n",
    "    text_1 = '. '.join(section['text'] for section in paper).lower()\n",
    "    text_2 = totally_clean_text(text_1)\n",
    "    \n",
    "    labels = set()\n",
    "    for label in all_labels:\n",
    "        if label in text_1 or label in text_2:\n",
    "            labels.add(clean_text(label))\n",
    "    \n",
    "    literal_preds.append('|'.join(labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-06-09T09:41:31.343207Z",
     "iopub.status.busy": "2021-06-09T09:41:31.342508Z",
     "iopub.status.idle": "2021-06-09T09:41:31.346212Z",
     "shell.execute_reply": "2021-06-09T09:41:31.346603Z",
     "shell.execute_reply.started": "2021-06-06T13:25:16.273309Z"
    },
    "papermill": {
     "duration": 0.050802,
     "end_time": "2021-06-09T09:41:31.346729",
     "exception": false,
     "start_time": "2021-06-09T09:41:31.295927",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['alzheimer s disease neuroimaging initiative adni|adni',\n",
       " 'integrated postsecondary education data system|common core of data|schools and staffing survey|nces common core of data|trends in international mathematics and science study|progress in international reading literacy study',\n",
       " 'sea lake and overland surges from hurricanes|noaa storm surge inundation|slosh model',\n",
       " 'rural urban continuum codes']"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "literal_preds[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-06-09T09:41:31.435812Z",
     "iopub.status.busy": "2021-06-09T09:41:31.435047Z",
     "iopub.status.idle": "2021-06-09T09:41:31.785222Z",
     "shell.execute_reply": "2021-06-09T09:41:31.785626Z",
     "shell.execute_reply.started": "2021-06-06T13:25:16.285303Z"
    },
    "papermill": {
     "duration": 0.399123,
     "end_time": "2021-06-09T09:41:31.785789",
     "exception": false,
     "start_time": "2021-06-09T09:41:31.386666",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No. grouped training rows: 14316\n"
     ]
    }
   ],
   "source": [
    "train = train.groupby('Id').agg({\n",
    "    'pub_title': 'first',\n",
    "    'dataset_title': '|'.join,\n",
    "    'dataset_label': '|'.join,\n",
    "    'cleaned_label': '|'.join\n",
    "}).reset_index()\n",
    "\n",
    "print(f'No. grouped training rows: {len(train)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-06-09T09:41:31.872291Z",
     "iopub.status.busy": "2021-06-09T09:41:31.871479Z",
     "iopub.status.idle": "2021-06-09T09:41:31.874065Z",
     "shell.execute_reply": "2021-06-09T09:41:31.874464Z",
     "shell.execute_reply.started": "2021-06-06T13:25:16.66181Z"
    },
    "papermill": {
     "duration": 0.048441,
     "end_time": "2021-06-09T09:41:31.874596",
     "exception": false,
     "start_time": "2021-06-09T09:41:31.826155",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def clean_training_text(txt):\n",
    "    \"\"\"\n",
    "    similar to the default clean_text function but without lowercasing.\n",
    "    \"\"\"\n",
    "    return re.sub('[^A-Za-z0-9]+', ' ', str(txt)).strip()\n",
    "\n",
    "def shorten_sentences(sentences):\n",
    "    short_sentences = []\n",
    "    for sentence in sentences:\n",
    "        words = sentence.split()\n",
    "        if len(words) > MAX_LENGTH:\n",
    "            for p in range(0, len(words), MAX_LENGTH - OVERLAP):\n",
    "                short_sentences.append(' '.join(words[p:p+MAX_LENGTH]))\n",
    "        else:\n",
    "            short_sentences.append(sentence)\n",
    "    return short_sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-06-09T09:41:31.963362Z",
     "iopub.status.busy": "2021-06-09T09:41:31.962627Z",
     "iopub.status.idle": "2021-06-09T09:41:31.965520Z",
     "shell.execute_reply": "2021-06-09T09:41:31.965123Z",
     "shell.execute_reply.started": "2021-06-06T13:41:13.681549Z"
    },
    "papermill": {
     "duration": 0.05059,
     "end_time": "2021-06-09T09:41:31.965625",
     "exception": false,
     "start_time": "2021-06-09T09:41:31.915035",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def preprocess_test_ner():\n",
    "    test_rows = [] # test data in NER format\n",
    "    paper_lengths = []\n",
    "\n",
    "    for paper_id in sample_submission['Id']:\n",
    "       \n",
    "        paper = papers[paper_id]\n",
    "        \n",
    "        sentences = [clean_training_text(sentence) for section in paper \n",
    "                 for sentence in section['text'].split('.')\n",
    "                ]\n",
    "        #The author of this code does this, but I am not sure if this is necessary\n",
    "        sentences = [sentence for sentence in sentences if len(sentence) > 10] # only accept sentences with length > 10 chars\n",
    "        sentences = [sentence for sentence in sentences if any(word in sentence.lower() for word in ['data', 'study'])]\n",
    "        \n",
    "        #Add NER-labels to each token of each sentence\n",
    "        for sentence in sentences:\n",
    "            \n",
    "            tokens = sentence.split()\n",
    "            dummy_tags = ['O'] * len(tokens)\n",
    "            \n",
    "            test_rows.append({'tokens' : tokens, 'tags' : dummy_tags, 'id': paper_id})\n",
    "        paper_lengths.append(len(sentences))\n",
    "    \n",
    "    #For testing purposes\n",
    "#     first_100_papers = sorted(os.listdir(path_train))[0:100]\n",
    "#     for papername in first_100_papers:\n",
    "    \n",
    "#         with open(f'{path_train}{papername}', 'r') as f:\n",
    "#             paper = json.load(f)\n",
    "#             sentences = [clean_training_text(sentence) for section in paper \n",
    "#                      for sentence in section['text'].split('.')\n",
    "#                     ]\n",
    "#             #The author of this code does this, but I am not sure if this is necessary\n",
    "#             sentences = [sentence for sentence in sentences if len(sentence) > 10] # only accept sentences with length > 10 chars\n",
    "#             sentences = [sentence for sentence in sentences if any(word in sentence.lower() for word in ['data', 'study'])]\n",
    "\n",
    "#             #Add NER-labels to each token of each sentence\n",
    "#             for sentence in sentences:\n",
    "\n",
    "#                 tokens = sentence.split()\n",
    "#                 dummy_tags = ['O'] * len(tokens)\n",
    "\n",
    "#                 test_rows.append({'tokens' : tokens, 'tags' : dummy_tags, 'id': paper_id})\n",
    "#             paper_lengths.append(len(sentences))\n",
    "  \n",
    "\n",
    "    return test_rows, paper_lengths\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.040154,
     "end_time": "2021-06-09T09:41:32.045941",
     "exception": false,
     "start_time": "2021-06-09T09:41:32.005787",
     "status": "completed"
    },
    "tags": []
   },
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-06-09T09:41:32.164927Z",
     "iopub.status.busy": "2021-06-09T09:41:32.161237Z",
     "iopub.status.idle": "2021-06-09T09:41:32.168112Z",
     "shell.execute_reply": "2021-06-09T09:41:32.167669Z",
     "shell.execute_reply.started": "2021-06-06T13:41:15.501484Z"
    },
    "papermill": {
     "duration": 0.0807,
     "end_time": "2021-06-09T09:41:32.168227",
     "exception": false,
     "start_time": "2021-06-09T09:41:32.087527",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "365\n",
      "[{'tokens': ['A', 'recent', 'large', 'genomewide', 'association', 'study', 'GWAS', 'reported', 'a', 'genome', 'wide', 'significant', 'locus', 'for', 'years', 'of', 'education', 'which', 'subsequently', 'demonstrated', 'association', 'to', 'general', 'cognitive', 'ability', 'g', 'in', 'overlapping', 'cohorts'], 'tags': ['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O'], 'id': '2100032a-7c33-4bff-97ef-690822c43466'}, {'tokens': ['The', 'current', 'study', 'was', 'designed', 'to', 'test', 'whether', 'GWAS', 'hits', 'for', 'educational', 'attainment', 'are', 'involved', 'in', 'general', 'cognitive', 'ability', 'in', 'an', 'independent', 'large', 'scale', 'collection', 'of', 'cohorts'], 'tags': ['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O'], 'id': '2100032a-7c33-4bff-97ef-690822c43466'}, {'tokens': ['We', 'next', 'conducted', 'meta', 'analyses', 'with', '24', '189', 'individuals', 'with', 'neurocognitive', 'data', 'from', 'the', 'educational', 'attainment', 'studies', 'and', 'then', 'with', '53', '188', 'largely', 'independent', 'individuals', 'from', 'a', 'recent', 'GWAS', 'of', 'cognition'], 'tags': ['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O'], 'id': '2100032a-7c33-4bff-97ef-690822c43466'}, {'tokens': ['These', 'results', 'provide', 'independent', 'replication', 'in', 'a', 'large', 'scale', 'dataset', 'of', 'a', 'genetic', 'locus', 'associated', 'with', 'cognitive', 'function', 'and', 'education'], 'tags': ['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O'], 'id': '2100032a-7c33-4bff-97ef-690822c43466'}, {'tokens': ['The', 'decision', 'to', 'study', 'g', 'in', 'COGENT', 'stemmed', 'from', 'longstanding', 'evidence', 'that', 'a', 'g', 'factor', 'can', 'be', 'derived', 'consistently', 'captures', 'almost', 'half', 'the', 'variance', 'in', 'overall', 'test', 'performance', 'and', 'is', 'relatively', 'invariant', 'to', 'the', 'neurocognitive', 'test', 'battery', 'used', 'and', 'specific', 'abilities', 'assessed'], 'tags': ['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O'], 'id': '2100032a-7c33-4bff-97ef-690822c43466'}]\n"
     ]
    }
   ],
   "source": [
    "test_rows, paper_lengths = preprocess_test_ner()\n",
    "print(len(test_rows))\n",
    "print(test_rows[0:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "papermill": {
     "duration": 0.040126,
     "end_time": "2021-06-09T09:41:32.248637",
     "exception": false,
     "start_time": "2021-06-09T09:41:32.208511",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-06-09T09:41:32.335050Z",
     "iopub.status.busy": "2021-06-09T09:41:32.334401Z",
     "iopub.status.idle": "2021-06-09T09:41:32.337831Z",
     "shell.execute_reply": "2021-06-09T09:41:32.338387Z",
     "shell.execute_reply.started": "2021-06-06T13:41:20.382149Z"
    },
    "papermill": {
     "duration": 0.049706,
     "end_time": "2021-06-09T09:41:32.338519",
     "exception": false,
     "start_time": "2021-06-09T09:41:32.288813",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "max_length = 64 # max no. words for each sentence.\n",
    "overlap = 20 # if a sentence exceeds MAX_LENGTH, we split it to multiple sentences with overlapping\n",
    "path_train_nerjson = '/kaggle/input/tdmsci/train_ner.json'\n",
    "pred_save_path = './pred'\n",
    "prediction_file = 'test_predictions.txt'\n",
    "test_input_save_path = './input_data'\n",
    "path_pretrained_scibert = '/kaggle/input/tdmsci/results/output' #'/kaggle/working/output'\n",
    "train_file = path_train_nerjson\n",
    "filename_test = 'test_ner_input.json'\n",
    "\n",
    "os.environ[\"MODEL_PATH\"] = f\"{path_pretrained_scibert}\"\n",
    "os.environ[\"TRAIN_FILE\"] = f\"{train_file}\"\n",
    "os.environ[\"VALIDATION_FILE\"] = f\"{train_file}\"\n",
    "os.environ[\"TEST_FILE\"] = f\"{test_input_save_path}/{filename_test}\"\n",
    "os.environ[\"OUTPUT_DIR\"] = f\"{pred_save_path}\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-06-09T09:41:32.441856Z",
     "iopub.status.busy": "2021-06-09T09:41:32.423919Z",
     "iopub.status.idle": "2021-06-09T09:41:33.090735Z",
     "shell.execute_reply": "2021-06-09T09:41:33.090271Z",
     "shell.execute_reply.started": "2021-06-06T13:41:20.517738Z"
    },
    "papermill": {
     "duration": 0.711614,
     "end_time": "2021-06-09T09:41:33.090885",
     "exception": false,
     "start_time": "2021-06-09T09:41:32.379271",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# copy my_seqeval.py to the working directory because the input directory is non-writable\n",
    "!cp /kaggle/input/coleridge-packages/my_seqeval.py ./\n",
    "os.makedirs(test_input_save_path, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-06-09T09:41:33.179450Z",
     "iopub.status.busy": "2021-06-09T09:41:33.178581Z",
     "iopub.status.idle": "2021-06-09T09:41:33.181457Z",
     "shell.execute_reply": "2021-06-09T09:41:33.181051Z",
     "shell.execute_reply.started": "2021-06-06T13:41:21.200605Z"
    },
    "papermill": {
     "duration": 0.049262,
     "end_time": "2021-06-09T09:41:33.181570",
     "exception": false,
     "start_time": "2021-06-09T09:41:33.132308",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def predict_scibert_ner():\n",
    "    !python ../input/kaggle-ner-utils/kaggle_run_ner.py \\\n",
    "    --model_name_or_path \"$MODEL_PATH\" \\\n",
    "    --train_file \"$TRAIN_FILE\" \\\n",
    "    --validation_file \"$VALIDATION_FILE\" \\\n",
    "    --test_file \"$TEST_FILE\" \\\n",
    "    --output_dir \"$OUTPUT_DIR\" \\\n",
    "    --report_to 'none' \\\n",
    "    --seed 123 \\\n",
    "    --do_predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-06-09T09:41:33.265916Z",
     "iopub.status.busy": "2021-06-09T09:41:33.265263Z",
     "iopub.status.idle": "2021-06-09T09:41:33.267817Z",
     "shell.execute_reply": "2021-06-09T09:41:33.268238Z",
     "shell.execute_reply.started": "2021-06-06T13:41:21.212578Z"
    },
    "papermill": {
     "duration": 0.046096,
     "end_time": "2021-06-09T09:41:33.268359",
     "exception": false,
     "start_time": "2021-06-09T09:41:33.222263",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# import requests\n",
    "\n",
    "# paper_length = [] # store the number of sentences each paper has\n",
    "# def prepare_testdata(filename):\n",
    "#     test_rows = []\n",
    "#     with open(filename) as f:\n",
    "#         for row in f:\n",
    "#             json_row = json.loads(row)\n",
    "#             test_rows.append(json_row)\n",
    "#     return test_rows\n",
    "    \n",
    "# test_rows = prepare_testdata(train_file) #test data in NER format\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-06-09T09:41:33.357954Z",
     "iopub.status.busy": "2021-06-09T09:41:33.357182Z",
     "iopub.status.idle": "2021-06-09T09:42:37.071284Z",
     "shell.execute_reply": "2021-06-09T09:42:37.070296Z",
     "shell.execute_reply.started": "2021-06-06T13:41:21.220854Z"
    },
    "papermill": {
     "duration": 63.762863,
     "end_time": "2021-06-09T09:42:37.071433",
     "exception": false,
     "start_time": "2021-06-09T09:41:33.308570",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rm: cannot remove './pred': No such file or directory\r\n",
      "2021-06-09 09:41:36.646120: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudart.so.10.2\r\n",
      "Downloading and preparing dataset json/default (download: Unknown size, generated: Unknown size, post-processed: Unknown size, total: Unknown size) to /root/.cache/huggingface/datasets/json/default-3e668983be5bb6cc/0.0.0/83d5b3a2f62630efc6b5315f00f20209b4ad91a00ac586597caee3a4da0bef02...\r\n",
      "Dataset json downloaded and prepared to /root/.cache/huggingface/datasets/json/default-3e668983be5bb6cc/0.0.0/83d5b3a2f62630efc6b5315f00f20209b4ad91a00ac586597caee3a4da0bef02. Subsequent calls will reuse this data.\r\n",
      "[INFO|configuration_utils.py:470] 2021-06-09 09:42:17,670 >> loading configuration file /kaggle/input/tdmsci/results/output/config.json\r\n",
      "[INFO|configuration_utils.py:508] 2021-06-09 09:42:17,671 >> Model config BertConfig {\r\n",
      "  \"_name_or_path\": \"allenai/scibert_scivocab_cased\",\r\n",
      "  \"architectures\": [\r\n",
      "    \"BertForTokenClassification\"\r\n",
      "  ],\r\n",
      "  \"attention_probs_dropout_prob\": 0.1,\r\n",
      "  \"finetuning_task\": \"ner\",\r\n",
      "  \"gradient_checkpointing\": false,\r\n",
      "  \"hidden_act\": \"gelu\",\r\n",
      "  \"hidden_dropout_prob\": 0.1,\r\n",
      "  \"hidden_size\": 768,\r\n",
      "  \"id2label\": {\r\n",
      "    \"0\": \"LABEL_0\",\r\n",
      "    \"1\": \"LABEL_1\",\r\n",
      "    \"2\": \"LABEL_2\"\r\n",
      "  },\r\n",
      "  \"initializer_range\": 0.02,\r\n",
      "  \"intermediate_size\": 3072,\r\n",
      "  \"label2id\": {\r\n",
      "    \"LABEL_0\": 0,\r\n",
      "    \"LABEL_1\": 1,\r\n",
      "    \"LABEL_2\": 2\r\n",
      "  },\r\n",
      "  \"layer_norm_eps\": 1e-12,\r\n",
      "  \"max_position_embeddings\": 512,\r\n",
      "  \"model_type\": \"bert\",\r\n",
      "  \"num_attention_heads\": 12,\r\n",
      "  \"num_hidden_layers\": 12,\r\n",
      "  \"pad_token_id\": 0,\r\n",
      "  \"position_embedding_type\": \"absolute\",\r\n",
      "  \"transformers_version\": \"4.5.0.dev0\",\r\n",
      "  \"type_vocab_size\": 2,\r\n",
      "  \"use_cache\": true,\r\n",
      "  \"vocab_size\": 31116\r\n",
      "}\r\n",
      "\r\n",
      "[INFO|configuration_utils.py:470] 2021-06-09 09:42:17,672 >> loading configuration file /kaggle/input/tdmsci/results/output/config.json\r\n",
      "[INFO|configuration_utils.py:508] 2021-06-09 09:42:17,672 >> Model config BertConfig {\r\n",
      "  \"_name_or_path\": \"allenai/scibert_scivocab_cased\",\r\n",
      "  \"architectures\": [\r\n",
      "    \"BertForTokenClassification\"\r\n",
      "  ],\r\n",
      "  \"attention_probs_dropout_prob\": 0.1,\r\n",
      "  \"finetuning_task\": \"ner\",\r\n",
      "  \"gradient_checkpointing\": false,\r\n",
      "  \"hidden_act\": \"gelu\",\r\n",
      "  \"hidden_dropout_prob\": 0.1,\r\n",
      "  \"hidden_size\": 768,\r\n",
      "  \"id2label\": {\r\n",
      "    \"0\": \"LABEL_0\",\r\n",
      "    \"1\": \"LABEL_1\",\r\n",
      "    \"2\": \"LABEL_2\"\r\n",
      "  },\r\n",
      "  \"initializer_range\": 0.02,\r\n",
      "  \"intermediate_size\": 3072,\r\n",
      "  \"label2id\": {\r\n",
      "    \"LABEL_0\": 0,\r\n",
      "    \"LABEL_1\": 1,\r\n",
      "    \"LABEL_2\": 2\r\n",
      "  },\r\n",
      "  \"layer_norm_eps\": 1e-12,\r\n",
      "  \"max_position_embeddings\": 512,\r\n",
      "  \"model_type\": \"bert\",\r\n",
      "  \"num_attention_heads\": 12,\r\n",
      "  \"num_hidden_layers\": 12,\r\n",
      "  \"pad_token_id\": 0,\r\n",
      "  \"position_embedding_type\": \"absolute\",\r\n",
      "  \"transformers_version\": \"4.5.0.dev0\",\r\n",
      "  \"type_vocab_size\": 2,\r\n",
      "  \"use_cache\": true,\r\n",
      "  \"vocab_size\": 31116\r\n",
      "}\r\n",
      "\r\n",
      "[INFO|tokenization_utils_base.py:1637] 2021-06-09 09:42:17,674 >> Didn't find file /kaggle/input/tdmsci/results/output/tokenizer.json. We won't load it.\r\n",
      "[INFO|tokenization_utils_base.py:1637] 2021-06-09 09:42:17,675 >> Didn't find file /kaggle/input/tdmsci/results/output/added_tokens.json. We won't load it.\r\n",
      "[INFO|tokenization_utils_base.py:1700] 2021-06-09 09:42:17,677 >> loading file /kaggle/input/tdmsci/results/output/vocab.txt\r\n",
      "[INFO|tokenization_utils_base.py:1700] 2021-06-09 09:42:17,677 >> loading file None\r\n",
      "[INFO|tokenization_utils_base.py:1700] 2021-06-09 09:42:17,678 >> loading file None\r\n",
      "[INFO|tokenization_utils_base.py:1700] 2021-06-09 09:42:17,678 >> loading file /kaggle/input/tdmsci/results/output/special_tokens_map.json\r\n",
      "[INFO|tokenization_utils_base.py:1700] 2021-06-09 09:42:17,678 >> loading file /kaggle/input/tdmsci/results/output/tokenizer_config.json\r\n",
      "[INFO|modeling_utils.py:1049] 2021-06-09 09:42:17,758 >> loading weights file /kaggle/input/tdmsci/results/output/pytorch_model.bin\r\n",
      "[INFO|modeling_utils.py:1167] 2021-06-09 09:42:26,103 >> All model checkpoint weights were used when initializing BertForTokenClassification.\r\n",
      "\r\n",
      "[INFO|modeling_utils.py:1176] 2021-06-09 09:42:26,103 >> All the weights of BertForTokenClassification were initialized from the model checkpoint at /kaggle/input/tdmsci/results/output.\r\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use BertForTokenClassification for predictions without further training.\r\n",
      "[WARNING|tokenization_utils_base.py:2137] 2021-06-09 09:42:26,104 >> Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\r\n",
      "100%|█████████████████████████████████████████████| 1/1 [00:00<00:00,  8.20ba/s]\r\n",
      "[INFO|trainer.py:485] 2021-06-09 09:42:33,270 >> The following columns in the test set  don't have a corresponding argument in `BertForTokenClassification.forward` and have been ignored: id, tags, tokens.\r\n",
      "[INFO|trainer.py:1817] 2021-06-09 09:42:33,272 >> ***** Running Prediction *****\r\n",
      "[INFO|trainer.py:1818] 2021-06-09 09:42:33,272 >>   Num examples = 365\r\n",
      "[INFO|trainer.py:1819] 2021-06-09 09:42:33,272 >>   Batch size = 8\r\n",
      " 98%|██████████████████████████████████████████ | 45/46 [00:01<00:00, 28.20it/s]/opt/conda/lib/python3.7/site-packages/seqeval/metrics/v1.py:57: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 due to no true samples. Use `zero_division` parameter to control this behavior.\r\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\r\n",
      "[INFO|trainer_pt_utils.py:735] 2021-06-09 09:42:35,957 >> ***** test metrics *****\r\n",
      "[INFO|trainer_pt_utils.py:740] 2021-06-09 09:42:35,957 >>   init_mem_cpu_alloc_delta  =     1563MB\r\n",
      "[INFO|trainer_pt_utils.py:740] 2021-06-09 09:42:35,957 >>   init_mem_cpu_peaked_delta =      305MB\r\n",
      "[INFO|trainer_pt_utils.py:740] 2021-06-09 09:42:35,957 >>   init_mem_gpu_alloc_delta  =      418MB\r\n",
      "[INFO|trainer_pt_utils.py:740] 2021-06-09 09:42:35,957 >>   init_mem_gpu_peaked_delta =        0MB\r\n",
      "[INFO|trainer_pt_utils.py:740] 2021-06-09 09:42:35,957 >>   test_accuracy             =     0.9947\r\n",
      "[INFO|trainer_pt_utils.py:740] 2021-06-09 09:42:35,957 >>   test_f1                   =        0.0\r\n",
      "[INFO|trainer_pt_utils.py:740] 2021-06-09 09:42:35,957 >>   test_loss                 =     0.0403\r\n",
      "[INFO|trainer_pt_utils.py:740] 2021-06-09 09:42:35,957 >>   test_mem_cpu_alloc_delta  =       80MB\r\n",
      "[INFO|trainer_pt_utils.py:740] 2021-06-09 09:42:35,957 >>   test_mem_cpu_peaked_delta =        0MB\r\n",
      "[INFO|trainer_pt_utils.py:740] 2021-06-09 09:42:35,957 >>   test_mem_gpu_alloc_delta  =        0MB\r\n",
      "[INFO|trainer_pt_utils.py:740] 2021-06-09 09:42:35,958 >>   test_mem_gpu_peaked_delta =       29MB\r\n",
      "[INFO|trainer_pt_utils.py:740] 2021-06-09 09:42:35,958 >>   test_precision            =        0.0\r\n",
      "[INFO|trainer_pt_utils.py:740] 2021-06-09 09:42:35,958 >>   test_recall               =        0.0\r\n",
      "[INFO|trainer_pt_utils.py:740] 2021-06-09 09:42:35,958 >>   test_runtime              = 0:00:02.54\r\n",
      "[INFO|trainer_pt_utils.py:740] 2021-06-09 09:42:35,958 >>   test_samples_per_second   =    143.518\r\n",
      "100%|███████████████████████████████████████████| 46/46 [00:02<00:00, 22.89it/s]\r\n"
     ]
    }
   ],
   "source": [
    "bert_outputs = []\n",
    "batch_size = 64000\n",
    "\n",
    "for batch_begin in range(0, len(test_rows), batch_size):#len(test_rows), batch_size):\n",
    "    # write data rows to input file\n",
    "    with open(f\"{test_input_save_path}/{filename_test}\", 'w') as f:\n",
    "        for row in test_rows[batch_begin:batch_begin+batch_size]:\n",
    "            json.dump(row, f)\n",
    "            f.write('\\n')\n",
    "            \n",
    "    with open(f\"{test_input_save_path}/{filename_test}\", 'r') as f:\n",
    "        content = f.read()\n",
    "        \n",
    "    # remove output dir\n",
    "    !rm -r \"$OUTPUT_DIR\"\n",
    "    \n",
    "    # do predict\n",
    "    predict_scibert_ner()\n",
    "    \n",
    "    # read predictions\n",
    "    with open(f'{pred_save_path}/{prediction_file}') as f:\n",
    "        this_preds = f.read().split('\\n')[:-1]\n",
    "        bert_outputs += [pred.split() for pred in this_preds]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-06-09T09:42:37.173796Z",
     "iopub.status.busy": "2021-06-09T09:42:37.173080Z",
     "iopub.status.idle": "2021-06-09T09:42:37.176015Z",
     "shell.execute_reply": "2021-06-09T09:42:37.175593Z",
     "shell.execute_reply.started": "2021-06-06T13:42:34.424775Z"
    },
    "papermill": {
     "duration": 0.054833,
     "end_time": "2021-06-09T09:42:37.176128",
     "exception": false,
     "start_time": "2021-06-09T09:42:37.121295",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "#print(bert_outputs)\n",
    "\n",
    "# labelArrays = []\n",
    "# for output in bert_outputs:\n",
    "#     if('B-DATASET' in output or 'I-DATASET' in output):\n",
    "#         labelArrays.append(output)\n",
    "# print(labelArrays)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.047641,
     "end_time": "2021-06-09T09:42:37.272146",
     "exception": false,
     "start_time": "2021-06-09T09:42:37.224505",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## 5.5. Restore labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-06-09T09:42:37.380153Z",
     "iopub.status.busy": "2021-06-09T09:42:37.379430Z",
     "iopub.status.idle": "2021-06-09T09:42:37.383319Z",
     "shell.execute_reply": "2021-06-09T09:42:37.382916Z",
     "shell.execute_reply.started": "2021-06-06T13:42:34.43494Z"
    },
    "papermill": {
     "duration": 0.063204,
     "end_time": "2021-06-09T09:42:37.383432",
     "exception": false,
     "start_time": "2021-06-09T09:42:37.320228",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "34\n",
      "150\n",
      "97\n",
      "84\n",
      "0\n"
     ]
    }
   ],
   "source": [
    "\n",
    "test_sentences = [row['tokens'] for row in test_rows]\n",
    "#del test_rows\n",
    "\n",
    "bert_dataset_labels = [] # store all dataset labels for each publication\n",
    "\n",
    "for length in paper_lengths:\n",
    "    print(length)\n",
    "    labels = set()\n",
    "    for sentence, pred in zip(test_sentences[:length], bert_outputs[:length]):\n",
    "        curr_phrase = ''\n",
    "        for word, tag in zip(sentence, pred):\n",
    "            if tag == 'B-DATASET': # start a new phrase\n",
    "                if curr_phrase:\n",
    "                    labels.add(curr_phrase)\n",
    "                    curr_phrase = ''\n",
    "                curr_phrase = word\n",
    "            elif tag == 'I-DATASET' and curr_phrase: # continue the phrase\n",
    "                curr_phrase += ' ' + word\n",
    "            else: # end last phrase (if any)\n",
    "                if curr_phrase:\n",
    "                    labels.add(curr_phrase)\n",
    "                    curr_phrase = ''\n",
    "        # check if the label is the suffix of the sentence\n",
    "        if curr_phrase:\n",
    "            labels.add(curr_phrase)\n",
    "            curr_phrase = ''\n",
    "    \n",
    "    # record dataset labels for this publication\n",
    "    bert_dataset_labels.append(labels)\n",
    "    \n",
    "    del test_sentences[:length], bert_outputs[:length]\n",
    "print(len(bert_outputs))\n",
    "\n",
    "# def getTags(sentences, preds):\n",
    "#     labels = []\n",
    "#     dataset = \"\"\n",
    "\n",
    "#     for sentence, preds in zip(sentences, preds):\n",
    "#         dataset = \"\"\n",
    "\n",
    "#         for word, tag in zip(sentence.split(), preds):\n",
    "#             #print(\"Word:{}, tag:{}\".format(word,tag) )\n",
    "#             if(tag == \"B-DATASET\"):\n",
    "#                 dataset += tag + ' '\n",
    "#             elif (tag == \"I-DATASET\" and dataset != \"\"):\n",
    "#                 print(\"dfsdfdsf\")\n",
    "#                 dataset += tag + ' '\n",
    "#             elif(tag != \"B-DATSET\" and tag != \"I-DATASET\" and dataset != \"\"):\n",
    "#                 labels.append(dataset)\n",
    "#                 dataset= \"\"\n",
    "    \n",
    "#     if(dataset ==\"\"):\n",
    "#         labels.append(\"\")\n",
    "#     return (\"|\".join(labels))\n",
    "            \n",
    "    \n",
    "# def restoreLabels(predictions, test_df):\n",
    "#     test_sentences_papers = test_df[\"Sentences\"]\n",
    "#     ids = test_df[\"Id\"]\n",
    "#     startIndex = 0\n",
    "    \n",
    "#     acc = 0\n",
    "#     predictionStrings = []\n",
    "#     for paper_i in range(len(test_sentences_papers)):\n",
    "#         length = len(test_sentences_papers[paper_i])\n",
    "#         paper_id = ids[paper_i]\n",
    "#         predictions_paper = predictions[startIndex:startIndex+length]\n",
    "#         labels = getTags(test_sentences_papers[paper_i], predictions_paper)\n",
    "#         predictionStrings.append(labels)\n",
    "#         startIndex += length\n",
    "#         acc += 1\n",
    "#     return predictionStrings\n",
    "      \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-06-09T09:42:37.486169Z",
     "iopub.status.busy": "2021-06-09T09:42:37.485417Z",
     "iopub.status.idle": "2021-06-09T09:42:37.489135Z",
     "shell.execute_reply": "2021-06-09T09:42:37.488677Z",
     "shell.execute_reply.started": "2021-06-06T13:42:34.488295Z"
    },
    "papermill": {
     "duration": 0.057116,
     "end_time": "2021-06-09T09:42:37.489252",
     "exception": false,
     "start_time": "2021-06-09T09:42:37.432136",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'Alzheimer s Disease Neuroimaging Initiative ADNI'},\n",
       " {'Common Core of Data',\n",
       "  'Trends in International Mathematics and Science Study',\n",
       "  'trends in International Mathematics and Science Study'},\n",
       " set(),\n",
       " set()]"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bert_dataset_labels[:5]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-06-09T09:42:37.595044Z",
     "iopub.status.busy": "2021-06-09T09:42:37.594225Z",
     "iopub.status.idle": "2021-06-09T09:42:37.596554Z",
     "shell.execute_reply": "2021-06-09T09:42:37.596948Z",
     "shell.execute_reply.started": "2021-06-06T13:42:34.503356Z"
    },
    "papermill": {
     "duration": 0.0589,
     "end_time": "2021-06-09T09:42:37.597091",
     "exception": false,
     "start_time": "2021-06-09T09:42:37.538191",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def jaccard_similarity(s1, s2):\n",
    "    l1 = s1.split(\" \")\n",
    "    l2 = s2.split(\" \")    \n",
    "    intersection = len(list(set(l1).intersection(l2)))\n",
    "    union = (len(l1) + len(l2)) - intersection\n",
    "    return float(intersection) / union\n",
    "\n",
    "filtered_bert_labels = []\n",
    "\n",
    "for labels in bert_dataset_labels:\n",
    "    filtered = []\n",
    "    \n",
    "    for label in sorted(labels, key=len):\n",
    "        label = clean_text(label)\n",
    "        if len(filtered) == 0 or all(jaccard_similarity(label, got_label) < 0.75 for got_label in filtered):\n",
    "            filtered.append(label)\n",
    "    \n",
    "    filtered_bert_labels.append('|'.join(filtered))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-06-09T09:42:37.698075Z",
     "iopub.status.busy": "2021-06-09T09:42:37.697246Z",
     "iopub.status.idle": "2021-06-09T09:42:37.700605Z",
     "shell.execute_reply": "2021-06-09T09:42:37.701117Z",
     "shell.execute_reply.started": "2021-06-06T13:43:24.611401Z"
    },
    "papermill": {
     "duration": 0.055557,
     "end_time": "2021-06-09T09:42:37.701244",
     "exception": false,
     "start_time": "2021-06-09T09:42:37.645687",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['alzheimer s disease neuroimaging initiative adni',\n",
       " 'common core of data|trends in international mathematics and science study',\n",
       " '',\n",
       " '']"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "filtered_bert_labels[:100]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-06-09T09:42:37.803973Z",
     "iopub.status.busy": "2021-06-09T09:42:37.803290Z",
     "iopub.status.idle": "2021-06-09T09:42:37.807000Z",
     "shell.execute_reply": "2021-06-09T09:42:37.806575Z",
     "shell.execute_reply.started": "2021-06-06T13:45:14.156727Z"
    },
    "papermill": {
     "duration": 0.056837,
     "end_time": "2021-06-09T09:42:37.807109",
     "exception": false,
     "start_time": "2021-06-09T09:42:37.750272",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "literal_match:alzheimer s disease neuroimaging initiative adni|adni --- bert_pred:alzheimer s disease neuroimaging initiative adni \n",
      "--------------------------------------\n",
      "literal_match:integrated postsecondary education data system|common core of data|schools and staffing survey|nces common core of data|trends in international mathematics and science study|progress in international reading literacy study --- bert_pred:common core of data|trends in international mathematics and science study \n",
      "--------------------------------------\n",
      "literal_match:sea lake and overland surges from hurricanes|noaa storm surge inundation|slosh model --- bert_pred: \n",
      "--------------------------------------\n",
      "literal_match:rural urban continuum codes --- bert_pred: \n",
      "--------------------------------------\n"
     ]
    }
   ],
   "source": [
    "final_predictions = []\n",
    "for literal_match, bert_pred in zip(literal_preds, filtered_bert_labels):\n",
    "    print(\"literal_match:{} --- bert_pred:{} \\n--------------------------------------\".format(literal_match, bert_pred))\n",
    "    if literal_match:\n",
    "        final_predictions.append(literal_match)\n",
    "    else:\n",
    "        final_predictions.append(bert_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-06-09T09:42:37.909556Z",
     "iopub.status.busy": "2021-06-09T09:42:37.908845Z",
     "iopub.status.idle": "2021-06-09T09:42:37.911955Z",
     "shell.execute_reply": "2021-06-09T09:42:37.912358Z",
     "shell.execute_reply.started": "2021-06-06T13:45:14.433398Z"
    },
    "papermill": {
     "duration": 0.05649,
     "end_time": "2021-06-09T09:42:37.912493",
     "exception": false,
     "start_time": "2021-06-09T09:42:37.856003",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['alzheimer s disease neuroimaging initiative adni|adni',\n",
       " 'integrated postsecondary education data system|common core of data|schools and staffing survey|nces common core of data|trends in international mathematics and science study|progress in international reading literacy study',\n",
       " 'sea lake and overland surges from hurricanes|noaa storm surge inundation|slosh model',\n",
       " 'rural urban continuum codes']"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_predictions[0:100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-06-09T09:42:38.028370Z",
     "iopub.status.busy": "2021-06-09T09:42:38.027814Z",
     "iopub.status.idle": "2021-06-09T09:42:38.035626Z",
     "shell.execute_reply": "2021-06-09T09:42:38.035070Z",
     "shell.execute_reply.started": "2021-06-06T13:45:14.893322Z"
    },
    "papermill": {
     "duration": 0.064933,
     "end_time": "2021-06-09T09:42:38.035737",
     "exception": false,
     "start_time": "2021-06-09T09:42:37.970804",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Id</th>\n",
       "      <th>PredictionString</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2100032a-7c33-4bff-97ef-690822c43466</td>\n",
       "      <td>alzheimer s disease neuroimaging initiative ad...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2f392438-e215-4169-bebf-21ac4ff253e1</td>\n",
       "      <td>integrated postsecondary education data system...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3f316b38-1a24-45a9-8d8c-4e05a42257c6</td>\n",
       "      <td>sea lake and overland surges from hurricanes|n...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>8e6996b4-ca08-4c0b-bed2-aaf07a4c6a60</td>\n",
       "      <td>rural urban continuum codes</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                     Id  \\\n",
       "0  2100032a-7c33-4bff-97ef-690822c43466   \n",
       "1  2f392438-e215-4169-bebf-21ac4ff253e1   \n",
       "2  3f316b38-1a24-45a9-8d8c-4e05a42257c6   \n",
       "3  8e6996b4-ca08-4c0b-bed2-aaf07a4c6a60   \n",
       "\n",
       "                                    PredictionString  \n",
       "0  alzheimer s disease neuroimaging initiative ad...  \n",
       "1  integrated postsecondary education data system...  \n",
       "2  sea lake and overland surges from hurricanes|n...  \n",
       "3                        rural urban continuum codes  "
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample_submission['PredictionString'] = final_predictions\n",
    "sample_submission.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-06-09T09:42:38.145629Z",
     "iopub.status.busy": "2021-06-09T09:42:38.144925Z",
     "iopub.status.idle": "2021-06-09T09:42:38.151510Z",
     "shell.execute_reply": "2021-06-09T09:42:38.151056Z",
     "shell.execute_reply.started": "2021-06-05T07:48:45.237903Z"
    },
    "papermill": {
     "duration": 0.063075,
     "end_time": "2021-06-09T09:42:38.151624",
     "exception": false,
     "start_time": "2021-06-09T09:42:38.088549",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "sample_submission.to_csv(f'submission.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.04988,
     "end_time": "2021-06-09T09:42:38.251268",
     "exception": false,
     "start_time": "2021-06-09T09:42:38.201388",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Generate submission file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-06-09T09:42:38.354183Z",
     "iopub.status.busy": "2021-06-09T09:42:38.353501Z",
     "iopub.status.idle": "2021-06-09T09:42:38.356425Z",
     "shell.execute_reply": "2021-06-09T09:42:38.356027Z",
     "shell.execute_reply.started": "2021-05-31T10:44:24.764977Z"
    },
    "papermill": {
     "duration": 0.055402,
     "end_time": "2021-06-09T09:42:38.356530",
     "exception": false,
     "start_time": "2021-06-09T09:42:38.301128",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# def generate_submission_file(test_ids, test_predictions):\n",
    "#     submission_dict = {\"Id\": test_ids, \"PredictionString\": test_predictions}\n",
    "#     submission_df = pd.DataFrame.from_dict(submission_dict)\n",
    "#     submission_df.to_csv(f'submission.csv', index=False)\n",
    "    \n",
    "    \n",
    "\n",
    "# generate_submission_file(test_df_ner[\"Id\"], final_predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.049476,
     "end_time": "2021-06-09T09:42:38.455058",
     "exception": false,
     "start_time": "2021-06-09T09:42:38.405582",
     "status": "completed"
    },
    "tags": []
   },
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "papermill": {
     "duration": 0.049428,
     "end_time": "2021-06-09T09:42:38.554056",
     "exception": false,
     "start_time": "2021-06-09T09:42:38.504628",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "papermill": {
     "duration": 0.050722,
     "end_time": "2021-06-09T09:42:38.654540",
     "exception": false,
     "start_time": "2021-06-09T09:42:38.603818",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 259.952667,
   "end_time": "2021-06-09T09:42:40.317510",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2021-06-09T09:38:20.364843",
   "version": "2.3.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
