{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.036644,
     "end_time": "2021-06-06T17:41:19.959413",
     "exception": false,
     "start_time": "2021-06-06T17:41:19.922769",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# PART I (Data creation)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "papermill": {
     "duration": 0.035237,
     "end_time": "2021-06-06T17:41:20.030004",
     "exception": false,
     "start_time": "2021-06-06T17:41:19.994767",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# 1. Introduction\n",
    "\n",
    "This notebook contains the data creation for the Kaggle challenge \"Coleridge Initative: Show US the Data\" (https://www.kaggle.com/c/coleridgeinitiative-show-us-the-data/). This challenge is about recognizing public datasets used in scientific papers. In particular, we want to extract the datasets for scientific paper, with several NLP approaches. In this notebook, we test both BERT and SciBERT. The first model is introduced by Devlin, J., Chang, M. W., Lee, K., and Toutanova, K., in 2018 [1]. Source code of BERT can be fuond [here](https://github.com/google-research/bert). The second model is  introduced by Beltagy, I., Lo, K., and Cohan, A. in 2019 [2]. Source code of SciBERT can be found [here](https://github.com/allenai/scibert).\n",
    "\n",
    "Furthermore, we append the existing data with a specialized Corpus for dataset tagging. TDMSci is a Corpus existing of annotated data for tasks, metrices and datasets. Here, B-DATASET and I-DATASET are the NER-labels indicating a word is (part of) a dataset [3]. Source code (and annotated data) of TDMSci can be found [here](https://github.com/IBM/science-result-extractor).\n",
    "\n",
    "\n",
    "We have created three notebooks, one for **dataset creation** (Part I), one for **training** ([Part IIa]() and [Part IIb]()) and one for **testing** ([Part III]()). This notebook creates the data for our used method.\n",
    "\n",
    "\n",
    "[1] Devlin, J., Chang, M. W., Lee, K., & Toutanova, K. (2018). Bert: Pre-training of deep bidirectional transformers for language understanding. arXiv preprint arXiv:1810.04805.\n",
    "[2] Beltagy, I., Lo, K., & Cohan, A. (2019). SciBERT: A pretrained language model for scientific text. arXiv preprint arXiv:1903.10676.  \n",
    "[3] Hou, Y., Jochim, C., Gleize, M., Bonin, F., & Ganguly, D. (2021). TDMSci: A Specialized Corpus for Scientific Literature Entity Tagging of Tasks Datasets and Metrics. arXiv preprint arXiv:2101.10273.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.035248,
     "end_time": "2021-06-06T17:41:20.102006",
     "exception": false,
     "start_time": "2021-06-06T17:41:20.066758",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# 2. Preparing Notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-06-06T17:41:20.176899Z",
     "iopub.status.busy": "2021-06-06T17:41:20.175755Z",
     "iopub.status.idle": "2021-06-06T17:42:31.329887Z",
     "shell.execute_reply": "2021-06-06T17:42:31.329097Z",
     "shell.execute_reply.started": "2021-06-06T16:57:27.101132Z"
    },
    "papermill": {
     "duration": 71.192736,
     "end_time": "2021-06-06T17:42:31.330049",
     "exception": false,
     "start_time": "2021-06-06T17:41:20.137313",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in links: file:///kaggle/input/coleridge-packages/packages/datasets\r\n",
      "Processing /kaggle/input/coleridge-packages/packages/datasets/datasets-1.5.0-py3-none-any.whl\r\n",
      "Requirement already satisfied: numpy>=1.17 in /opt/conda/lib/python3.7/site-packages (from datasets) (1.19.5)\r\n",
      "Processing /kaggle/input/coleridge-packages/packages/datasets/huggingface_hub-0.0.7-py3-none-any.whl\r\n",
      "Requirement already satisfied: importlib-metadata in /opt/conda/lib/python3.7/site-packages (from datasets) (3.4.0)\r\n",
      "Requirement already satisfied: pandas in /opt/conda/lib/python3.7/site-packages (from datasets) (1.2.3)\r\n",
      "Requirement already satisfied: pyarrow>=0.17.1 in /opt/conda/lib/python3.7/site-packages (from datasets) (3.0.0)\r\n",
      "Requirement already satisfied: fsspec in /opt/conda/lib/python3.7/site-packages (from datasets) (0.8.7)\r\n",
      "Processing /kaggle/input/coleridge-packages/packages/datasets/xxhash-2.0.0-cp37-cp37m-manylinux2010_x86_64.whl\r\n",
      "Requirement already satisfied: dill in /opt/conda/lib/python3.7/site-packages (from datasets) (0.3.3)\r\n",
      "Requirement already satisfied: multiprocess in /opt/conda/lib/python3.7/site-packages (from datasets) (0.70.11.1)\r\n",
      "Processing /kaggle/input/coleridge-packages/packages/datasets/tqdm-4.49.0-py2.py3-none-any.whl\r\n",
      "Requirement already satisfied: requests>=2.19.0 in /opt/conda/lib/python3.7/site-packages (from datasets) (2.25.1)\r\n",
      "Requirement already satisfied: filelock in /opt/conda/lib/python3.7/site-packages (from huggingface-hub<0.1.0->datasets) (3.0.12)\r\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.7/site-packages (from requests>=2.19.0->datasets) (2020.12.5)\r\n",
      "Requirement already satisfied: chardet<5,>=3.0.2 in /opt/conda/lib/python3.7/site-packages (from requests>=2.19.0->datasets) (4.0.0)\r\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /opt/conda/lib/python3.7/site-packages (from requests>=2.19.0->datasets) (1.26.4)\r\n",
      "Requirement already satisfied: idna<3,>=2.5 in /opt/conda/lib/python3.7/site-packages (from requests>=2.19.0->datasets) (2.10)\r\n",
      "Requirement already satisfied: zipp>=0.5 in /opt/conda/lib/python3.7/site-packages (from importlib-metadata->datasets) (3.4.1)\r\n",
      "Requirement already satisfied: typing-extensions>=3.6.4 in /opt/conda/lib/python3.7/site-packages (from importlib-metadata->datasets) (3.7.4.3)\r\n",
      "Requirement already satisfied: python-dateutil>=2.7.3 in /opt/conda/lib/python3.7/site-packages (from pandas->datasets) (2.8.1)\r\n",
      "Requirement already satisfied: pytz>=2017.3 in /opt/conda/lib/python3.7/site-packages (from pandas->datasets) (2021.1)\r\n",
      "Requirement already satisfied: six>=1.5 in /opt/conda/lib/python3.7/site-packages (from python-dateutil>=2.7.3->pandas->datasets) (1.15.0)\r\n",
      "Installing collected packages: tqdm, xxhash, huggingface-hub, datasets\r\n",
      "  Attempting uninstall: tqdm\r\n",
      "    Found existing installation: tqdm 4.59.0\r\n",
      "    Uninstalling tqdm-4.59.0:\r\n",
      "      Successfully uninstalled tqdm-4.59.0\r\n",
      "Successfully installed datasets-1.5.0 huggingface-hub-0.0.7 tqdm-4.49.0 xxhash-2.0.0\r\n",
      "Processing /kaggle/input/coleridge-packages/seqeval-1.2.2-py3-none-any.whl\r\n",
      "Requirement already satisfied: numpy>=1.14.0 in /opt/conda/lib/python3.7/site-packages (from seqeval==1.2.2) (1.19.5)\r\n",
      "Requirement already satisfied: scikit-learn>=0.21.3 in /opt/conda/lib/python3.7/site-packages (from seqeval==1.2.2) (0.24.1)\r\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in /opt/conda/lib/python3.7/site-packages (from scikit-learn>=0.21.3->seqeval==1.2.2) (2.1.0)\r\n",
      "Requirement already satisfied: scipy>=0.19.1 in /opt/conda/lib/python3.7/site-packages (from scikit-learn>=0.21.3->seqeval==1.2.2) (1.5.4)\r\n",
      "Requirement already satisfied: joblib>=0.11 in /opt/conda/lib/python3.7/site-packages (from scikit-learn>=0.21.3->seqeval==1.2.2) (1.0.1)\r\n",
      "Installing collected packages: seqeval\r\n",
      "Successfully installed seqeval-1.2.2\r\n",
      "Processing /kaggle/input/coleridge-packages/tokenizers-0.10.1-cp37-cp37m-manylinux1_x86_64.whl\r\n",
      "Installing collected packages: tokenizers\r\n",
      "  Attempting uninstall: tokenizers\r\n",
      "    Found existing installation: tokenizers 0.10.2\r\n",
      "    Uninstalling tokenizers-0.10.2:\r\n",
      "      Successfully uninstalled tokenizers-0.10.2\r\n",
      "Successfully installed tokenizers-0.10.1\r\n",
      "\u001b[33mWARNING: Requirement '../input/coleridge-packages/transformers-4.5.0.dev0-py3-nshufflene-any.whl' looks like a filename, but the file does not exist\u001b[0m\r\n",
      "\u001b[31mERROR: transformers-4.5.0.dev0-py3-nshufflene-any.whl is not a supported wheel on this platform.\u001b[0m\r\n",
      "Requirement already satisfied: datasets in /opt/conda/lib/python3.7/site-packages (1.5.0)\r\n",
      "Requirement already satisfied: multiprocess in /opt/conda/lib/python3.7/site-packages (from datasets) (0.70.11.1)\r\n",
      "Requirement already satisfied: dill in /opt/conda/lib/python3.7/site-packages (from datasets) (0.3.3)\r\n",
      "Requirement already satisfied: pandas in /opt/conda/lib/python3.7/site-packages (from datasets) (1.2.3)\r\n",
      "Requirement already satisfied: tqdm<4.50.0,>=4.27 in /opt/conda/lib/python3.7/site-packages (from datasets) (4.49.0)\r\n",
      "Requirement already satisfied: fsspec in /opt/conda/lib/python3.7/site-packages (from datasets) (0.8.7)\r\n",
      "Requirement already satisfied: numpy>=1.17 in /opt/conda/lib/python3.7/site-packages (from datasets) (1.19.5)\r\n",
      "Requirement already satisfied: importlib-metadata in /opt/conda/lib/python3.7/site-packages (from datasets) (3.4.0)\r\n",
      "Requirement already satisfied: pyarrow>=0.17.1 in /opt/conda/lib/python3.7/site-packages (from datasets) (3.0.0)\r\n",
      "Requirement already satisfied: requests>=2.19.0 in /opt/conda/lib/python3.7/site-packages (from datasets) (2.25.1)\r\n",
      "Requirement already satisfied: xxhash in /opt/conda/lib/python3.7/site-packages (from datasets) (2.0.0)\r\n",
      "Requirement already satisfied: huggingface-hub<0.1.0 in /opt/conda/lib/python3.7/site-packages (from datasets) (0.0.7)\r\n",
      "Requirement already satisfied: filelock in /opt/conda/lib/python3.7/site-packages (from huggingface-hub<0.1.0->datasets) (3.0.12)\r\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.7/site-packages (from requests>=2.19.0->datasets) (2020.12.5)\r\n",
      "Requirement already satisfied: idna<3,>=2.5 in /opt/conda/lib/python3.7/site-packages (from requests>=2.19.0->datasets) (2.10)\r\n",
      "Requirement already satisfied: chardet<5,>=3.0.2 in /opt/conda/lib/python3.7/site-packages (from requests>=2.19.0->datasets) (4.0.0)\r\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /opt/conda/lib/python3.7/site-packages (from requests>=2.19.0->datasets) (1.26.4)\r\n",
      "Requirement already satisfied: zipp>=0.5 in /opt/conda/lib/python3.7/site-packages (from importlib-metadata->datasets) (3.4.1)\r\n",
      "Requirement already satisfied: typing-extensions>=3.6.4 in /opt/conda/lib/python3.7/site-packages (from importlib-metadata->datasets) (3.7.4.3)\r\n",
      "Requirement already satisfied: python-dateutil>=2.7.3 in /opt/conda/lib/python3.7/site-packages (from pandas->datasets) (2.8.1)\r\n",
      "Requirement already satisfied: pytz>=2017.3 in /opt/conda/lib/python3.7/site-packages (from pandas->datasets) (2021.1)\r\n",
      "Requirement already satisfied: six>=1.5 in /opt/conda/lib/python3.7/site-packages (from python-dateutil>=2.7.3->pandas->datasets) (1.15.0)\r\n",
      "Requirement already satisfied: fsspec in /opt/conda/lib/python3.7/site-packages (0.8.7)\r\n",
      "Collecting fsspec\r\n",
      "  Downloading fsspec-2021.5.0-py3-none-any.whl (111 kB)\r\n",
      "\u001b[K     |████████████████████████████████| 111 kB 835 kB/s \r\n",
      "\u001b[?25hInstalling collected packages: fsspec\r\n",
      "  Attempting uninstall: fsspec\r\n",
      "    Found existing installation: fsspec 0.8.7\r\n",
      "    Uninstalling fsspec-0.8.7:\r\n",
      "      Successfully uninstalled fsspec-0.8.7\r\n",
      "Successfully installed fsspec-2021.5.0\r\n",
      "Collecting flair\r\n",
      "  Downloading flair-0.8.0.post1-py3-none-any.whl (284 kB)\r\n",
      "\u001b[K     |████████████████████████████████| 284 kB 858 kB/s \r\n",
      "\u001b[?25hRequirement already satisfied: huggingface-hub in /opt/conda/lib/python3.7/site-packages (from flair) (0.0.7)\r\n",
      "Collecting langdetect\r\n",
      "  Downloading langdetect-1.0.9.tar.gz (981 kB)\r\n",
      "\u001b[K     |████████████████████████████████| 981 kB 7.1 MB/s \r\n",
      "\u001b[?25hRequirement already satisfied: lxml in /opt/conda/lib/python3.7/site-packages (from flair) (4.6.3)\r\n",
      "Collecting ftfy\r\n",
      "  Downloading ftfy-6.0.3.tar.gz (64 kB)\r\n",
      "\u001b[K     |████████████████████████████████| 64 kB 1.3 MB/s \r\n",
      "\u001b[?25hRequirement already satisfied: deprecated>=1.2.4 in /opt/conda/lib/python3.7/site-packages (from flair) (1.2.12)\r\n",
      "Requirement already satisfied: tabulate in /opt/conda/lib/python3.7/site-packages (from flair) (0.8.9)\r\n",
      "Requirement already satisfied: python-dateutil>=2.6.1 in /opt/conda/lib/python3.7/site-packages (from flair) (2.8.1)\r\n",
      "Requirement already satisfied: hyperopt>=0.1.1 in /opt/conda/lib/python3.7/site-packages (from flair) (0.2.5)\r\n",
      "Requirement already satisfied: scikit-learn>=0.21.3 in /opt/conda/lib/python3.7/site-packages (from flair) (0.24.1)\r\n",
      "Requirement already satisfied: regex in /opt/conda/lib/python3.7/site-packages (from flair) (2021.3.17)\r\n",
      "Collecting gensim<=3.8.3,>=3.4.0\r\n",
      "  Downloading gensim-3.8.3-cp37-cp37m-manylinux1_x86_64.whl (24.2 MB)\r\n",
      "\u001b[K     |████████████████████████████████| 24.2 MB 8.1 MB/s \r\n",
      "\u001b[?25hRequirement already satisfied: tqdm>=4.26.0 in /opt/conda/lib/python3.7/site-packages (from flair) (4.49.0)\r\n",
      "Requirement already satisfied: janome in /opt/conda/lib/python3.7/site-packages (from flair) (0.4.1)\r\n",
      "Requirement already satisfied: numpy<1.20.0 in /opt/conda/lib/python3.7/site-packages (from flair) (1.19.5)\r\n",
      "Requirement already satisfied: sentencepiece==0.1.95 in /opt/conda/lib/python3.7/site-packages (from flair) (0.1.95)\r\n",
      "Collecting mpld3==0.3\r\n",
      "  Downloading mpld3-0.3.tar.gz (788 kB)\r\n",
      "\u001b[K     |████████████████████████████████| 788 kB 44.7 MB/s \r\n",
      "\u001b[?25hCollecting konoha<5.0.0,>=4.0.0\r\n",
      "  Downloading konoha-4.6.5-py3-none-any.whl (20 kB)\r\n",
      "Collecting sqlitedict>=1.6.0\r\n",
      "  Downloading sqlitedict-1.7.0.tar.gz (28 kB)\r\n",
      "Collecting bpemb>=0.3.2\r\n",
      "  Downloading bpemb-0.3.3-py3-none-any.whl (19 kB)\r\n",
      "Requirement already satisfied: transformers>=4.0.0 in /opt/conda/lib/python3.7/site-packages (from flair) (4.5.1)\r\n",
      "Requirement already satisfied: matplotlib>=2.2.3 in /opt/conda/lib/python3.7/site-packages (from flair) (3.4.1)\r\n",
      "Requirement already satisfied: torch<=1.7.1,>=1.5.0 in /opt/conda/lib/python3.7/site-packages (from flair) (1.7.0)\r\n",
      "Collecting gdown==3.12.2\r\n",
      "  Downloading gdown-3.12.2.tar.gz (8.2 kB)\r\n",
      "  Installing build dependencies ... \u001b[?25l-\b \b\\\b \b|\b \b/\b \b-\b \bdone\r\n",
      "\u001b[?25h  Getting requirements to build wheel ... \u001b[?25l-\b \bdone\r\n",
      "\u001b[?25h    Preparing wheel metadata ... \u001b[?25l-\b \bdone\r\n",
      "\u001b[?25hCollecting segtok>=1.5.7\r\n",
      "  Downloading segtok-1.5.10.tar.gz (25 kB)\r\n",
      "Requirement already satisfied: filelock in /opt/conda/lib/python3.7/site-packages (from gdown==3.12.2->flair) (3.0.12)\r\n",
      "Requirement already satisfied: six in /opt/conda/lib/python3.7/site-packages (from gdown==3.12.2->flair) (1.15.0)\r\n",
      "Requirement already satisfied: requests[socks] in /opt/conda/lib/python3.7/site-packages (from gdown==3.12.2->flair) (2.25.1)\r\n",
      "Requirement already satisfied: wrapt<2,>=1.10 in /opt/conda/lib/python3.7/site-packages (from deprecated>=1.2.4->flair) (1.12.1)\r\n",
      "Requirement already satisfied: scipy>=0.18.1 in /opt/conda/lib/python3.7/site-packages (from gensim<=3.8.3,>=3.4.0->flair) (1.5.4)\r\n",
      "Requirement already satisfied: smart-open>=1.8.1 in /opt/conda/lib/python3.7/site-packages (from gensim<=3.8.3,>=3.4.0->flair) (5.0.0)\r\n",
      "Requirement already satisfied: cloudpickle in /opt/conda/lib/python3.7/site-packages (from hyperopt>=0.1.1->flair) (1.6.0)\r\n",
      "Requirement already satisfied: networkx>=2.2 in /opt/conda/lib/python3.7/site-packages (from hyperopt>=0.1.1->flair) (2.5)\r\n",
      "Requirement already satisfied: future in /opt/conda/lib/python3.7/site-packages (from hyperopt>=0.1.1->flair) (0.18.2)\r\n",
      "Collecting importlib-metadata<4.0.0,>=3.7.0\r\n",
      "  Downloading importlib_metadata-3.10.1-py3-none-any.whl (14 kB)\r\n",
      "Requirement already satisfied: overrides<4.0.0,>=3.0.0 in /opt/conda/lib/python3.7/site-packages (from konoha<5.0.0,>=4.0.0->flair) (3.1.0)\r\n",
      "Requirement already satisfied: zipp>=0.5 in /opt/conda/lib/python3.7/site-packages (from importlib-metadata<4.0.0,>=3.7.0->konoha<5.0.0,>=4.0.0->flair) (3.4.1)\r\n",
      "Requirement already satisfied: typing-extensions>=3.6.4 in /opt/conda/lib/python3.7/site-packages (from importlib-metadata<4.0.0,>=3.7.0->konoha<5.0.0,>=4.0.0->flair) (3.7.4.3)\r\n",
      "Requirement already satisfied: pillow>=6.2.0 in /opt/conda/lib/python3.7/site-packages (from matplotlib>=2.2.3->flair) (7.2.0)\r\n",
      "Requirement already satisfied: cycler>=0.10 in /opt/conda/lib/python3.7/site-packages (from matplotlib>=2.2.3->flair) (0.10.0)\r\n",
      "Requirement already satisfied: pyparsing>=2.2.1 in /opt/conda/lib/python3.7/site-packages (from matplotlib>=2.2.3->flair) (2.4.7)\r\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in /opt/conda/lib/python3.7/site-packages (from matplotlib>=2.2.3->flair) (1.3.1)\r\n",
      "Requirement already satisfied: decorator>=4.3.0 in /opt/conda/lib/python3.7/site-packages (from networkx>=2.2->hyperopt>=0.1.1->flair) (4.4.2)\r\n",
      "Requirement already satisfied: idna<3,>=2.5 in /opt/conda/lib/python3.7/site-packages (from requests[socks]->gdown==3.12.2->flair) (2.10)\r\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /opt/conda/lib/python3.7/site-packages (from requests[socks]->gdown==3.12.2->flair) (1.26.4)\r\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.7/site-packages (from requests[socks]->gdown==3.12.2->flair) (2020.12.5)\r\n",
      "Requirement already satisfied: chardet<5,>=3.0.2 in /opt/conda/lib/python3.7/site-packages (from requests[socks]->gdown==3.12.2->flair) (4.0.0)\r\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in /opt/conda/lib/python3.7/site-packages (from scikit-learn>=0.21.3->flair) (2.1.0)\r\n",
      "Requirement already satisfied: joblib>=0.11 in /opt/conda/lib/python3.7/site-packages (from scikit-learn>=0.21.3->flair) (1.0.1)\r\n",
      "Requirement already satisfied: dataclasses in /opt/conda/lib/python3.7/site-packages (from torch<=1.7.1,>=1.5.0->flair) (0.6)\r\n",
      "Requirement already satisfied: tokenizers<0.11,>=0.10.1 in /opt/conda/lib/python3.7/site-packages (from transformers>=4.0.0->flair) (0.10.1)\r\n",
      "Requirement already satisfied: sacremoses in /opt/conda/lib/python3.7/site-packages (from transformers>=4.0.0->flair) (0.0.45)\r\n",
      "Requirement already satisfied: packaging in /opt/conda/lib/python3.7/site-packages (from transformers>=4.0.0->flair) (20.9)\r\n",
      "Requirement already satisfied: wcwidth in /opt/conda/lib/python3.7/site-packages (from ftfy->flair) (0.2.5)\r\n",
      "Requirement already satisfied: PySocks!=1.5.7,>=1.5.6 in /opt/conda/lib/python3.7/site-packages (from requests[socks]->gdown==3.12.2->flair) (1.7.1)\r\n",
      "Requirement already satisfied: click in /opt/conda/lib/python3.7/site-packages (from sacremoses->transformers>=4.0.0->flair) (7.1.2)\r\n",
      "Building wheels for collected packages: gdown, mpld3, segtok, sqlitedict, ftfy, langdetect\r\n",
      "  Building wheel for gdown (PEP 517) ... \u001b[?25l-\b \bdone\r\n",
      "\u001b[?25h  Created wheel for gdown: filename=gdown-3.12.2-py3-none-any.whl size=9693 sha256=1dcf5500e4f02e4f32d4267866d95fafb9941a49cd37f65108858f0069120c10\r\n",
      "  Stored in directory: /root/.cache/pip/wheels/ba/e0/7e/726e872a53f7358b4b96a9975b04e98113b005cd8609a63abc\r\n",
      "  Building wheel for mpld3 (setup.py) ... \u001b[?25l-\b \b\\\b \bdone\r\n",
      "\u001b[?25h  Created wheel for mpld3: filename=mpld3-0.3-py3-none-any.whl size=116678 sha256=d50ac29b1a2852d65e01a8463db1cc84b295d5f36c2dd4c3b6303b8713925693\r\n",
      "  Stored in directory: /root/.cache/pip/wheels/26/70/6a/1c79e59951a41b4045497da187b2724f5659ca64033cf4548e\r\n",
      "  Building wheel for segtok (setup.py) ... \u001b[?25l-\b \b\\\b \bdone\r\n",
      "\u001b[?25h  Created wheel for segtok: filename=segtok-1.5.10-py3-none-any.whl size=25018 sha256=93fd80ee16a9eb6024a89cfd9da4ccc6d311dae03a0292b6e32b14139e27b7b9\r\n",
      "  Stored in directory: /root/.cache/pip/wheels/67/b7/d0/a121106e61339eee5ed083bc230b1c8dc422c49a5a28c2addd\r\n",
      "  Building wheel for sqlitedict (setup.py) ... \u001b[?25l-\b \b\\\b \bdone\r\n",
      "\u001b[?25h  Created wheel for sqlitedict: filename=sqlitedict-1.7.0-py3-none-any.whl size=14376 sha256=5a2df66ec59e02b1c587f06e7730419d420bbdb9cc8d552872d7f0a7bf58fb23\r\n",
      "  Stored in directory: /root/.cache/pip/wheels/af/94/06/18c0e83e9e227da8f3582810b51f319bbfd181e508676a56c8\r\n",
      "  Building wheel for ftfy (setup.py) ... \u001b[?25l-\b \bdone\r\n",
      "\u001b[?25h  Created wheel for ftfy: filename=ftfy-6.0.3-py3-none-any.whl size=41913 sha256=aa9154c115024b3abacfaae1514856923a8de3871dba36b7de50657a8ce215bc\r\n",
      "  Stored in directory: /root/.cache/pip/wheels/19/f5/38/273eb3b5e76dfd850619312f693716ac4518b498f5ffb6f56d\r\n",
      "  Building wheel for langdetect (setup.py) ... \u001b[?25l-\b \b\\\b \b|\b \bdone\r\n",
      "\u001b[?25h  Created wheel for langdetect: filename=langdetect-1.0.9-py3-none-any.whl size=993221 sha256=c9303a3252602901d079a50713400ade22246c318020c736383e01a7c236cac9\r\n",
      "  Stored in directory: /root/.cache/pip/wheels/c5/96/8a/f90c59ed25d75e50a8c10a1b1c2d4c402e4dacfa87f3aff36a\r\n",
      "Successfully built gdown mpld3 segtok sqlitedict ftfy langdetect\r\n",
      "Installing collected packages: importlib-metadata, gensim, sqlitedict, segtok, mpld3, langdetect, konoha, gdown, ftfy, bpemb, flair\r\n",
      "  Attempting uninstall: importlib-metadata\r\n",
      "    Found existing installation: importlib-metadata 3.4.0\r\n",
      "    Uninstalling importlib-metadata-3.4.0:\r\n",
      "      Successfully uninstalled importlib-metadata-3.4.0\r\n",
      "  Attempting uninstall: gensim\r\n",
      "    Found existing installation: gensim 4.0.1\r\n",
      "    Uninstalling gensim-4.0.1:\r\n",
      "      Successfully uninstalled gensim-4.0.1\r\n",
      "  Attempting uninstall: mpld3\r\n",
      "    Found existing installation: mpld3 0.5.2\r\n",
      "    Uninstalling mpld3-0.5.2:\r\n",
      "      Successfully uninstalled mpld3-0.5.2\r\n",
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\r\n",
      "pyldavis 3.3.1 requires numpy>=1.20.0, but you have numpy 1.19.5 which is incompatible.\u001b[0m\r\n",
      "Successfully installed bpemb-0.3.3 flair-0.8.0.post1 ftfy-6.0.3 gdown-3.12.2 gensim-3.8.3 importlib-metadata-3.10.1 konoha-4.6.5 langdetect-1.0.9 mpld3-0.3 segtok-1.5.10 sqlitedict-1.7.0\r\n"
     ]
    }
   ],
   "source": [
    "!pip install datasets --no-index --find-links=file:///kaggle/input/coleridge-packages/packages/datasets \n",
    "!pip install ../input/coleridge-packages/seqeval-1.2.2-py3-none-any.whl \n",
    "!pip install ../input/coleridge-packages/tokenizers-0.10.1-cp37-cp37m-manylinux1_x86_64.whl \n",
    "!pip install ../input/coleridge-packages/transformers-4.5.0.dev0-py3-nshufflene-any.whl \n",
    "!pip install datasets \n",
    "!pip install --upgrade fsspec\n",
    "!pip install flair"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-06-06T17:42:31.541396Z",
     "iopub.status.busy": "2021-06-06T17:42:31.540853Z",
     "iopub.status.idle": "2021-06-06T17:42:46.177069Z",
     "shell.execute_reply": "2021-06-06T17:42:46.176268Z",
     "shell.execute_reply.started": "2021-06-06T16:58:30.408459Z"
    },
    "papermill": {
     "duration": 14.744309,
     "end_time": "2021-06-06T17:42:46.177198",
     "exception": false,
     "start_time": "2021-06-06T17:42:31.432889",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.7/site-packages/torchaudio/backend/utils.py:54: UserWarning: \"sox\" backend is being deprecated. The default backend will be changed to \"sox_io\" backend in 0.8.0 and \"sox\" backend will be removed in 0.9.0. Please migrate to \"sox_io\" backend. Please refer to https://github.com/pytorch/audio/issues/903 for the detail.\n",
      "  '\"sox\" backend is being deprecated. '\n"
     ]
    }
   ],
   "source": [
    "#Import necessary libraries\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "import nltk\n",
    "import re\n",
    "import os\n",
    "from os import listdir\n",
    "from os.path import isfile, join\n",
    "import re\n",
    "import json\n",
    "import time\n",
    "import datetime\n",
    "import random\n",
    "import glob\n",
    "import importlib\n",
    "import allennlp\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from transformers import *\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import torch\n",
    "from flair.datasets import ColumnCorpus\n",
    "from sklearn.utils import shuffle\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-06-06T17:42:46.320546Z",
     "iopub.status.busy": "2021-06-06T17:42:46.320034Z",
     "iopub.status.idle": "2021-06-06T17:42:46.323017Z",
     "shell.execute_reply": "2021-06-06T17:42:46.323512Z",
     "shell.execute_reply.started": "2021-06-06T17:09:42.811676Z"
    },
    "papermill": {
     "duration": 0.075375,
     "end_time": "2021-06-06T17:42:46.323655",
     "exception": false,
     "start_time": "2021-06-06T17:42:46.248280",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "#Initialize paths for data\n",
    "path_abs = '/kaggle/input/coleridgeinitiative-show-us-the-data/'\n",
    "path_train = os.path.join(path_abs,'train/')\n",
    "path_train_metadata = os.path.join(path_abs, 'train.csv')\n",
    "path_test = os.path.join(path_abs, 'test/')\n",
    "path_sample_submission = os.path.join(path_abs, 'sample_submission.csv')\n",
    "\n",
    "path_abs_tdmsci = '/kaggle/input/tdmsci/'\n",
    "path_test_tdmsci = os.path.join(path_abs_tdmsci, 'test_500_v2.txt')\n",
    "path_train_tdmsci = os.path.join(path_abs_tdmsci,'train_1500_v2.txt')\n",
    "path_train_nerjson = '/kaggle/working/train_ner.json'\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.068034,
     "end_time": "2021-06-06T17:42:46.459655",
     "exception": false,
     "start_time": "2021-06-06T17:42:46.391621",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# 3. Get to know the data\n",
    "\n",
    "Here, we load the provided train-, and test data and look into the provided labels.\n",
    "In total, there are 14316 papers for training purposes nad 4 papers for testing purposes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.068636,
     "end_time": "2021-06-06T17:42:46.597186",
     "exception": false,
     "start_time": "2021-06-06T17:42:46.528550",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## 3.1. Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-06-06T17:42:46.876972Z",
     "iopub.status.busy": "2021-06-06T17:42:46.876499Z",
     "iopub.status.idle": "2021-06-06T17:42:47.818051Z",
     "shell.execute_reply": "2021-06-06T17:42:47.818556Z",
     "shell.execute_reply.started": "2021-06-06T17:09:43.496311Z"
    },
    "papermill": {
     "duration": 1.012515,
     "end_time": "2021-06-06T17:42:47.818748",
     "exception": false,
     "start_time": "2021-06-06T17:42:46.806233",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Id</th>\n",
       "      <th>pub_title</th>\n",
       "      <th>dataset_title</th>\n",
       "      <th>dataset_label</th>\n",
       "      <th>cleaned_label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0007f880-0a9b-492d-9a58-76eb0b0e0bd7</td>\n",
       "      <td>The Impact of ICT Training on Income Generatio...</td>\n",
       "      <td>Program for the International Assessment of Ad...</td>\n",
       "      <td>Program for the International Assessment of Ad...</td>\n",
       "      <td>program for the international assessment of ad...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0008656f-0ba2-4632-8602-3017b44c2e90</td>\n",
       "      <td>Finnish Ninth Graders’ Gender Appropriateness ...</td>\n",
       "      <td>Trends in International Mathematics and Scienc...</td>\n",
       "      <td>Trends in International Mathematics and Scienc...</td>\n",
       "      <td>trends in international mathematics and scienc...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>000e04d6-d6ef-442f-b070-4309493221ba</td>\n",
       "      <td>Economic Research Service: Specialized Agency...</td>\n",
       "      <td>Agricultural Resource Management Survey</td>\n",
       "      <td>Agricultural Resources Management Survey</td>\n",
       "      <td>agricultural resources management survey</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>000efc17-13d8-433d-8f62-a3932fe4f3b8</td>\n",
       "      <td>Risk factors and global cognitive status relat...</td>\n",
       "      <td>Alzheimer's Disease Neuroimaging Initiative (A...</td>\n",
       "      <td>ADNI|Alzheimer's Disease Neuroimaging Initiati...</td>\n",
       "      <td>adni|alzheimer s disease neuroimaging initiati...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0010357a-6365-4e5f-b982-582e6d32c3ee</td>\n",
       "      <td>Timelines of COVID-19 Vaccines</td>\n",
       "      <td>SARS-CoV-2 genome sequence</td>\n",
       "      <td>genome sequence of COVID-19</td>\n",
       "      <td>genome sequence of covid 19</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14311</th>\n",
       "      <td>ffd19b3c-f941-45e5-9382-934b5041ec96</td>\n",
       "      <td>Water quality of the Mississippian carbonate a...</td>\n",
       "      <td>Census of Agriculture</td>\n",
       "      <td>Census of Agriculture</td>\n",
       "      <td>census of agriculture</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14312</th>\n",
       "      <td>ffd4d86a-0f26-44cc-baed-f0e209cc22af</td>\n",
       "      <td>A Spherical Brain Mapping of MR Images for the...</td>\n",
       "      <td>Alzheimer's Disease Neuroimaging Initiative (A...</td>\n",
       "      <td>Alzheimer's Disease Neuroimaging Initiative (A...</td>\n",
       "      <td>alzheimer s disease neuroimaging initiative adni</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14313</th>\n",
       "      <td>ffe7f334-245a-4de7-b600-d7ff4e28bfca</td>\n",
       "      <td>COVID-19 and Possible Pharmacological Preventi...</td>\n",
       "      <td>SARS-CoV-2 genome sequence</td>\n",
       "      <td>genome sequences of SARS-CoV-2</td>\n",
       "      <td>genome sequences of sars cov 2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14314</th>\n",
       "      <td>ffeb3568-7aed-4dbe-b177-cbd7f46f34af</td>\n",
       "      <td>Abandoning mathematics. Reconstructing the pro...</td>\n",
       "      <td>Trends in International Mathematics and Scienc...</td>\n",
       "      <td>Trends in International Mathematics and Scienc...</td>\n",
       "      <td>trends in international mathematics and scienc...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14315</th>\n",
       "      <td>ffee2676-a778-4521-b947-e1e420b126c5</td>\n",
       "      <td>A different viewpoint on student retention</td>\n",
       "      <td>Beginning Postsecondary Student|Beginning Post...</td>\n",
       "      <td>Beginning Postsecondary Student|Beginning Post...</td>\n",
       "      <td>beginning postsecondary student|beginning post...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>14316 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                         Id  \\\n",
       "0      0007f880-0a9b-492d-9a58-76eb0b0e0bd7   \n",
       "1      0008656f-0ba2-4632-8602-3017b44c2e90   \n",
       "2      000e04d6-d6ef-442f-b070-4309493221ba   \n",
       "3      000efc17-13d8-433d-8f62-a3932fe4f3b8   \n",
       "4      0010357a-6365-4e5f-b982-582e6d32c3ee   \n",
       "...                                     ...   \n",
       "14311  ffd19b3c-f941-45e5-9382-934b5041ec96   \n",
       "14312  ffd4d86a-0f26-44cc-baed-f0e209cc22af   \n",
       "14313  ffe7f334-245a-4de7-b600-d7ff4e28bfca   \n",
       "14314  ffeb3568-7aed-4dbe-b177-cbd7f46f34af   \n",
       "14315  ffee2676-a778-4521-b947-e1e420b126c5   \n",
       "\n",
       "                                               pub_title  \\\n",
       "0      The Impact of ICT Training on Income Generatio...   \n",
       "1      Finnish Ninth Graders’ Gender Appropriateness ...   \n",
       "2       Economic Research Service: Specialized Agency...   \n",
       "3      Risk factors and global cognitive status relat...   \n",
       "4                         Timelines of COVID-19 Vaccines   \n",
       "...                                                  ...   \n",
       "14311  Water quality of the Mississippian carbonate a...   \n",
       "14312  A Spherical Brain Mapping of MR Images for the...   \n",
       "14313  COVID-19 and Possible Pharmacological Preventi...   \n",
       "14314  Abandoning mathematics. Reconstructing the pro...   \n",
       "14315         A different viewpoint on student retention   \n",
       "\n",
       "                                           dataset_title  \\\n",
       "0      Program for the International Assessment of Ad...   \n",
       "1      Trends in International Mathematics and Scienc...   \n",
       "2                Agricultural Resource Management Survey   \n",
       "3      Alzheimer's Disease Neuroimaging Initiative (A...   \n",
       "4                             SARS-CoV-2 genome sequence   \n",
       "...                                                  ...   \n",
       "14311                              Census of Agriculture   \n",
       "14312  Alzheimer's Disease Neuroimaging Initiative (A...   \n",
       "14313                         SARS-CoV-2 genome sequence   \n",
       "14314  Trends in International Mathematics and Scienc...   \n",
       "14315  Beginning Postsecondary Student|Beginning Post...   \n",
       "\n",
       "                                           dataset_label  \\\n",
       "0      Program for the International Assessment of Ad...   \n",
       "1      Trends in International Mathematics and Scienc...   \n",
       "2               Agricultural Resources Management Survey   \n",
       "3      ADNI|Alzheimer's Disease Neuroimaging Initiati...   \n",
       "4                            genome sequence of COVID-19   \n",
       "...                                                  ...   \n",
       "14311                              Census of Agriculture   \n",
       "14312  Alzheimer's Disease Neuroimaging Initiative (A...   \n",
       "14313                     genome sequences of SARS-CoV-2   \n",
       "14314  Trends in International Mathematics and Scienc...   \n",
       "14315  Beginning Postsecondary Student|Beginning Post...   \n",
       "\n",
       "                                           cleaned_label  \n",
       "0      program for the international assessment of ad...  \n",
       "1      trends in international mathematics and scienc...  \n",
       "2               agricultural resources management survey  \n",
       "3      adni|alzheimer s disease neuroimaging initiati...  \n",
       "4                            genome sequence of covid 19  \n",
       "...                                                  ...  \n",
       "14311                              census of agriculture  \n",
       "14312  alzheimer s disease neuroimaging initiative adni   \n",
       "14313                     genome sequences of sars cov 2  \n",
       "14314  trends in international mathematics and scienc...  \n",
       "14315  beginning postsecondary student|beginning post...  \n",
       "\n",
       "[14316 rows x 5 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Load metadata\n",
    "label_df = pd.read_csv(path_train_metadata)\n",
    "label_df = label_df.groupby('Id').agg({\n",
    "    'pub_title': 'first',\n",
    "    'dataset_title': '|'.join,\n",
    "    'dataset_label': '|'.join,\n",
    "    'cleaned_label': '|'.join\n",
    "}).reset_index()\n",
    "\n",
    "sample_submission = pd.read_csv(path_sample_submission)\n",
    "#Inpsect data\n",
    "label_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-06-06T17:42:48.028462Z",
     "iopub.status.busy": "2021-06-06T17:42:48.027789Z",
     "iopub.status.idle": "2021-06-06T17:42:48.034782Z",
     "shell.execute_reply": "2021-06-06T17:42:48.034086Z",
     "shell.execute_reply.started": "2021-06-06T17:09:43.991674Z"
    },
    "papermill": {
     "duration": 0.11372,
     "end_time": "2021-06-06T17:42:48.034927",
     "exception": false,
     "start_time": "2021-06-06T17:42:47.921207",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "#Create dataframe with labels\n",
    "label_df = label_df[['Id','cleaned_label']]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-06-06T17:42:48.246031Z",
     "iopub.status.busy": "2021-06-06T17:42:48.245341Z",
     "iopub.status.idle": "2021-06-06T17:42:58.032234Z",
     "shell.execute_reply": "2021-06-06T17:42:58.031413Z",
     "shell.execute_reply.started": "2021-06-06T17:09:43.999572Z"
    },
    "papermill": {
     "duration": 9.894331,
     "end_time": "2021-06-06T17:42:58.032384",
     "exception": false,
     "start_time": "2021-06-06T17:42:48.138053",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['section_title', 'text'])\n",
      "14316\n"
     ]
    }
   ],
   "source": [
    "filenames_train = [f for f in listdir(path_train) if isfile(join(path_train, f))]\n",
    "sample_filename = filenames_train[0] \n",
    "\n",
    "with open(path_train + sample_filename) as f:\n",
    "    sample_dict = json.load(f)\n",
    "    \n",
    "print(sample_dict[0].keys())\n",
    "print(len(filenames_train))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.068084,
     "end_time": "2021-06-06T17:42:58.546873",
     "exception": false,
     "start_time": "2021-06-06T17:42:58.478789",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# 4. Load train data (and apply basic NLP)\n",
    "\n",
    "Here, we will load our provided train data and apply basic NLP methods on this. THe basic NLP methods include normalization of the sentences and shorten the sentenes. The normalization is done by removing basic punciation such like: +,/.\\,- etc. The second adjustment is done. because BERT handles a maximum sequence length of 512 tokens as input. Hence, we need to shorten the sentences. This is done by splitting the sentences in words and cutting them off after 64 words, with an overlap of 20 words. \n",
    "\n",
    "Because we deal with quite a huge amount of training data (14316 papers, let alone sentences), we create the training data in batches of 5000 papers each. Because the test data is quite small, we decided to leave the first 100 papers of the training data for validation purposes. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-06-06T17:42:58.685848Z",
     "iopub.status.busy": "2021-06-06T17:42:58.685299Z",
     "iopub.status.idle": "2021-06-06T17:42:58.701198Z",
     "shell.execute_reply": "2021-06-06T17:42:58.701594Z",
     "shell.execute_reply.started": "2021-06-06T17:09:48.359566Z"
    },
    "papermill": {
     "duration": 0.087279,
     "end_time": "2021-06-06T17:42:58.701763",
     "exception": false,
     "start_time": "2021-06-06T17:42:58.614484",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "MAX_LENGTH = 64\n",
    "OVERLAP = 20\n",
    "\n",
    "def clean_paper_sentence(sentence):\n",
    "    \"\"\"\n",
    "    Input: sentence (string), Returns: sentence (string)\n",
    "    This function is essentially clean_text without lowercasing.\n",
    "    \"\"\"\n",
    "    sentence = re.sub('[^A-Za-z0-9]+', ' ', str(sentence)).strip()\n",
    "    sentence = re.sub(' +', ' ', sentence)\n",
    "    return sentence\n",
    "\n",
    "def shorten_sentences(sentences):\n",
    "    \"\"\"\n",
    "    Input: sentences (list), Returns: short_sentences (list)\n",
    "    \n",
    "    Sentences that have more than MAX_LENGTH words will be split\n",
    "    into multiple sentences with overlappings.\n",
    "    \"\"\"\n",
    "    short_sentences = []\n",
    "    for sentence in sentences:\n",
    "        words = sentence.split()\n",
    "        if len(words) > MAX_LENGTH:\n",
    "            for p in range(0, len(words), MAX_LENGTH - OVERLAP):\n",
    "                short_sentences.append(' '.join(words[p:p+MAX_LENGTH]))\n",
    "        else:\n",
    "            short_sentences.append(sentence)\n",
    "    return short_sentences\n",
    "\n",
    "\n",
    "def concatenate_text(json_dict):\n",
    "    '''\n",
    "    Input: json_dict (dictionary), Returns: sentences (list)\n",
    "    \n",
    "    Concatenate text and split sentences, as the BERT (and SciBERT) model\n",
    "    has the contraint of maximum sequence length of 512.\n",
    "    '''\n",
    "    total_text = \"\"\n",
    "        \n",
    "    for section_dict in json_dict:\n",
    "        total_text += section_dict['text']+ '\\n'\n",
    "    #sentences = nltk.tokenize.sent_tokenize(total_text) # This seems to take a lot of time?\n",
    "    sentences = re.split('\\. ', total_text)\n",
    "    sentences = [clean_paper_sentence(s) for s in sentences]\n",
    "   \n",
    "    sentences = shorten_sentences(sentences)\n",
    "    sentences = [sentence for sentence in sentences if len(sentence) > 10]\n",
    "    \n",
    "\n",
    "    return sentences\n",
    "\n",
    "def create_train_df(path, start_batch, end_batch):\n",
    "    '''\n",
    "    Arguments: path (string), start_batch (int), end_batch (int)\n",
    "    '''\n",
    "    #Initialize dictionary with right keys\n",
    "    final_dict = dict()\n",
    "    final_dict[\"Id\"] = []\n",
    "    final_dict[\"Sentences\"] = []\n",
    "    \n",
    "    \n",
    "    counter = 0\n",
    "    for root, _, files in os.walk(path):\n",
    "        files.sort()\n",
    "        files = files[start_batch:end_batch]\n",
    "        \n",
    "        for filename in range(0,len(files)):\n",
    "            id = files[filename][:-5] #Remove .json from filename to retrieve id\n",
    "            with open(path_train + files[filename]) as f:\n",
    "                json_dict = json.load(f)\n",
    "                sentences = concatenate_text(json_dict)\n",
    "                final_dict[\"Id\"].append(id)\n",
    "                final_dict[\"Sentences\"].append(sentences)\n",
    "                \n",
    "            counter += 1\n",
    "    df = pd.DataFrame.from_dict(final_dict)\n",
    "\n",
    "    return df\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.100846,
     "end_time": "2021-06-06T17:42:59.158071",
     "exception": false,
     "start_time": "2021-06-06T17:42:59.057225",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# 5. Create dataset\n",
    "\n",
    "We see that the training data for now exists of an Id (of a paper) with a list of corresponding sentences. Now we need to preprocess this, in order to apply NER. As we are only interest in datasets, we annotated the data just as is done by the authors introducing TDMSci [1]. We make use of the same annotation format as is used in the [CoNLL-2003 dataset](https://www.clips.uantwerpen.be/conll2003/ner/).  \n",
    "\n",
    "This means we can have two types of NER-tags: _B-types_ and _I-types_. The first type indicates the beginning of a named entity (or beginning of a phrase). The second type indicates other words in a named entity (or a phrase). When there is no entity (or word that is not part of a phrase), this word is labelled with 'O'. For this task, we use three different NER-task: **B-DATASET**, **I-DATASET** and **O**.  \n",
    "\n",
    "This boils down to labelling every word not being (part of) a dataset as **O**, whilst datasets are being tagged with either **B-DATASET**, or **I-DATASET**.\n",
    "\n",
    "[1] Hou, Y., Jochim, C., Gleize, M., Bonin, F., & Ganguly, D. (2021). TDMSci: A Specialized Corpus for Scientific Literature Entity Tagging of Tasks Datasets and Metrics. arXiv preprint arXiv:2101.10273.\n",
    "\n",
    "\n",
    "We first apply NER in combination with SciBERT. For this purpose, we need the pretrained [AutoModelForTokenClassification](https://huggingface.co/transformers/model_doc/auto.html#automodelfortokenclassification) from SciBERT, which we already loaded at the beginning of this notebook."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.109586,
     "end_time": "2021-06-06T17:42:59.374026",
     "exception": false,
     "start_time": "2021-06-06T17:42:59.264440",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## 5.1. Preprocess data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-06-06T17:42:59.604286Z",
     "iopub.status.busy": "2021-06-06T17:42:59.592357Z",
     "iopub.status.idle": "2021-06-06T17:42:59.614114Z",
     "shell.execute_reply": "2021-06-06T17:42:59.614911Z",
     "shell.execute_reply.started": "2021-06-06T17:34:15.632992Z"
    },
    "papermill": {
     "duration": 0.133237,
     "end_time": "2021-06-06T17:42:59.615118",
     "exception": false,
     "start_time": "2021-06-06T17:42:59.481881",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "def find_dataset_indices(tokens, label_tokens):\n",
    "    '''\n",
    "    Arugments: tokens (list), label_tokens (list)\n",
    "    Returns: dataset_indices(list)\n",
    "    \n",
    "    Gets indices of mentioned dataset in a list of words.\n",
    "    '''\n",
    "    dataset_indices = []\n",
    "    for i in range(0, len(tokens)):\n",
    "        if(tokens[i] == label_tokens[0] and tokens[i:i+len(label_tokens)]== label_tokens):\n",
    "            dataset_indices = [*range(i, i+len(label_tokens))]\n",
    "    return dataset_indices\n",
    "\n",
    "def tag_sentence(sentence, tokenizer, datasets, words_with_label, counter_sent):\n",
    "    '''\n",
    "    Arguments: sentence (string), tokenizer (boolean), datasets (list), words_with_label (list), counter_sent (int)\n",
    "    Returns: tokens (list), NER_sent (list), found_dataset (boolean), words_with_label (list)\n",
    "    \n",
    "    Senteces are tagged with either 'B-DATASET', 'I-DATAST' or 'O', based on whether a datset in [datasets] is mentioned\n",
    "    in [sentence].\n",
    "    \n",
    "    NB: Tokenizer and words_with_label for testing purposes included. Can be ignored.\n",
    "    '''\n",
    "    NER_sent = []\n",
    "    \n",
    "    tokens = sentence.split() #Tokenize sentence\n",
    "    assert(len(tokens) < 512)\n",
    "    found_dataset = False\n",
    "\n",
    "    if(any(re.findall(dataset,sentence.lower()) for dataset in datasets)):#If a dataset is found in a sentence , add right NER-labels to tokens\n",
    "        found_dataset = True\n",
    "        NER_sent = ['O']* len(tokens)   \n",
    "    \n",
    "        for dataset in datasets:              \n",
    "            tokens_dataset = dataset.split()  #Tokenize dataset labels\n",
    "      \n",
    "            dataset_indices = find_dataset_indices([token.lower() for token in tokens], tokens_dataset)\n",
    "            if(dataset_indices != []):\n",
    "                for token in range(0, len(tokens)): #If current token is first word of dataset, add B-DATASET\n",
    "                    if(token == dataset_indices[0]):\n",
    "                        NER_sent[token] = 'B-DATASET'\n",
    "                        words_with_label.append((tokens[token], 'B-DATASET', counter_sent)) \n",
    "                        first_found = True\n",
    "                    elif(token in dataset_indices): #If current token is not the first word, but in tokenized dataset, add I-DATASET\n",
    "                        NER_sent[token]= 'I-DATASET'\n",
    "                        words_with_label.append((tokens[token], 'I-DATASET', counter_sent))\n",
    "                    else: #If current token is not part of a dataset add O\n",
    "                        NER_sent[token] = ('O')\n",
    "                        first_found = False\n",
    "\n",
    "    else: #No dataset found in a sentence\n",
    "        NER_sent = ['O']* len(tokens)\n",
    "     \n",
    "    return list(zip(tokens, NER_sent)), found_dataset, words_with_label\n",
    "\n",
    "def preprocess_data_ner(data_df, label_df, tokenizer):\n",
    "    '''\n",
    "    Arguments: data_df (dataframe), label_df (dataframe), tokenizer (SciBERT tokenizer)\n",
    "    Returns: dict_final (dictionary), words_with_label (list), count_postives (int), count_negatives (int), counter_sent (int)\n",
    "    \n",
    "    Adds NER-labels to each token of data_df based on the following rules:\n",
    "       - IF token is start of dataset THEN tag =  'B-DATASET'\n",
    "       - IF token is not start but part of dataset THEN tag =  'I-DATASET'\n",
    "       - IF token is not part of dataset THEN tag = 'O'\n",
    "    \n",
    "    Only positive sentences and 50% of hard negatives are included in final dictionary [dict_final].\n",
    "    Hard negatives are sentences with the word 'data' or 'study' in it, but does not actaully contain the mention of a dataset.\n",
    "    \n",
    "    NB: words_with_label for testing purposes included. Can be ignored.\n",
    "    '''\n",
    "     \n",
    "    #Initalize and prepare dictionary\n",
    "    dict_final = dict()\n",
    "    dict_final['Id'] = []\n",
    "    dict_final['Sentences'] = []\n",
    "    dict_final['NER-labels'] = []\n",
    "    \n",
    "    words_with_label = [] # For checking purposess\n",
    "    count_postives = 0\n",
    "    count_negatives = 0\n",
    "    counter_sent = 0\n",
    "\n",
    "    for id, row in data_df.iterrows():\n",
    "            key_id = row[0]\n",
    "            dict_final['Id'].append(key_id)\n",
    "            sentences = row[1]\n",
    "            dict_final['Sentences'].append(sentences)\n",
    "            datasets = re.split('\\|', list(label_df.loc[label_df['Id'] == key_id][\"cleaned_label\"])[0]) #Look up labels based on id\n",
    "\n",
    "            NER_labels = []\n",
    "            NER_labels_NEG = []\n",
    "            \n",
    "#             random_sentence_id = random.randrange(0, len(sentences))\n",
    "            #Add NER-labels to each token of each sentence\n",
    "            for sent_id, sentence in enumerate(sentences):\n",
    "                \n",
    "                \n",
    "                NER_sent, dataset_found, words_with_label = tag_sentence(sentence, tokenizer, datasets, words_with_label, counter_sent)\n",
    "                if dataset_found:\n",
    "                                  \n",
    "                    count_postives += 1\n",
    "                    NER_labels.append(NER_sent)\n",
    "                elif ( any(word in sentence.lower() for word in ['data', 'study'])): \n",
    "                   #Randomly remove half of the hard negatives\n",
    "\n",
    "                    if random.choice([False, True]):\n",
    "                        NER_labels_NEG.append(NER_sent)\n",
    "                        count_negatives+= 1\n",
    "\n",
    "                \n",
    "                counter_sent += 1\n",
    "            \n",
    "            random.shuffle(NER_labels_NEG)\n",
    "            NER_labels+= NER_labels_NEG[0:len(NER_labels)]\n",
    "            dict_final['NER-labels'].append(NER_labels)\n",
    "                        \n",
    "            \n",
    "    return dict_final, words_with_label, count_postives, count_negatives, counter_sent\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.070292,
     "end_time": "2021-06-06T17:43:00.061458",
     "exception": false,
     "start_time": "2021-06-06T17:42:59.991166",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# 5.2. Save preprocessed data\n",
    "\n",
    "Now we save the preprocessed training data. We can load this data into our project to use it in the future."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-06-06T17:43:00.206362Z",
     "iopub.status.busy": "2021-06-06T17:43:00.205717Z",
     "iopub.status.idle": "2021-06-06T17:43:00.213895Z",
     "shell.execute_reply": "2021-06-06T17:43:00.214260Z",
     "shell.execute_reply.started": "2021-06-06T17:34:16.420159Z"
    },
    "papermill": {
     "duration": 0.081501,
     "end_time": "2021-06-06T17:43:00.214419",
     "exception": false,
     "start_time": "2021-06-06T17:43:00.132918",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def save_train_ner(filename, column, ids):\n",
    "    ''' \n",
    "    Arguments: fileneme (string), column (string), ids (list)\n",
    "    Returns: Nothing\n",
    "    \n",
    "    Writes the (batch of the) training data to a file under the name of [filename], in a fomrat usable for BERT training and testing.\n",
    "    '''\n",
    "    \n",
    "    with open(filename, 'a+') as f:\n",
    "        for row_i in range(len(column)):\n",
    "            if(column[row_i] != []):\n",
    "                for sentence in column[row_i]:      \n",
    "                    if(sentence != []):\n",
    "                        \n",
    "                        words, nes = list(zip(*sentence))\n",
    "                        assert(len(words) == len(nes))\n",
    "                \n",
    "                        if(len(words) > 512):\n",
    "                            print(\"uhhoh words\")\n",
    "                            print(ids[ row_i])\n",
    "                        row_json = {'tokens' : words, 'tags' : nes, 'id': ids[row_i]}\n",
    "                       \n",
    "                        json.dump(row_json, f)\n",
    "                        f.write('\\n')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.068979,
     "end_time": "2021-06-06T17:43:00.352601",
     "exception": false,
     "start_time": "2021-06-06T17:43:00.283622",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# 6. Create the training data in batches\n",
    "\n",
    "Here, the actual training datasets are created in batches of 5000 articles to prevent OOM errors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-06-06T17:43:00.497177Z",
     "iopub.status.busy": "2021-06-06T17:43:00.496503Z",
     "iopub.status.idle": "2021-06-06T17:43:00.509589Z",
     "shell.execute_reply": "2021-06-06T17:43:00.509994Z",
     "shell.execute_reply.started": "2021-06-06T17:34:16.741257Z"
    },
    "papermill": {
     "duration": 0.085801,
     "end_time": "2021-06-06T17:43:00.510199",
     "exception": false,
     "start_time": "2021-06-06T17:43:00.424398",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Load TDMSci data\n",
    "\n",
    "from flair.data import Corpus\n",
    "from flair.datasets import ColumnCorpus\n",
    "\n",
    "# Make flair column corpus with TDMSci data\n",
    "def read_tdmsci(filename):\n",
    "    '''\n",
    "    Arguments: filename (string)\n",
    "    Retunrs: corpus1 (<ColumnCorpus>)\n",
    "    \n",
    "    Impports the TDMSCI dataset in the ColumnCorpus format.\n",
    "    '''\n",
    "    # define columns\n",
    "    columns = {0: 'text', 1: 'pos', 2: 'ner'} \n",
    "\n",
    "    # init a corpus using column format, data folder and the names of the train, dev and test files\n",
    "    corpus1: Corpus = ColumnCorpus(path_abs_tdmsci, columns,\n",
    "                                   train_file=filename,\n",
    "                                   test_file=filename,\n",
    "                                   dev_file=filename)\n",
    "    return corpus1\n",
    "\n",
    "# Get tuple of text and NER-tag for TDMSci input token\n",
    "def get_tdmsci_tuple(token):\n",
    "    return (token.text, token.annotation_layers['ner'][0].value)\n",
    "\n",
    "# Process TDMSci data\n",
    "def tdmsci_to_df(filename, tdmsci_id):\n",
    "    '''\n",
    "    Arguments: filename (string), tdmsci_id (string)\n",
    "    Retunrs: df_final (dictionary)\n",
    "    \n",
    "    Creates training data from the TDMSCI dataset, in a fomrat usable for BERT training and testing.\n",
    "    '''\n",
    "    tdmsci_corpus = read_tdmsci(filename)\n",
    "    dict_final = dict()\n",
    "    dict_final['Id'] = []\n",
    "    dict_final['Sentences'] = []\n",
    "    dict_final['NER-labels'] = []\n",
    "    for sent in tdmsci_corpus.train:\n",
    "        intermediate_NER_labels = []\n",
    "        for tok in sent:\n",
    "            (token, tag) = get_tdmsci_tuple(tok)\n",
    "            intermediate_NER_labels.append((token, tag))\n",
    "        sentence = sent.to_plain_string()\n",
    "        sentence = clean_paper_sentence(sentence)\n",
    "        sentences = shorten_sentences([sentence])\n",
    "        allTags = [x[1] for x in intermediate_NER_labels]\n",
    "        if(len(sentence) > 10):\n",
    "            if(np.all([x not in allTags for x in ['B-METRIC', 'B-TASK', 'I-METRIC', 'I-TASK']])):\n",
    "                dict_final['Id'].append(tdmsci_id)\n",
    "                dict_final['Sentences'].append(sentences)\n",
    "                dict_final['NER-labels'].append([intermediate_NER_labels])\n",
    "    df_final = pd.DataFrame.from_dict(dict_final)\n",
    "    return df_final\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-06-06T17:43:01.086819Z",
     "iopub.status.busy": "2021-06-06T17:43:01.086090Z",
     "iopub.status.idle": "2021-06-06T17:43:01.089916Z",
     "shell.execute_reply": "2021-06-06T17:43:01.089424Z",
     "shell.execute_reply.started": "2021-06-06T17:34:17.224590Z"
    },
    "papermill": {
     "duration": 0.115264,
     "end_time": "2021-06-06T17:43:01.090033",
     "exception": false,
     "start_time": "2021-06-06T17:43:00.974769",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def make_training_data(batch_size):\n",
    "    '''\n",
    "    Arguments: batch_size(int)\n",
    "    Returns: Nothing\n",
    "    \n",
    "    Creates training data suitable for executing NER with the BERT model, with addition of TDMSCI data.\n",
    "    Based on batch size, from the 100th training paper, the final training dataset is created.\n",
    "    '''\n",
    "    N_train = len(filenames_train)\n",
    "    start_idx = 100\n",
    "    \n",
    "    #Empty file\n",
    "    with open('train_ner.json', 'w') as f:\n",
    "              f.write('')\n",
    "    \n",
    "    \n",
    "    while((start_idx + batch_size) < (N_train + batch_size)):\n",
    "        print(f'Batch {start_idx} through {start_idx + batch_size} of {N_train}')\n",
    "        #Create train data\n",
    "        train_df_ner = create_train_df(path_train, start_idx, (start_idx + batch_size))\n",
    "        print(f'Batch size: {len(train_df_ner)}')\n",
    "\n",
    "        #Preprocess train data for NER classification\n",
    "        train_preprocessed, words_with_label, count_postives, count_negatives, counter_sent = preprocess_data_ner(train_df_ner, label_df, True)\n",
    "        print(f'There are {count_postives} positive sentences and {count_negatives} negative sentences out of {counter_sent} sentences')\n",
    "        train_preprocessed_df = pd.DataFrame.from_dict(train_preprocessed)\n",
    "        \n",
    "        #Append TDMSci NER to Coleridge dataset\n",
    "        if((start_idx + batch_size) > N_train):\n",
    "            \n",
    "            train_tdmsci_df = tdmsci_to_df(path_train_tdmsci, 'TDMSCI_train')\n",
    "            test_tdmsci_df = tdmsci_to_df(path_test_tdmsci, 'TMDSCI_test')\n",
    "            \n",
    "            #Append to Coleridge data\n",
    "            print(f'Adding TDMSci data (size: {(len(train_tdmsci_df[\"Sentences\"])+len(test_tdmsci_df[\"Sentences\"]))})')\n",
    "            train_preprocessed_df = train_preprocessed_df.append(train_tdmsci_df, ignore_index=True)\n",
    "            train_preprocessed_df = train_preprocessed_df.append(test_tdmsci_df, ignore_index=True)\n",
    "        \n",
    "        train_preprocessed_df = shuffle(train_preprocessed_df)\n",
    "        #Append batch to final json file\n",
    "        save_train_ner('train_ner.json', train_preprocessed_df[\"NER-labels\"], train_preprocessed_df[\"Id\"])\n",
    "        start_idx = start_idx + batch_size\n",
    "        \n",
    "        \n",
    "       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-06-06T17:43:01.810707Z",
     "iopub.status.busy": "2021-06-06T17:43:01.810004Z",
     "iopub.status.idle": "2021-06-06T17:47:52.289196Z",
     "shell.execute_reply": "2021-06-06T17:47:52.288516Z",
     "shell.execute_reply.started": "2021-06-06T17:34:19.240931Z"
    },
    "papermill": {
     "duration": 290.553817,
     "end_time": "2021-06-06T17:47:52.289336",
     "exception": false,
     "start_time": "2021-06-06T17:43:01.735519",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 100 through 5100 of 14316\n",
      "Batch size: 5000\n",
      "There are 19252 positive sentences and 88353 negative sentences out of 1378229 sentences\n",
      "Batch 5100 through 10100 of 14316\n",
      "Batch size: 5000\n",
      "There are 19346 positive sentences and 87899 negative sentences out of 1335900 sentences\n",
      "Batch 10100 through 15100 of 14316\n",
      "Batch size: 4216\n",
      "There are 15163 positive sentences and 79173 negative sentences out of 1156448 sentences\n",
      "2021-06-06 17:47:40,619 Reading data from /kaggle/input/tdmsci\n",
      "2021-06-06 17:47:40,621 Train: /kaggle/input/tdmsci/train_1500_v2.txt\n",
      "2021-06-06 17:47:40,621 Dev: /kaggle/input/tdmsci/train_1500_v2.txt\n",
      "2021-06-06 17:47:40,622 Test: /kaggle/input/tdmsci/train_1500_v2.txt\n",
      "2021-06-06 17:47:48,289 Reading data from /kaggle/input/tdmsci\n",
      "2021-06-06 17:47:48,290 Train: /kaggle/input/tdmsci/test_500_v2.txt\n",
      "2021-06-06 17:47:48,291 Dev: /kaggle/input/tdmsci/test_500_v2.txt\n",
      "2021-06-06 17:47:48,292 Test: /kaggle/input/tdmsci/test_500_v2.txt\n",
      "Adding TDMSci data (size: 551)\n"
     ]
    }
   ],
   "source": [
    "make_training_data(5000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-06-06T17:47:52.520518Z",
     "iopub.status.busy": "2021-06-06T17:47:52.519559Z",
     "iopub.status.idle": "2021-06-06T17:47:54.018201Z",
     "shell.execute_reply": "2021-06-06T17:47:54.017793Z",
     "shell.execute_reply.started": "2021-06-06T17:13:35.649353Z"
    },
    "papermill": {
     "duration": 1.618287,
     "end_time": "2021-06-06T17:47:54.018313",
     "exception": false,
     "start_time": "2021-06-06T17:47:52.400026",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 95394 rows in train_ner.json\n",
      "The unique labels are:['B-DATASET' 'I-DATASET' 'O']\n"
     ]
    }
   ],
   "source": [
    "with open(path_train_nerjson) as f:\n",
    "    acc = 0\n",
    "    labels = []\n",
    "    for row in f:\n",
    "        rowjson = json.loads(row)\n",
    "        acc +=1\n",
    "        labels+= rowjson[\"tags\"]\n",
    "    \n",
    "print(\"There are {} rows in train_ner.json\".format(acc))\n",
    "print(\"The unique labels are:{}\".format(np.unique(labels)))\n",
    "        "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 408.331239,
   "end_time": "2021-06-06T17:47:58.215856",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2021-06-06T17:41:09.884617",
   "version": "2.3.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
