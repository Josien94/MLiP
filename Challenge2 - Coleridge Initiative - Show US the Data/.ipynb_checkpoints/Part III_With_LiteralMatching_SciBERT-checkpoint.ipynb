{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.035702,
     "end_time": "2021-06-13T09:21:07.430638",
     "exception": false,
     "start_time": "2021-06-13T09:21:07.394936",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Part III (Testing)\n",
    "\n",
    "# 1. Introduction\n",
    "\n",
    "This notebook contains the prediction phase for the Kaggle challenge \"Coleridge Initative: Show US the Data\" (https://www.kaggle.com/c/coleridgeinitiative-show-us-the-data/)\n",
    "\n",
    "To recap, this challenge is about datasets used in scientific papers. In particular, we want to extract the datasets for scientific paper, with several NLP approaches. In this notebook, we test both BERT and SciBERT. The first model is introduced by Devlin, J., Chang, M. W., Lee, K., and Toutanova, K., in 2018 [1]. Source code of BERT can be fuond [here](https://github.com/google-research/bert). The second model is  introduced by Beltagy, I., Lo, K., and Cohan, A. in 2019 [2]. Source code of SciBERT can be found [here](https://github.com/allenai/scibert).\n",
    "\n",
    "\n",
    "Furthermore, we append the existing data with a specialized Corpus for dataset tagging. TDMSci is a Corpus existing of annotated data for tasks, metrices and datasets. Here, B-DATASET and I-DATASET are the NER-labels indicating a word is (part of) a dataset [3]. Source code (and annotated data) of TDMSci can be found [here](https://github.com/IBM/science-result-extractor).\n",
    "\n",
    "This boils down to exectuing Named Entity Recognition (NER), in particular token classfication.\n",
    "\n",
    "We have created three notebooks, one for **dataset creation** ([Part I]()), one for **training** ([Part IIa]() and [Part IIb]()) and this one for **testing** (Part III). This part exists of the testing phase of the model. We loaded the pre-trained model from either [part IIa()] or [part IIb()]  and executed predictions with the pre-trained model on our validation set.\n",
    "\n",
    "For part I (creating dataset), we refer to [this notebook](https://www.kaggle.com/lunaelise/fork-of-mlip-group25-scibert-dataset)\n",
    "For part II (training), we refer to [this notebook](https://www.kaggle.com/lunaelise/fork-of-mlip-group25-scibert-training).\n",
    "\n",
    "\n",
    "\n",
    "[1] Devlin, J., Chang, M. W., Lee, K., & Toutanova, K. (2018). Bert: Pre-training of deep bidirectional transformers for language understanding. arXiv preprint arXiv:1810.04805.\n",
    "[2] Beltagy, I., Lo, K., & Cohan, A. (2019). SciBERT: A pretrained language model for scientific text. arXiv preprint arXiv:1903.10676.  \n",
    "[3] Hou, Y., Jochim, C., Gleize, M., Bonin, F., & Ganguly, D. (2021). TDMSci: A Specialized Corpus for Scientific Literature Entity Tagging of Tasks Datasets and Metrics. arXiv preprint arXiv:2101.10273.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.032639,
     "end_time": "2021-06-13T09:21:07.496115",
     "exception": false,
     "start_time": "2021-06-13T09:21:07.463476",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# 2. Preparing Notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-06-13T09:21:07.573112Z",
     "iopub.status.busy": "2021-06-13T09:21:07.572572Z",
     "iopub.status.idle": "2021-06-13T09:22:59.896621Z",
     "shell.execute_reply": "2021-06-13T09:22:59.895982Z",
     "shell.execute_reply.started": "2021-06-13T08:59:37.127064Z"
    },
    "papermill": {
     "duration": 112.367936,
     "end_time": "2021-06-13T09:22:59.896810",
     "exception": false,
     "start_time": "2021-06-13T09:21:07.528874",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in links: file:///kaggle/input/coleridge-packages/packages/datasets\r\n",
      "Processing /kaggle/input/coleridge-packages/packages/datasets/datasets-1.5.0-py3-none-any.whl\r\n",
      "Processing /kaggle/input/coleridge-packages/packages/datasets/huggingface_hub-0.0.7-py3-none-any.whl\r\n",
      "Requirement already satisfied: fsspec in /opt/conda/lib/python3.7/site-packages (from datasets) (0.8.5)\r\n",
      "Requirement already satisfied: importlib-metadata in /opt/conda/lib/python3.7/site-packages (from datasets) (3.4.0)\r\n",
      "Requirement already satisfied: pandas in /opt/conda/lib/python3.7/site-packages (from datasets) (1.1.5)\r\n",
      "Processing /kaggle/input/coleridge-packages/packages/datasets/xxhash-2.0.0-cp37-cp37m-manylinux2010_x86_64.whl\r\n",
      "Requirement already satisfied: numpy>=1.17 in /opt/conda/lib/python3.7/site-packages (from datasets) (1.19.5)\r\n",
      "Requirement already satisfied: pyarrow>=0.17.1 in /opt/conda/lib/python3.7/site-packages (from datasets) (1.0.1)\r\n",
      "Processing /kaggle/input/coleridge-packages/packages/datasets/tqdm-4.49.0-py2.py3-none-any.whl\r\n",
      "Requirement already satisfied: requests>=2.19.0 in /opt/conda/lib/python3.7/site-packages (from datasets) (2.25.1)\r\n",
      "Requirement already satisfied: dill in /opt/conda/lib/python3.7/site-packages (from datasets) (0.3.3)\r\n",
      "Requirement already satisfied: multiprocess in /opt/conda/lib/python3.7/site-packages (from datasets) (0.70.11.1)\r\n",
      "Requirement already satisfied: filelock in /opt/conda/lib/python3.7/site-packages (from huggingface-hub<0.1.0->datasets) (3.0.12)\r\n",
      "Requirement already satisfied: idna<3,>=2.5 in /opt/conda/lib/python3.7/site-packages (from requests>=2.19.0->datasets) (2.10)\r\n",
      "Requirement already satisfied: chardet<5,>=3.0.2 in /opt/conda/lib/python3.7/site-packages (from requests>=2.19.0->datasets) (3.0.4)\r\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /opt/conda/lib/python3.7/site-packages (from requests>=2.19.0->datasets) (1.26.3)\r\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.7/site-packages (from requests>=2.19.0->datasets) (2020.12.5)\r\n",
      "Requirement already satisfied: zipp>=0.5 in /opt/conda/lib/python3.7/site-packages (from importlib-metadata->datasets) (3.4.0)\r\n",
      "Requirement already satisfied: typing-extensions>=3.6.4 in /opt/conda/lib/python3.7/site-packages (from importlib-metadata->datasets) (3.7.4.3)\r\n",
      "Requirement already satisfied: python-dateutil>=2.7.3 in /opt/conda/lib/python3.7/site-packages (from pandas->datasets) (2.8.1)\r\n",
      "Requirement already satisfied: pytz>=2017.2 in /opt/conda/lib/python3.7/site-packages (from pandas->datasets) (2021.1)\r\n",
      "Requirement already satisfied: six>=1.5 in /opt/conda/lib/python3.7/site-packages (from python-dateutil>=2.7.3->pandas->datasets) (1.15.0)\r\n",
      "Installing collected packages: tqdm, xxhash, huggingface-hub, datasets\r\n",
      "  Attempting uninstall: tqdm\r\n",
      "    Found existing installation: tqdm 4.56.2\r\n",
      "    Uninstalling tqdm-4.56.2:\r\n",
      "      Successfully uninstalled tqdm-4.56.2\r\n",
      "Successfully installed datasets-1.5.0 huggingface-hub-0.0.7 tqdm-4.49.0 xxhash-2.0.0\r\n",
      "Processing /kaggle/input/coleridge-packages/seqeval-1.2.2-py3-none-any.whl\r\n",
      "Requirement already satisfied: numpy>=1.14.0 in /opt/conda/lib/python3.7/site-packages (from seqeval==1.2.2) (1.19.5)\r\n",
      "Requirement already satisfied: scikit-learn>=0.21.3 in /opt/conda/lib/python3.7/site-packages (from seqeval==1.2.2) (0.24.1)\r\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in /opt/conda/lib/python3.7/site-packages (from scikit-learn>=0.21.3->seqeval==1.2.2) (2.1.0)\r\n",
      "Requirement already satisfied: scipy>=0.19.1 in /opt/conda/lib/python3.7/site-packages (from scikit-learn>=0.21.3->seqeval==1.2.2) (1.5.4)\r\n",
      "Requirement already satisfied: joblib>=0.11 in /opt/conda/lib/python3.7/site-packages (from scikit-learn>=0.21.3->seqeval==1.2.2) (1.0.1)\r\n",
      "Installing collected packages: seqeval\r\n",
      "Successfully installed seqeval-1.2.2\r\n",
      "Processing /kaggle/input/coleridge-packages/tokenizers-0.10.1-cp37-cp37m-manylinux1_x86_64.whl\r\n",
      "tokenizers is already installed with the same version as the provided wheel. Use --force-reinstall to force an installation of the wheel.\r\n",
      "Processing /kaggle/input/coleridge-packages/transformers-4.5.0.dev0-py3-none-any.whl\r\n",
      "Requirement already satisfied: requests in /opt/conda/lib/python3.7/site-packages (from transformers==4.5.0.dev0) (2.25.1)\r\n",
      "Requirement already satisfied: sacremoses in /opt/conda/lib/python3.7/site-packages (from transformers==4.5.0.dev0) (0.0.43)\r\n",
      "Requirement already satisfied: tokenizers<0.11,>=0.10.1 in /opt/conda/lib/python3.7/site-packages (from transformers==4.5.0.dev0) (0.10.1)\r\n",
      "Requirement already satisfied: tqdm>=4.27 in /opt/conda/lib/python3.7/site-packages (from transformers==4.5.0.dev0) (4.49.0)\r\n",
      "Requirement already satisfied: filelock in /opt/conda/lib/python3.7/site-packages (from transformers==4.5.0.dev0) (3.0.12)\r\n",
      "Requirement already satisfied: regex!=2019.12.17 in /opt/conda/lib/python3.7/site-packages (from transformers==4.5.0.dev0) (2020.11.13)\r\n",
      "Requirement already satisfied: numpy>=1.17 in /opt/conda/lib/python3.7/site-packages (from transformers==4.5.0.dev0) (1.19.5)\r\n",
      "Requirement already satisfied: packaging in /opt/conda/lib/python3.7/site-packages (from transformers==4.5.0.dev0) (20.9)\r\n",
      "Requirement already satisfied: importlib-metadata in /opt/conda/lib/python3.7/site-packages (from transformers==4.5.0.dev0) (3.4.0)\r\n",
      "Requirement already satisfied: zipp>=0.5 in /opt/conda/lib/python3.7/site-packages (from importlib-metadata->transformers==4.5.0.dev0) (3.4.0)\r\n",
      "Requirement already satisfied: typing-extensions>=3.6.4 in /opt/conda/lib/python3.7/site-packages (from importlib-metadata->transformers==4.5.0.dev0) (3.7.4.3)\r\n",
      "Requirement already satisfied: pyparsing>=2.0.2 in /opt/conda/lib/python3.7/site-packages (from packaging->transformers==4.5.0.dev0) (2.4.7)\r\n",
      "Requirement already satisfied: idna<3,>=2.5 in /opt/conda/lib/python3.7/site-packages (from requests->transformers==4.5.0.dev0) (2.10)\r\n",
      "Requirement already satisfied: chardet<5,>=3.0.2 in /opt/conda/lib/python3.7/site-packages (from requests->transformers==4.5.0.dev0) (3.0.4)\r\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.7/site-packages (from requests->transformers==4.5.0.dev0) (2020.12.5)\r\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /opt/conda/lib/python3.7/site-packages (from requests->transformers==4.5.0.dev0) (1.26.3)\r\n",
      "Requirement already satisfied: six in /opt/conda/lib/python3.7/site-packages (from sacremoses->transformers==4.5.0.dev0) (1.15.0)\r\n",
      "Requirement already satisfied: joblib in /opt/conda/lib/python3.7/site-packages (from sacremoses->transformers==4.5.0.dev0) (1.0.1)\r\n",
      "Requirement already satisfied: click in /opt/conda/lib/python3.7/site-packages (from sacremoses->transformers==4.5.0.dev0) (7.1.2)\r\n",
      "Installing collected packages: transformers\r\n",
      "  Attempting uninstall: transformers\r\n",
      "    Found existing installation: transformers 4.4.2\r\n",
      "    Uninstalling transformers-4.4.2:\r\n",
      "      Successfully uninstalled transformers-4.4.2\r\n",
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\r\n",
      "allennlp 2.2.0 requires transformers<4.5,>=4.1, but you have transformers 4.5.0.dev0 which is incompatible.\u001b[0m\r\n",
      "Successfully installed transformers-4.5.0.dev0\r\n",
      "Requirement already satisfied: datasets in /opt/conda/lib/python3.7/site-packages (1.5.0)\r\n",
      "Requirement already satisfied: pyarrow>=0.17.1 in /opt/conda/lib/python3.7/site-packages (from datasets) (1.0.1)\r\n",
      "Requirement already satisfied: xxhash in /opt/conda/lib/python3.7/site-packages (from datasets) (2.0.0)\r\n",
      "Requirement already satisfied: multiprocess in /opt/conda/lib/python3.7/site-packages (from datasets) (0.70.11.1)\r\n",
      "Requirement already satisfied: requests>=2.19.0 in /opt/conda/lib/python3.7/site-packages (from datasets) (2.25.1)\r\n",
      "Requirement already satisfied: pandas in /opt/conda/lib/python3.7/site-packages (from datasets) (1.1.5)\r\n",
      "Requirement already satisfied: tqdm<4.50.0,>=4.27 in /opt/conda/lib/python3.7/site-packages (from datasets) (4.49.0)\r\n",
      "Requirement already satisfied: huggingface-hub<0.1.0 in /opt/conda/lib/python3.7/site-packages (from datasets) (0.0.7)\r\n",
      "Requirement already satisfied: dill in /opt/conda/lib/python3.7/site-packages (from datasets) (0.3.3)\r\n",
      "Requirement already satisfied: importlib-metadata in /opt/conda/lib/python3.7/site-packages (from datasets) (3.4.0)\r\n",
      "Requirement already satisfied: fsspec in /opt/conda/lib/python3.7/site-packages (from datasets) (0.8.5)\r\n",
      "Requirement already satisfied: numpy>=1.17 in /opt/conda/lib/python3.7/site-packages (from datasets) (1.19.5)\r\n",
      "Requirement already satisfied: filelock in /opt/conda/lib/python3.7/site-packages (from huggingface-hub<0.1.0->datasets) (3.0.12)\r\n",
      "Requirement already satisfied: idna<3,>=2.5 in /opt/conda/lib/python3.7/site-packages (from requests>=2.19.0->datasets) (2.10)\r\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /opt/conda/lib/python3.7/site-packages (from requests>=2.19.0->datasets) (1.26.3)\r\n",
      "Requirement already satisfied: chardet<5,>=3.0.2 in /opt/conda/lib/python3.7/site-packages (from requests>=2.19.0->datasets) (3.0.4)\r\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.7/site-packages (from requests>=2.19.0->datasets) (2020.12.5)\r\n",
      "Requirement already satisfied: zipp>=0.5 in /opt/conda/lib/python3.7/site-packages (from importlib-metadata->datasets) (3.4.0)\r\n",
      "Requirement already satisfied: typing-extensions>=3.6.4 in /opt/conda/lib/python3.7/site-packages (from importlib-metadata->datasets) (3.7.4.3)\r\n",
      "Requirement already satisfied: python-dateutil>=2.7.3 in /opt/conda/lib/python3.7/site-packages (from pandas->datasets) (2.8.1)\r\n",
      "Requirement already satisfied: pytz>=2017.2 in /opt/conda/lib/python3.7/site-packages (from pandas->datasets) (2021.1)\r\n",
      "Requirement already satisfied: six>=1.5 in /opt/conda/lib/python3.7/site-packages (from python-dateutil>=2.7.3->pandas->datasets) (1.15.0)\r\n"
     ]
    }
   ],
   "source": [
    "!pip install datasets --no-index --find-links=file:///kaggle/input/coleridge-packages/packages/datasets \n",
    "!pip install ../input/coleridge-packages/seqeval-1.2.2-py3-none-any.whl \n",
    "!pip install ../input/coleridge-packages/tokenizers-0.10.1-cp37-cp37m-manylinux1_x86_64.whl \n",
    "!pip install ../input/coleridge-packages/transformers-4.5.0.dev0-py3-none-any.whl \n",
    "!pip install datasets "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-06-13T09:22:59.984148Z",
     "iopub.status.busy": "2021-06-13T09:22:59.983370Z",
     "iopub.status.idle": "2021-06-13T09:23:10.548414Z",
     "shell.execute_reply": "2021-06-13T09:23:10.547399Z",
     "shell.execute_reply.started": "2021-06-13T09:01:34.817355Z"
    },
    "papermill": {
     "duration": 10.610904,
     "end_time": "2021-06-13T09:23:10.548552",
     "exception": false,
     "start_time": "2021-06-13T09:22:59.937648",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.7/site-packages/torchaudio/backend/utils.py:54: UserWarning: \"sox\" backend is being deprecated. The default backend will be changed to \"sox_io\" backend in 0.8.0 and \"sox\" backend will be removed in 0.9.0. Please migrate to \"sox_io\" backend. Please refer to https://github.com/pytorch/audio/issues/903 for the detail.\n",
      "  '\"sox\" backend is being deprecated. '\n"
     ]
    }
   ],
   "source": [
    "#Import necessary libraries\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "import nltk\n",
    "import re\n",
    "import os\n",
    "from os import listdir\n",
    "from os.path import isfile, join\n",
    "import re\n",
    "import json\n",
    "import time\n",
    "import datetime\n",
    "import random\n",
    "import glob\n",
    "import importlib\n",
    "import allennlp\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from transformers import *\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import torch\n",
    "from typing import List\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-06-13T09:23:10.633466Z",
     "iopub.status.busy": "2021-06-13T09:23:10.632934Z",
     "iopub.status.idle": "2021-06-13T09:23:11.360591Z",
     "shell.execute_reply": "2021-06-13T09:23:11.359505Z",
     "shell.execute_reply.started": "2021-06-13T09:01:34.832603Z"
    },
    "papermill": {
     "duration": 0.771925,
     "end_time": "2021-06-13T09:23:11.360719",
     "exception": false,
     "start_time": "2021-06-13T09:23:10.588794",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 95394 rows in the training set!\n"
     ]
    }
   ],
   "source": [
    "acc = 0\n",
    "#Sanity check\n",
    "with open('/kaggle/input/tdmsci/train_ner.json') as f:\n",
    "    for row in f:\n",
    "        acc += 1\n",
    "\n",
    "print(\"There are {} rows in the training set!\".format(acc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-06-13T09:23:11.449961Z",
     "iopub.status.busy": "2021-06-13T09:23:11.449402Z",
     "iopub.status.idle": "2021-06-13T09:23:11.461105Z",
     "shell.execute_reply": "2021-06-13T09:23:11.460617Z",
     "shell.execute_reply.started": "2021-06-13T09:04:14.260234Z"
    },
    "papermill": {
     "duration": 0.059611,
     "end_time": "2021-06-13T09:23:11.461219",
     "exception": false,
     "start_time": "2021-06-13T09:23:11.401608",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "#Initialize paths for data\n",
    "path_abs = '/kaggle/input/coleridgeinitiative-show-us-the-data/'\n",
    "path_test = os.path.join(path_abs, 'test/')\n",
    "path_train = os.path.join(path_abs, 'train/')\n",
    "path_metadata = os.path.join(path_abs, 'train.csv')\n",
    "path_ner_json = '/kaggle/input/tdmsci/train_ner.json'\n",
    "sample_submission_path = '../input/coleridgeinitiative-show-us-the-data/sample_submission.csv'\n",
    "sample_submission = pd.read_csv(sample_submission_path)\n",
    "\n",
    "adnl_govt_labels_path = '../input/coleridge-additional-gov-datasets-22000popular/data_set_800_with10000popular.csv'\n",
    "\n",
    "onlyBert = False #CHANGE THIS TO FALSE FOR FINAL SUBMSSION "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.04036,
     "end_time": "2021-06-13T09:23:11.542220",
     "exception": false,
     "start_time": "2021-06-13T09:23:11.501860",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# 3. Get to know the data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.039856,
     "end_time": "2021-06-13T09:23:11.622396",
     "exception": false,
     "start_time": "2021-06-13T09:23:11.582540",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## 3.1. Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-06-13T09:23:11.709978Z",
     "iopub.status.busy": "2021-06-13T09:23:11.709423Z",
     "iopub.status.idle": "2021-06-13T09:23:12.166750Z",
     "shell.execute_reply": "2021-06-13T09:23:12.166296Z",
     "shell.execute_reply.started": "2021-06-13T09:04:14.707880Z"
    },
    "papermill": {
     "duration": 0.504112,
     "end_time": "2021-06-13T09:23:12.166882",
     "exception": false,
     "start_time": "2021-06-13T09:23:11.662770",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Id</th>\n",
       "      <th>pub_title</th>\n",
       "      <th>dataset_title</th>\n",
       "      <th>dataset_label</th>\n",
       "      <th>cleaned_label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0007f880-0a9b-492d-9a58-76eb0b0e0bd7</td>\n",
       "      <td>The Impact of ICT Training on Income Generatio...</td>\n",
       "      <td>Program for the International Assessment of Ad...</td>\n",
       "      <td>Program for the International Assessment of Ad...</td>\n",
       "      <td>program for the international assessment of ad...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0008656f-0ba2-4632-8602-3017b44c2e90</td>\n",
       "      <td>Finnish Ninth Graders’ Gender Appropriateness ...</td>\n",
       "      <td>Trends in International Mathematics and Scienc...</td>\n",
       "      <td>Trends in International Mathematics and Scienc...</td>\n",
       "      <td>trends in international mathematics and scienc...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>000e04d6-d6ef-442f-b070-4309493221ba</td>\n",
       "      <td>Economic Research Service: Specialized Agency...</td>\n",
       "      <td>Agricultural Resource Management Survey</td>\n",
       "      <td>Agricultural Resources Management Survey</td>\n",
       "      <td>agricultural resources management survey</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>000efc17-13d8-433d-8f62-a3932fe4f3b8</td>\n",
       "      <td>Risk factors and global cognitive status relat...</td>\n",
       "      <td>Alzheimer's Disease Neuroimaging Initiative (A...</td>\n",
       "      <td>ADNI|Alzheimer's Disease Neuroimaging Initiati...</td>\n",
       "      <td>adni|alzheimer s disease neuroimaging initiati...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0010357a-6365-4e5f-b982-582e6d32c3ee</td>\n",
       "      <td>Timelines of COVID-19 Vaccines</td>\n",
       "      <td>SARS-CoV-2 genome sequence</td>\n",
       "      <td>genome sequence of COVID-19</td>\n",
       "      <td>genome sequence of covid 19</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                     Id  \\\n",
       "0  0007f880-0a9b-492d-9a58-76eb0b0e0bd7   \n",
       "1  0008656f-0ba2-4632-8602-3017b44c2e90   \n",
       "2  000e04d6-d6ef-442f-b070-4309493221ba   \n",
       "3  000efc17-13d8-433d-8f62-a3932fe4f3b8   \n",
       "4  0010357a-6365-4e5f-b982-582e6d32c3ee   \n",
       "\n",
       "                                           pub_title  \\\n",
       "0  The Impact of ICT Training on Income Generatio...   \n",
       "1  Finnish Ninth Graders’ Gender Appropriateness ...   \n",
       "2   Economic Research Service: Specialized Agency...   \n",
       "3  Risk factors and global cognitive status relat...   \n",
       "4                     Timelines of COVID-19 Vaccines   \n",
       "\n",
       "                                       dataset_title  \\\n",
       "0  Program for the International Assessment of Ad...   \n",
       "1  Trends in International Mathematics and Scienc...   \n",
       "2            Agricultural Resource Management Survey   \n",
       "3  Alzheimer's Disease Neuroimaging Initiative (A...   \n",
       "4                         SARS-CoV-2 genome sequence   \n",
       "\n",
       "                                       dataset_label  \\\n",
       "0  Program for the International Assessment of Ad...   \n",
       "1  Trends in International Mathematics and Scienc...   \n",
       "2           Agricultural Resources Management Survey   \n",
       "3  ADNI|Alzheimer's Disease Neuroimaging Initiati...   \n",
       "4                        genome sequence of COVID-19   \n",
       "\n",
       "                                       cleaned_label  \n",
       "0  program for the international assessment of ad...  \n",
       "1  trends in international mathematics and scienc...  \n",
       "2           agricultural resources management survey  \n",
       "3  adni|alzheimer s disease neuroimaging initiati...  \n",
       "4                        genome sequence of covid 19  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Load metadata\n",
    "label_df = pd.read_csv(path_metadata)\n",
    "label_df = label_df.groupby('Id').agg({\n",
    "    'pub_title': 'first',\n",
    "    'dataset_title': '|'.join,\n",
    "    'dataset_label': '|'.join,\n",
    "    'cleaned_label': '|'.join\n",
    "}).reset_index()\n",
    "\n",
    "# sample_submission = pd.read_csv(path_sample_submission)\n",
    "# #Inpsect data\n",
    "\n",
    "label_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-06-13T09:23:12.253417Z",
     "iopub.status.busy": "2021-06-13T09:23:12.252897Z",
     "iopub.status.idle": "2021-06-13T09:23:12.256282Z",
     "shell.execute_reply": "2021-06-13T09:23:12.256670Z",
     "shell.execute_reply.started": "2021-06-13T09:04:15.211818Z"
    },
    "papermill": {
     "duration": 0.048818,
     "end_time": "2021-06-13T09:23:12.256828",
     "exception": false,
     "start_time": "2021-06-13T09:23:12.208010",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "#In case of development (using the validation set)\n",
    "computeFBeta = True\n",
    "if(len(sample_submission) > 4):\n",
    "    computeFBeta = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.040631,
     "end_time": "2021-06-13T09:23:12.338352",
     "exception": false,
     "start_time": "2021-06-13T09:23:12.297721",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# 4. Create testdata (and basic NLP)\n",
    "\n",
    "Here, we will load our provided test data and apply basic NLP methods on this. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-06-13T09:23:12.435394Z",
     "iopub.status.busy": "2021-06-13T09:23:12.433955Z",
     "iopub.status.idle": "2021-06-13T09:23:12.436467Z",
     "shell.execute_reply": "2021-06-13T09:23:12.436881Z",
     "shell.execute_reply.started": "2021-06-13T09:04:15.220685Z"
    },
    "papermill": {
     "duration": 0.057322,
     "end_time": "2021-06-13T09:23:12.437009",
     "exception": false,
     "start_time": "2021-06-13T09:23:12.379687",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "MAX_LENGTH = 64\n",
    "OVERLAP = 20\n",
    "\n",
    "def clean_paper_sentence(s):\n",
    "   \"\"\"\n",
    "    Arguments: sentence (string), Returns: sentence (string)\n",
    "    This function is essentially clean_text without lowercasing.\n",
    "    \"\"\"\n",
    "    s = re.sub('[^A-Za-z0-9]+', ' ', str(s)).strip()\n",
    "    s = re.sub(' +', ' ', s)\n",
    "    return s\n",
    "\n",
    "def shorten_sentences(sentences):\n",
    "    \"\"\"\n",
    "    Arguments: sentences (list), Returns: short_sentences (list)\n",
    "    \n",
    "    Sentences that have more than MAX_LENGTH words will be split\n",
    "    into multiple sentences with overlappings.\n",
    "    \"\"\"\n",
    "    short_sentences = []\n",
    "    for sentence in sentences:\n",
    "        words = sentence.split()\n",
    "        if len(words) > MAX_LENGTH:\n",
    "            for p in range(0, len(words), MAX_LENGTH - OVERLAP):\n",
    "                short_sentences.append(' '.join(words[p:p+MAX_LENGTH]))\n",
    "        else:\n",
    "            short_sentences.append(sentence)\n",
    "    return short_sentences\n",
    "\n",
    "#Concatenate text and split sentences, as the BERT (and SciBERT) model \n",
    "#has the contraint of maximum sequence length of 512.\n",
    "def concatenate_text(json_dict):  \n",
    "    '''\n",
    "    Arguments: json_dict (dictionary), Returns: sentences (list)\n",
    "    \n",
    "    Concatenate text and split sentences, as the BERT (and SciBERT) model\n",
    "    has the contraint of maximum sequence length of 512.\n",
    "    '''\n",
    "    total_text = \"\"\n",
    "        \n",
    "    for section_dict in json_dict:\n",
    "        total_text += section_dict['text']+ '\\n'\n",
    "    #sentences = nltk.tokenize.sent_tokenize(total_text) # This seems to take a lot of time?\n",
    "    sentences = re.split('\\. ', total_text)\n",
    "    sentences = [clean_paper_sentence(s) for s in sentences]\n",
    "   \n",
    "    sentences = shorten_sentences(sentences)\n",
    "    return sentences\n",
    "\n",
    "def create_test_df(path, dataset, N_test):\n",
    "     '''\n",
    "    Arguments: path (filnemae), dataset (string), N_test (itn)\n",
    "    Returns: df (dictionary)\n",
    "    \n",
    "    Create dataframe of the provided testdata \n",
    "    '''\n",
    "    #Initialize dictionary with right keys\n",
    "    final_dict = dict()\n",
    "    final_dict[\"Id\"] = []\n",
    "    final_dict[\"Sentences\"] = []\n",
    "    \n",
    "    max_length = N_test\n",
    "    \n",
    "    counter = 0\n",
    "    for root, _, files in os.walk(path):\n",
    "        for filename in range(0,max_length):#files:\n",
    "            id = files[filename][:-5] #Remove .json from filename to retrieve id\n",
    "            with open(path + files[filename]) as f:\n",
    "                json_dict = json.load(f)\n",
    "                sentences = concatenate_text(json_dict)\n",
    "                final_dict[\"Id\"].append(id)\n",
    "                final_dict[\"Sentences\"].append(sentences)\n",
    "                \n",
    "            counter += 1\n",
    "    df = pd.DataFrame.from_dict(final_dict)\n",
    "\n",
    "    return df\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-06-13T09:23:12.523598Z",
     "iopub.status.busy": "2021-06-13T09:23:12.523102Z",
     "iopub.status.idle": "2021-06-13T09:23:12.602319Z",
     "shell.execute_reply": "2021-06-13T09:23:12.601882Z",
     "shell.execute_reply.started": "2021-06-13T09:04:15.290853Z"
    },
    "papermill": {
     "duration": 0.124618,
     "end_time": "2021-06-13T09:23:12.602432",
     "exception": false,
     "start_time": "2021-06-13T09:23:12.477814",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4\n"
     ]
    }
   ],
   "source": [
    "\n",
    "#Create train test data for NER classification\n",
    "if(computeFBeta):\n",
    "    N_test = len([name for name in os.listdir(path_test)])\n",
    "    print(N_test)\n",
    "    test_df_ner = create_test_df(path_test, \"test\", N_test)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.041168,
     "end_time": "2021-06-13T09:23:12.939520",
     "exception": false,
     "start_time": "2021-06-13T09:23:12.898352",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# 5. Literal matching\n",
    "To improve performance, we introduce literal matching, where sentences from the test data are matched against a created knowledge bank. Here, literal matches between datasets in the knowledge bank and sentences in the test data are found. \n",
    "\n",
    "Furthermore, we add the top [10,000 datasets](https://www.kaggle.com/chienhsianghung/coleridge-additional-gov-datasets-22000popular) from the [U.S. Government's open data](https://www.data.gov/). To exclude datasets exiting of one word - mostly a common used word like \"earth\" - we restricted the inclusion of a dataset with a wordcount larger than 3."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-06-13T09:23:13.028913Z",
     "iopub.status.busy": "2021-06-13T09:23:13.028126Z",
     "iopub.status.idle": "2021-06-13T09:24:09.830061Z",
     "shell.execute_reply": "2021-06-13T09:24:09.830473Z",
     "shell.execute_reply.started": "2021-06-13T09:04:15.964716Z"
    },
    "papermill": {
     "duration": 56.849759,
     "end_time": "2021-06-13T09:24:09.830625",
     "exception": false,
     "start_time": "2021-06-13T09:23:12.980866",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "14316\n"
     ]
    }
   ],
   "source": [
    "#Literal matching (create knowledge bank)\n",
    "train = pd.read_csv(path_metadata)\n",
    "papers = {}\n",
    "\n",
    "for paper_id in train['Id'].unique():\n",
    "    with open(f'{path_train}/{paper_id}.json', 'r') as f:\n",
    "        paper = json.load(f)\n",
    "        papers[paper_id] = paper\n",
    "\n",
    "print(len(list(papers.keys())))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-06-13T09:24:09.925628Z",
     "iopub.status.busy": "2021-06-13T09:24:09.925046Z",
     "iopub.status.idle": "2021-06-13T09:24:09.936477Z",
     "shell.execute_reply": "2021-06-13T09:24:09.935990Z",
     "shell.execute_reply.started": "2021-06-13T09:05:17.660380Z"
    },
    "papermill": {
     "duration": 0.063474,
     "end_time": "2021-06-13T09:24:09.936584",
     "exception": false,
     "start_time": "2021-06-13T09:24:09.873110",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "sample_submission_path = '../input/coleridgeinitiative-show-us-the-data/sample_submission.csv'\n",
    "sample_submission = pd.read_csv(sample_submission_path)\n",
    "paper_test_folder = '../input/coleridgeinitiative-show-us-the-data/test'\n",
    "for paper_id in sample_submission['Id']:\n",
    "    with open(f'{paper_test_folder}/{paper_id}.json', 'r') as f:\n",
    "        paper = json.load(f)\n",
    "        papers[paper_id] = paper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-06-13T09:24:10.030065Z",
     "iopub.status.busy": "2021-06-13T09:24:10.029410Z",
     "iopub.status.idle": "2021-06-13T09:24:10.256983Z",
     "shell.execute_reply": "2021-06-13T09:24:10.256190Z",
     "shell.execute_reply.started": "2021-06-13T09:05:17.682686Z"
    },
    "papermill": {
     "duration": 0.277796,
     "end_time": "2021-06-13T09:24:10.257114",
     "exception": false,
     "start_time": "2021-06-13T09:24:09.979318",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No. different labels: 12230\n"
     ]
    }
   ],
   "source": [
    "all_labels = set()\n",
    "\n",
    "for label_1, label_2, label_3 in train[['dataset_title', 'dataset_label', 'cleaned_label']].itertuples(index=False):\n",
    "    all_labels.add(str(label_1).lower())\n",
    "    all_labels.add(str(label_2).lower())\n",
    "    all_labels.add(str(label_3).lower())\n",
    "\n",
    "adnl_govt_labels = pd.read_csv(adnl_govt_labels_path)\n",
    "\n",
    "for l in adnl_govt_labels.title:\n",
    "    \n",
    "    if (len(l.split()) > 3):\n",
    "        all_labels.add(l.lower())\n",
    "        all_labels.add(l)\n",
    "        all_labels.add(clean_paper_sentence(l))\n",
    "    \n",
    "    \n",
    "all_labels = set(all_labels)\n",
    "print(f'No. different labels: {len(all_labels)}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.043751,
     "end_time": "2021-06-13T09:24:10.345653",
     "exception": false,
     "start_time": "2021-06-13T09:24:10.301902",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# 5. NER with SciBERT\n",
    "\n",
    "Here we apply NER on the pretrained model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-06-13T09:24:10.524509Z",
     "iopub.status.busy": "2021-06-13T09:24:10.522866Z",
     "iopub.status.idle": "2021-06-13T09:24:10.525201Z",
     "shell.execute_reply": "2021-06-13T09:24:10.525649Z",
     "shell.execute_reply.started": "2021-06-13T09:05:17.955977Z"
    },
    "papermill": {
     "duration": 0.051471,
     "end_time": "2021-06-13T09:24:10.525801",
     "exception": false,
     "start_time": "2021-06-13T09:24:10.474330",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def clean_text(txt):\n",
    "    \"\"\"\n",
    "    Input: txt (string), Returns: txt_cleaned (string)\n",
    "    This function is essentially clean_text without lowercasing.\n",
    "    \"\"\"\n",
    "    return re.sub('[^A-Za-z0-9]+', ' ', str(txt).lower()).strip()\n",
    "\n",
    "def totally_clean_text(txt):\n",
    "    txt = clean_text(txt)\n",
    "    txt = re.sub(' +', ' ', txt)\n",
    "    return txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.042401,
     "end_time": "2021-06-13T09:24:10.610763",
     "exception": false,
     "start_time": "2021-06-13T09:24:10.568362",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## 5.1. Test SciBERT model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-06-13T09:24:10.704903Z",
     "iopub.status.busy": "2021-06-13T09:24:10.704275Z",
     "iopub.status.idle": "2021-06-13T09:24:44.867366Z",
     "shell.execute_reply": "2021-06-13T09:24:44.868328Z",
     "shell.execute_reply.started": "2021-06-13T09:05:17.969350Z"
    },
    "papermill": {
     "duration": 34.214638,
     "end_time": "2021-06-13T09:24:44.868555",
     "exception": false,
     "start_time": "2021-06-13T09:24:10.653917",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "literal_preds = []\n",
    "\n",
    "#In case of submission\n",
    "if(not computeFBeta):\n",
    "    for paper_id in sample_submission['Id']:\n",
    "        paper = papers[paper_id]\n",
    "        text_1 = '. '.join(section['text'] for section in paper).lower()\n",
    "        text_2 = totally_clean_text(text_1)\n",
    "\n",
    "        labels = set()\n",
    "        for label in all_labels:\n",
    "            if label in text_1 or label in text_2:\n",
    "                labels.add(clean_text(label))\n",
    "\n",
    "        literal_preds.append('|'.join(labels))\n",
    "#In case of development (using validation set)\n",
    "else:\n",
    "      first_100_papers = sorted(os.listdir(path_train))[0:100]\n",
    "  \n",
    "      for papername in first_100_papers:\n",
    "        with open(f'{path_train}{papername}', 'r') as f:\n",
    "            paper = json.load(f)\n",
    "       \n",
    "        text_1 = '. '.join(section['text'] for section in paper).lower()\n",
    "        text_2 = totally_clean_text(text_1)\n",
    "\n",
    "        labels = set()\n",
    "        for label in all_labels:\n",
    "            if label in text_1 or label in text_2:\n",
    "                labels.add(clean_text(label))\n",
    "\n",
    "        literal_preds.append('|'.join(labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-06-13T09:24:45.008814Z",
     "iopub.status.busy": "2021-06-13T09:24:45.007980Z",
     "iopub.status.idle": "2021-06-13T09:24:45.011308Z",
     "shell.execute_reply": "2021-06-13T09:24:45.009467Z",
     "shell.execute_reply.started": "2021-06-13T09:05:55.596827Z"
    },
    "papermill": {
     "duration": 0.075754,
     "end_time": "2021-06-13T09:24:45.011461",
     "exception": false,
     "start_time": "2021-06-13T09:24:44.935707",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['program for the international assessment of adult competencies',\n",
       " 'trends in international mathematics and science study',\n",
       " 'agricultural resources management survey',\n",
       " 'adni|alzheimer s disease neuroimaging initiative adni',\n",
       " 'genome sequence of covid 19']"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "literal_preds[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6.2. Preprocess test data\n",
    "\n",
    "After applying literal matching, we preprocess our test data by applying the same basic NLP techniques to our test data. Furthermore, to make the input suitable for BERT, dummy tags are added."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-06-13T09:24:45.157537Z",
     "iopub.status.busy": "2021-06-13T09:24:45.156779Z",
     "iopub.status.idle": "2021-06-13T09:24:45.698162Z",
     "shell.execute_reply": "2021-06-13T09:24:45.697396Z",
     "shell.execute_reply.started": "2021-06-13T09:05:55.607366Z"
    },
    "papermill": {
     "duration": 0.621799,
     "end_time": "2021-06-13T09:24:45.698295",
     "exception": false,
     "start_time": "2021-06-13T09:24:45.076496",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No. grouped training rows: 14316\n"
     ]
    }
   ],
   "source": [
    "train = train.groupby('Id').agg({\n",
    "    'pub_title': 'first',\n",
    "    'dataset_title': '|'.join,\n",
    "    'dataset_label': '|'.join,\n",
    "    'cleaned_label': '|'.join\n",
    "}).reset_index()\n",
    "\n",
    "print(f'No. grouped training rows: {len(train)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-06-13T09:24:45.788989Z",
     "iopub.status.busy": "2021-06-13T09:24:45.788165Z",
     "iopub.status.idle": "2021-06-13T09:24:45.791114Z",
     "shell.execute_reply": "2021-06-13T09:24:45.790694Z",
     "shell.execute_reply.started": "2021-06-13T09:05:56.038963Z"
    },
    "papermill": {
     "duration": 0.050758,
     "end_time": "2021-06-13T09:24:45.791242",
     "exception": false,
     "start_time": "2021-06-13T09:24:45.740484",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def clean_training_text(txt):\n",
    "    \"\"\"\n",
    "    Input: sentence (string), Returns: sentence (string)\n",
    "    This function is essentially clean_text without lowercasing.\n",
    "    \"\"\"\n",
    "    return re.sub('[^A-Za-z0-9]+', ' ', str(txt)).strip()\n",
    "\n",
    "def shorten_sentences(sentences):\n",
    "     '''\n",
    "    Input: sentences (list), Returns: short_sentences (list)\n",
    "    \n",
    "    Sentences that have more than MAX_LENGTH words will be split\n",
    "    into multiple sentences with overlappings.\n",
    "    '''\n",
    "    short_sentences = []\n",
    "    for sentence in sentences:\n",
    "        words = sentence.split()\n",
    "        if len(words) > MAX_LENGTH:\n",
    "            for p in range(0, len(words), MAX_LENGTH - OVERLAP):\n",
    "                short_sentences.append(' '.join(words[p:p+MAX_LENGTH]))\n",
    "        else:\n",
    "            short_sentences.append(sentence)\n",
    "    return short_sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-06-13T09:24:45.888287Z",
     "iopub.status.busy": "2021-06-13T09:24:45.887541Z",
     "iopub.status.idle": "2021-06-13T09:24:45.890464Z",
     "shell.execute_reply": "2021-06-13T09:24:45.890045Z",
     "shell.execute_reply.started": "2021-06-13T09:05:56.050877Z"
    },
    "papermill": {
     "duration": 0.057277,
     "end_time": "2021-06-13T09:24:45.890570",
     "exception": false,
     "start_time": "2021-06-13T09:24:45.833293",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def preprocess_test_ner():\n",
    "     '''\n",
    "    Arugments: None, Returns: test_rows (list), paper_lenghts (list)\n",
    "    \n",
    "    Preprocesses test data for NER, by splitting sentences into words and addnign dummy tags.\n",
    "    For each paper, it store the amount of sentences to restore predicted labels back to words.\n",
    "    '''\n",
    "    test_rows = [] # test data in NER format\n",
    "    paper_lengths = []\n",
    "\n",
    "    for paper_id in sample_submission['Id']:\n",
    "       \n",
    "        paper = papers[paper_id]\n",
    "        \n",
    "        sentences = [clean_training_text(sentence) for section in paper \n",
    "                 for sentence in section['text'].split('.')\n",
    "                ]\n",
    "        #The author of this code does this, but I am not sure if this is necessary\n",
    "        sentences = [sentence for sentence in sentences if len(sentence) > 10] # only accept sentences with length > 10 chars\n",
    "        sentences = [sentence for sentence in sentences if any(word in sentence.lower() for word in ['data', 'study'])]\n",
    "        \n",
    "        #Add NER-labels to each token of each sentence\n",
    "        for sentence in sentences:\n",
    "            \n",
    "            tokens = sentence.split()\n",
    "            dummy_tags = ['O'] * len(tokens)\n",
    "            \n",
    "            test_rows.append({'tokens' : tokens, 'tags' : dummy_tags, 'id': paper_id})\n",
    "        paper_lengths.append(len(sentences))\n",
    "    \n",
    "\n",
    "\n",
    "    return test_rows, paper_lengths\n",
    "\n",
    "def preprocess_test_validation():\n",
    "    '''\n",
    "    Arguments: None, Returns: test_rows (list), paper_lengths (list),  ids (list)\n",
    "    \n",
    "    NB: In case of development (using validation set)\n",
    "    \n",
    "    Preprocesses test data for NER, by splitting sentences into words and addnign dummy tags.\n",
    "    For each paper, it store the amount of sentences to restore predicted labels back to words.\n",
    "    '''\n",
    "    #For testing purposes\n",
    "    test_rows = [] # test data in NER format\n",
    "    paper_lengths = []\n",
    "    ids = []\n",
    "\n",
    "    first_100_papers = sorted(os.listdir(path_train))[0:100]\n",
    "  \n",
    "    for papername in first_100_papers:\n",
    "        ids.append(papername[:-5])\n",
    "        with open(f'{path_train}{papername}', 'r') as f:\n",
    "            paper = json.load(f)\n",
    "          \n",
    "            #print(paper[\"id\"])\n",
    "            sentences = [clean_training_text(sentence) for section in paper \n",
    "                     for sentence in section['text'].split('.')\n",
    "                    ]\n",
    "            #The author of this code does this, but I am not sure if this is necessary\n",
    "            sentences = [sentence for sentence in sentences if len(sentence) > 10] # only accept sentences with length > 10 chars\n",
    "            sentences = [sentence for sentence in sentences if any(word in sentence.lower() for word in ['data', 'study'])]\n",
    "\n",
    "            #Add NER-labels to each token of each sentence\n",
    "            for sentence in sentences:\n",
    "\n",
    "                tokens = sentence.split()\n",
    "                dummy_tags = ['O'] * len(tokens)\n",
    "\n",
    "                test_rows.append({'tokens' : tokens, 'tags' : dummy_tags, 'id': paper_id})\n",
    "            paper_lengths.append(len(sentences))\n",
    "    return test_rows, paper_lengths, ids\n",
    "  \n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.041533,
     "end_time": "2021-06-13T09:24:45.974451",
     "exception": false,
     "start_time": "2021-06-13T09:24:45.932918",
     "status": "completed"
    },
    "tags": []
   },
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-06-13T09:24:46.063771Z",
     "iopub.status.busy": "2021-06-13T09:24:46.063261Z",
     "iopub.status.idle": "2021-06-13T09:24:46.490022Z",
     "shell.execute_reply": "2021-06-13T09:24:46.490441Z",
     "shell.execute_reply.started": "2021-06-13T09:05:56.072556Z"
    },
    "papermill": {
     "duration": 0.474096,
     "end_time": "2021-06-13T09:24:46.490591",
     "exception": false,
     "start_time": "2021-06-13T09:24:46.016495",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3079\n",
      "[{'tokens': ['The', 'aim', 'of', 'this', 'study', 'was', 'to', 'identify', 'if', 'acquiring', 'ICT', 'skills', 'through', 'DOT', 'Lebanon', 's', 'ICT', 'training', 'program', 'a', 'local', 'NGO', 'improved', 'income', 'generation', 'opportunities', 'after', '3', 'months', 'of', 'completing', 'the', 'training'], 'tags': ['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O'], 'id': '8e6996b4-ca08-4c0b-bed2-aaf07a4c6a60'}, {'tokens': ['This', 'study', 'was', 'completed', 'in', 'an', 'effort', 'to', 'find', 'creative', 'and', 'digital', 'solutions', 'to', 'the', 'high', 'rate', 'of', 'youth', 'unemployment', 'in', 'Lebanon', '37', 'one', 'of', 'the', 'highest', 'rates', 'in', 'the', 'world'], 'tags': ['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O'], 'id': '8e6996b4-ca08-4c0b-bed2-aaf07a4c6a60'}, {'tokens': ['According', 'to', 'a', 'study', 'completed', 'by', 'UNDP', '2016', '96', 'of', 'semi', 'skilled', 'workers', 'in', 'the', 'Agro', 'food', 'industry', 'suffer', 'from', 'basic', 'ICT', 'skills', 'weaknesses', '89'], 'tags': ['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O'], 'id': '8e6996b4-ca08-4c0b-bed2-aaf07a4c6a60'}, {'tokens': ['The', 'subject', 'of', 'this', 'study', 'was', 'one', 'of', 'DOT', 'Lebanon', 's', 'programs', 'Digital', 'Media', 'Literacy', 'Program', 'DML', 'a', '2', 'level', 'digital', 'youth', 'training', 'program', 'delivered', 'in', '14', 'different', 'centers', 'across', 'Lebanon'], 'tags': ['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O'], 'id': '8e6996b4-ca08-4c0b-bed2-aaf07a4c6a60'}, {'tokens': ['The', 'aim', 'of', 'this', 'study', 'was', 'to', 'identify', 'if', 'acquiring', 'ICT', 'skills', 'through', 'DOT', 'Lebanon', 's', 'ICT', 'training', 'program', 'improved', 'income', 'generation', 'opportunities', 'after', '3', 'months', 'for', 'DOT', 'Lebanon', 's', 'beneficiaries', 'enrolled', 'in', 'the', 'training', 'from', 'February', '2018', 'till', 'September', '2018'], 'tags': ['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O'], 'id': '8e6996b4-ca08-4c0b-bed2-aaf07a4c6a60'}]\n"
     ]
    }
   ],
   "source": [
    "#In case of submission\n",
    "if(not computeFBeta):\n",
    "    test_rows, paper_lengths = preprocess_test_ner()\n",
    "#In case of development (using validation set)   \n",
    "else:\n",
    "    test_rows, paper_lengths, validationIds = preprocess_test_validation()\n",
    "    \n",
    "print(len(test_rows))\n",
    "print(test_rows[0:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6.3. Predict with BERT\n",
    "\n",
    "Now we run the BERT model with the preloaded trained model on the test set and save the results in *bert\\_outputs*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-06-13T09:24:46.666490Z",
     "iopub.status.busy": "2021-06-13T09:24:46.665780Z",
     "iopub.status.idle": "2021-06-13T09:24:46.668259Z",
     "shell.execute_reply": "2021-06-13T09:24:46.668620Z",
     "shell.execute_reply.started": "2021-06-13T09:05:56.587328Z"
    },
    "papermill": {
     "duration": 0.051002,
     "end_time": "2021-06-13T09:24:46.668779",
     "exception": false,
     "start_time": "2021-06-13T09:24:46.617777",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "max_length = 64 # max no. words for each sentence.\n",
    "overlap = 20 # if a sentence exceeds MAX_LENGTH, we split it to multiple sentences with overlapping\n",
    "path_train_nerjson = '/kaggle/input/tdmsci/train_ner.json'\n",
    "pred_save_path = './pred'\n",
    "prediction_file = 'test_predictions.txt'\n",
    "test_input_save_path = './input_data'\n",
    "path_pretrained_scibert = '/kaggle/input/tdmsci/scibert/output' \n",
    "path_pretrained_bert =  '/kaggle/input/tdmsci/results/output' \n",
    "train_file = path_train_nerjson\n",
    "filename_test = 'test_ner_input.json'\n",
    "\n",
    "os.environ[\"MODEL_PATH\"] = f\"{path_pretrained_scibert}\"\n",
    "os.environ[\"TRAIN_FILE\"] = f\"{train_file}\"\n",
    "os.environ[\"VALIDATION_FILE\"] = f\"{train_file}\"\n",
    "os.environ[\"TEST_FILE\"] = f\"{test_input_save_path}/{filename_test}\"\n",
    "os.environ[\"OUTPUT_DIR\"] = f\"{pred_save_path}\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-06-13T09:24:46.759000Z",
     "iopub.status.busy": "2021-06-13T09:24:46.758229Z",
     "iopub.status.idle": "2021-06-13T09:24:47.422374Z",
     "shell.execute_reply": "2021-06-13T09:24:47.421630Z",
     "shell.execute_reply.started": "2021-06-13T09:05:56.600471Z"
    },
    "papermill": {
     "duration": 0.711264,
     "end_time": "2021-06-13T09:24:47.422511",
     "exception": false,
     "start_time": "2021-06-13T09:24:46.711247",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# copy my_seqeval.py to the working directory because the input directory is non-writable\n",
    "!cp /kaggle/input/coleridge-packages/my_seqeval.py ./\n",
    "os.makedirs(test_input_save_path, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-06-13T09:24:47.513631Z",
     "iopub.status.busy": "2021-06-13T09:24:47.512694Z",
     "iopub.status.idle": "2021-06-13T09:24:47.515638Z",
     "shell.execute_reply": "2021-06-13T09:24:47.515238Z",
     "shell.execute_reply.started": "2021-06-13T09:05:57.414884Z"
    },
    "papermill": {
     "duration": 0.050705,
     "end_time": "2021-06-13T09:24:47.515768",
     "exception": false,
     "start_time": "2021-06-13T09:24:47.465063",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def predict_scibert_ner():\n",
    "    !python ../input/kaggle-ner-utils/kaggle_run_ner.py \\\n",
    "    --model_name_or_path \"$MODEL_PATH\" \\\n",
    "    --train_file \"$TRAIN_FILE\" \\\n",
    "    --validation_file \"$VALIDATION_FILE\" \\\n",
    "    --test_file \"$TEST_FILE\" \\\n",
    "    --output_dir \"$OUTPUT_DIR\" \\\n",
    "    --report_to 'none' \\\n",
    "    --seed 123 \\\n",
    "    --do_predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-06-13T09:24:47.700480Z",
     "iopub.status.busy": "2021-06-13T09:24:47.699669Z",
     "iopub.status.idle": "2021-06-13T09:25:55.511586Z",
     "shell.execute_reply": "2021-06-13T09:25:55.510428Z",
     "shell.execute_reply.started": "2021-06-13T09:05:57.436688Z"
    },
    "papermill": {
     "duration": 67.863095,
     "end_time": "2021-06-13T09:25:55.511786",
     "exception": false,
     "start_time": "2021-06-13T09:24:47.648691",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3079\n",
      "rm: cannot remove './pred': No such file or directory\r\n",
      "2021-06-13 09:24:51.117938: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudart.so.10.2\r\n",
      "Downloading and preparing dataset json/default (download: Unknown size, generated: Unknown size, post-processed: Unknown size, total: Unknown size) to /root/.cache/huggingface/datasets/json/default-8e160473283bb99f/0.0.0/83d5b3a2f62630efc6b5315f00f20209b4ad91a00ac586597caee3a4da0bef02...\r\n",
      "Dataset json downloaded and prepared to /root/.cache/huggingface/datasets/json/default-8e160473283bb99f/0.0.0/83d5b3a2f62630efc6b5315f00f20209b4ad91a00ac586597caee3a4da0bef02. Subsequent calls will reuse this data.\r\n",
      "[INFO|configuration_utils.py:470] 2021-06-13 09:25:19,286 >> loading configuration file /kaggle/input/tdmsci/scibert/output/config.json\r\n",
      "[INFO|configuration_utils.py:508] 2021-06-13 09:25:19,287 >> Model config BertConfig {\r\n",
      "  \"_name_or_path\": \"allenai/scibert_scivocab_cased\",\r\n",
      "  \"architectures\": [\r\n",
      "    \"BertForTokenClassification\"\r\n",
      "  ],\r\n",
      "  \"attention_probs_dropout_prob\": 0.1,\r\n",
      "  \"finetuning_task\": \"ner\",\r\n",
      "  \"gradient_checkpointing\": false,\r\n",
      "  \"hidden_act\": \"gelu\",\r\n",
      "  \"hidden_dropout_prob\": 0.1,\r\n",
      "  \"hidden_size\": 768,\r\n",
      "  \"id2label\": {\r\n",
      "    \"0\": \"LABEL_0\",\r\n",
      "    \"1\": \"LABEL_1\",\r\n",
      "    \"2\": \"LABEL_2\"\r\n",
      "  },\r\n",
      "  \"initializer_range\": 0.02,\r\n",
      "  \"intermediate_size\": 3072,\r\n",
      "  \"label2id\": {\r\n",
      "    \"LABEL_0\": 0,\r\n",
      "    \"LABEL_1\": 1,\r\n",
      "    \"LABEL_2\": 2\r\n",
      "  },\r\n",
      "  \"layer_norm_eps\": 1e-12,\r\n",
      "  \"max_position_embeddings\": 512,\r\n",
      "  \"model_type\": \"bert\",\r\n",
      "  \"num_attention_heads\": 12,\r\n",
      "  \"num_hidden_layers\": 12,\r\n",
      "  \"pad_token_id\": 0,\r\n",
      "  \"position_embedding_type\": \"absolute\",\r\n",
      "  \"transformers_version\": \"4.5.0.dev0\",\r\n",
      "  \"type_vocab_size\": 2,\r\n",
      "  \"use_cache\": true,\r\n",
      "  \"vocab_size\": 31116\r\n",
      "}\r\n",
      "\r\n",
      "[INFO|configuration_utils.py:470] 2021-06-13 09:25:19,288 >> loading configuration file /kaggle/input/tdmsci/scibert/output/config.json\r\n",
      "[INFO|configuration_utils.py:508] 2021-06-13 09:25:19,289 >> Model config BertConfig {\r\n",
      "  \"_name_or_path\": \"allenai/scibert_scivocab_cased\",\r\n",
      "  \"architectures\": [\r\n",
      "    \"BertForTokenClassification\"\r\n",
      "  ],\r\n",
      "  \"attention_probs_dropout_prob\": 0.1,\r\n",
      "  \"finetuning_task\": \"ner\",\r\n",
      "  \"gradient_checkpointing\": false,\r\n",
      "  \"hidden_act\": \"gelu\",\r\n",
      "  \"hidden_dropout_prob\": 0.1,\r\n",
      "  \"hidden_size\": 768,\r\n",
      "  \"id2label\": {\r\n",
      "    \"0\": \"LABEL_0\",\r\n",
      "    \"1\": \"LABEL_1\",\r\n",
      "    \"2\": \"LABEL_2\"\r\n",
      "  },\r\n",
      "  \"initializer_range\": 0.02,\r\n",
      "  \"intermediate_size\": 3072,\r\n",
      "  \"label2id\": {\r\n",
      "    \"LABEL_0\": 0,\r\n",
      "    \"LABEL_1\": 1,\r\n",
      "    \"LABEL_2\": 2\r\n",
      "  },\r\n",
      "  \"layer_norm_eps\": 1e-12,\r\n",
      "  \"max_position_embeddings\": 512,\r\n",
      "  \"model_type\": \"bert\",\r\n",
      "  \"num_attention_heads\": 12,\r\n",
      "  \"num_hidden_layers\": 12,\r\n",
      "  \"pad_token_id\": 0,\r\n",
      "  \"position_embedding_type\": \"absolute\",\r\n",
      "  \"transformers_version\": \"4.5.0.dev0\",\r\n",
      "  \"type_vocab_size\": 2,\r\n",
      "  \"use_cache\": true,\r\n",
      "  \"vocab_size\": 31116\r\n",
      "}\r\n",
      "\r\n",
      "[INFO|tokenization_utils_base.py:1637] 2021-06-13 09:25:19,290 >> Didn't find file /kaggle/input/tdmsci/scibert/output/tokenizer.json. We won't load it.\r\n",
      "[INFO|tokenization_utils_base.py:1637] 2021-06-13 09:25:19,291 >> Didn't find file /kaggle/input/tdmsci/scibert/output/added_tokens.json. We won't load it.\r\n",
      "[INFO|tokenization_utils_base.py:1700] 2021-06-13 09:25:19,293 >> loading file /kaggle/input/tdmsci/scibert/output/vocab.txt\r\n",
      "[INFO|tokenization_utils_base.py:1700] 2021-06-13 09:25:19,293 >> loading file None\r\n",
      "[INFO|tokenization_utils_base.py:1700] 2021-06-13 09:25:19,293 >> loading file None\r\n",
      "[INFO|tokenization_utils_base.py:1700] 2021-06-13 09:25:19,293 >> loading file /kaggle/input/tdmsci/scibert/output/special_tokens_map.json\r\n",
      "[INFO|tokenization_utils_base.py:1700] 2021-06-13 09:25:19,293 >> loading file /kaggle/input/tdmsci/scibert/output/tokenizer_config.json\r\n",
      "[INFO|modeling_utils.py:1049] 2021-06-13 09:25:19,383 >> loading weights file /kaggle/input/tdmsci/scibert/output/pytorch_model.bin\r\n",
      "[INFO|modeling_utils.py:1167] 2021-06-13 09:25:27,839 >> All model checkpoint weights were used when initializing BertForTokenClassification.\r\n",
      "\r\n",
      "[INFO|modeling_utils.py:1176] 2021-06-13 09:25:27,839 >> All the weights of BertForTokenClassification were initialized from the model checkpoint at /kaggle/input/tdmsci/scibert/output.\r\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use BertForTokenClassification for predictions without further training.\r\n",
      "[WARNING|tokenization_utils_base.py:2137] 2021-06-13 09:25:27,840 >> Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\r\n",
      "100%|█████████████████████████████████████████████| 4/4 [00:00<00:00,  4.25ba/s]\r\n",
      "[INFO|trainer.py:485] 2021-06-13 09:25:35,867 >> The following columns in the test set  don't have a corresponding argument in `BertForTokenClassification.forward` and have been ignored: tags, tokens, id.\r\n",
      "[INFO|trainer.py:1817] 2021-06-13 09:25:35,868 >> ***** Running Prediction *****\r\n",
      "[INFO|trainer.py:1818] 2021-06-13 09:25:35,869 >>   Num examples = 3079\r\n",
      "[INFO|trainer.py:1819] 2021-06-13 09:25:35,869 >>   Batch size = 8\r\n",
      "100%|████████████████████████████████████████▉| 384/385 [00:13<00:00, 28.23it/s]/opt/conda/lib/python3.7/site-packages/seqeval/metrics/v1.py:57: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 due to no true samples. Use `zero_division` parameter to control this behavior.\r\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\r\n",
      "[INFO|trainer_pt_utils.py:735] 2021-06-13 09:25:54,402 >> ***** test metrics *****\r\n",
      "[INFO|trainer_pt_utils.py:740] 2021-06-13 09:25:54,402 >>   init_mem_cpu_alloc_delta  =     1570MB\r\n",
      "[INFO|trainer_pt_utils.py:740] 2021-06-13 09:25:54,403 >>   init_mem_cpu_peaked_delta =      306MB\r\n",
      "[INFO|trainer_pt_utils.py:740] 2021-06-13 09:25:54,403 >>   init_mem_gpu_alloc_delta  =      418MB\r\n",
      "[INFO|trainer_pt_utils.py:740] 2021-06-13 09:25:54,403 >>   init_mem_gpu_peaked_delta =        0MB\r\n",
      "[INFO|trainer_pt_utils.py:740] 2021-06-13 09:25:54,403 >>   test_accuracy             =     0.9939\r\n",
      "[INFO|trainer_pt_utils.py:740] 2021-06-13 09:25:54,403 >>   test_f1                   =        0.0\r\n",
      "[INFO|trainer_pt_utils.py:740] 2021-06-13 09:25:54,403 >>   test_loss                 =     0.0308\r\n",
      "[INFO|trainer_pt_utils.py:740] 2021-06-13 09:25:54,403 >>   test_mem_cpu_alloc_delta  =       97MB\r\n",
      "[INFO|trainer_pt_utils.py:740] 2021-06-13 09:25:54,403 >>   test_mem_cpu_peaked_delta =        0MB\r\n",
      "[INFO|trainer_pt_utils.py:740] 2021-06-13 09:25:54,403 >>   test_mem_gpu_alloc_delta  =        0MB\r\n",
      "[INFO|trainer_pt_utils.py:740] 2021-06-13 09:25:54,403 >>   test_mem_gpu_peaked_delta =       75MB\r\n",
      "[INFO|trainer_pt_utils.py:740] 2021-06-13 09:25:54,403 >>   test_precision            =        0.0\r\n",
      "[INFO|trainer_pt_utils.py:740] 2021-06-13 09:25:54,403 >>   test_recall               =        0.0\r\n",
      "[INFO|trainer_pt_utils.py:740] 2021-06-13 09:25:54,403 >>   test_runtime              = 0:00:17.93\r\n",
      "[INFO|trainer_pt_utils.py:740] 2021-06-13 09:25:54,403 >>   test_samples_per_second   =    171.711\r\n",
      "100%|█████████████████████████████████████████| 385/385 [00:17<00:00, 21.59it/s]\r\n"
     ]
    }
   ],
   "source": [
    "bert_outputs = []\n",
    "batch_size = 64000\n",
    "\n",
    "for batch_begin in range(0, len(test_rows), batch_size):#len(test_rows), batch_size):\n",
    "    print(len(test_rows))\n",
    "    # write data rows to input file\n",
    "    with open(f\"{test_input_save_path}/{filename_test}\", 'w') as f:\n",
    "        for row in test_rows[batch_begin:batch_begin+batch_size]:\n",
    "            json.dump(row, f)\n",
    "            f.write('\\n')\n",
    "            \n",
    "    with open(f\"{test_input_save_path}/{filename_test}\", 'r') as f:\n",
    "        content = f.read()\n",
    "        \n",
    "    # remove output dir\n",
    "    !rm -r \"$OUTPUT_DIR\"\n",
    "    \n",
    "    # do predict\n",
    "    predict_scibert_ner()\n",
    "    \n",
    "    # read predictions\n",
    "    with open(f'{pred_save_path}/{prediction_file}') as f:\n",
    "        this_preds = f.read().split('\\n')[:-1]\n",
    "        bert_outputs += [pred.split() for pred in this_preds]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.079493,
     "end_time": "2021-06-13T09:25:55.838403",
     "exception": false,
     "start_time": "2021-06-13T09:25:55.758910",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## 6.4. Restore labels\n",
    "After prediction, we need to restore the labels to actual words, with the use of the earleir defined paper lengths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-06-13T09:25:56.011387Z",
     "iopub.status.busy": "2021-06-13T09:25:56.010035Z",
     "iopub.status.idle": "2021-06-13T09:25:56.040407Z",
     "shell.execute_reply": "2021-06-13T09:25:56.039676Z",
     "shell.execute_reply.started": "2021-06-13T09:07:16.768132Z"
    },
    "papermill": {
     "duration": 0.122254,
     "end_time": "2021-06-13T09:25:56.040583",
     "exception": false,
     "start_time": "2021-06-13T09:25:55.918329",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "14\n",
      "38\n",
      "46\n",
      "61\n",
      "2\n",
      "26\n",
      "41\n",
      "29\n",
      "43\n",
      "14\n",
      "38\n",
      "56\n",
      "40\n",
      "25\n",
      "6\n",
      "19\n",
      "43\n",
      "38\n",
      "20\n",
      "15\n",
      "25\n",
      "41\n",
      "6\n",
      "24\n",
      "22\n",
      "16\n",
      "19\n",
      "16\n",
      "62\n",
      "31\n",
      "28\n",
      "18\n",
      "27\n",
      "5\n",
      "28\n",
      "26\n",
      "36\n",
      "24\n",
      "14\n",
      "23\n",
      "14\n",
      "42\n",
      "24\n",
      "21\n",
      "89\n",
      "36\n",
      "65\n",
      "30\n",
      "21\n",
      "31\n",
      "96\n",
      "21\n",
      "42\n",
      "2\n",
      "1\n",
      "47\n",
      "15\n",
      "47\n",
      "36\n",
      "13\n",
      "106\n",
      "11\n",
      "10\n",
      "35\n",
      "26\n",
      "53\n",
      "23\n",
      "28\n",
      "26\n",
      "41\n",
      "45\n",
      "28\n",
      "8\n",
      "11\n",
      "15\n",
      "26\n",
      "33\n",
      "33\n",
      "31\n",
      "21\n",
      "22\n",
      "13\n",
      "10\n",
      "7\n",
      "23\n",
      "1\n",
      "7\n",
      "48\n",
      "28\n",
      "21\n",
      "10\n",
      "45\n",
      "38\n",
      "71\n",
      "41\n",
      "23\n",
      "12\n",
      "200\n",
      "27\n",
      "24\n"
     ]
    }
   ],
   "source": [
    "\n",
    "test_sentences = [row['tokens'] for row in test_rows]\n",
    "\n",
    "\n",
    "bert_dataset_labels = [] # store all dataset labels for each publication\n",
    "\n",
    "for length in paper_lengths:\n",
    "    print(length)\n",
    "    labels = set()\n",
    "    for sentence, pred in zip(test_sentences[:length], bert_outputs[:length]):\n",
    "        curr_phrase = ''\n",
    "        for word, tag in zip(sentence, pred):\n",
    "            if tag == 'B-DATASET': # start a new phrase\n",
    "                if curr_phrase:\n",
    "                    labels.add(curr_phrase)\n",
    "                    curr_phrase = ''\n",
    "                curr_phrase = word\n",
    "            elif tag == 'I-DATASET' and curr_phrase: # continue the phrase\n",
    "                curr_phrase += ' ' + word\n",
    "            else: # end last phrase (if any)\n",
    "                if curr_phrase:\n",
    "                    labels.add(curr_phrase)\n",
    "                    curr_phrase = ''\n",
    "        # check if the label is the suffix of the sentence\n",
    "        if curr_phrase:\n",
    "            labels.add(curr_phrase)\n",
    "            curr_phrase = ''\n",
    "    \n",
    "    # record dataset labels for this publication\n",
    "    bert_dataset_labels.append(labels)\n",
    "    \n",
    "    del test_sentences[:length], bert_outputs[:length]\n",
    "\n",
    "      \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-06-13T09:25:56.210084Z",
     "iopub.status.busy": "2021-06-13T09:25:56.209428Z",
     "iopub.status.idle": "2021-06-13T09:25:56.214218Z",
     "shell.execute_reply": "2021-06-13T09:25:56.213701Z",
     "shell.execute_reply.started": "2021-06-13T09:07:16.825324Z"
    },
    "papermill": {
     "duration": 0.089301,
     "end_time": "2021-06-13T09:25:56.214328",
     "exception": false,
     "start_time": "2021-06-13T09:25:56.125027",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[set(),\n",
       " {'Trends in International Mathematics and Science Study'},\n",
       " set(),\n",
       " {'ADNI', 'Alzheimer s Disease Neuroimaging Initiative ADNI'},\n",
       " set()]"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bert_dataset_labels[:5]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6.5. Filter results\n",
    "We filter the reults by first looking at the jaccard simalirty between the predicted labels, to not get nearly-duplicates.\n",
    "After this, we match our results to the literal matching list defined earlier in this notebook. In case a literal match is present for a predicted label, the literal match will be taken as final output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-06-13T09:25:56.382374Z",
     "iopub.status.busy": "2021-06-13T09:25:56.381629Z",
     "iopub.status.idle": "2021-06-13T09:25:56.384602Z",
     "shell.execute_reply": "2021-06-13T09:25:56.384217Z",
     "shell.execute_reply.started": "2021-06-13T09:07:16.839636Z"
    },
    "papermill": {
     "duration": 0.089905,
     "end_time": "2021-06-13T09:25:56.384720",
     "exception": false,
     "start_time": "2021-06-13T09:25:56.294815",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def jaccard_similarity(s1, s2):\n",
    "    l1 = s1.split(\" \")\n",
    "    l2 = s2.split(\" \")    \n",
    "    intersection = len(list(set(l1).intersection(l2)))\n",
    "    union = (len(l1) + len(l2)) - intersection\n",
    "    return float(intersection) / union\n",
    "\n",
    "filtered_bert_labels = []\n",
    "\n",
    "for labels in bert_dataset_labels:\n",
    "    filtered = []\n",
    "    \n",
    "    for label in sorted(labels, key=len):\n",
    "        label = clean_text(label)\n",
    "        if len(filtered) == 0 or all(jaccard_similarity(label, got_label) < 0.75 for got_label in filtered):\n",
    "            filtered.append(label)\n",
    "    \n",
    "    filtered_bert_labels.append('|'.join(filtered))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-06-13T09:25:56.555662Z",
     "iopub.status.busy": "2021-06-13T09:25:56.554850Z",
     "iopub.status.idle": "2021-06-13T09:25:56.558793Z",
     "shell.execute_reply": "2021-06-13T09:25:56.558377Z",
     "shell.execute_reply.started": "2021-06-13T09:07:16.860673Z"
    },
    "papermill": {
     "duration": 0.09369,
     "end_time": "2021-06-13T09:25:56.558907",
     "exception": false,
     "start_time": "2021-06-13T09:25:56.465217",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['',\n",
       " 'trends in international mathematics and science study',\n",
       " '',\n",
       " 'adni|alzheimer s disease neuroimaging initiative adni',\n",
       " '',\n",
       " 'adni|alzheimer s disease neuroimaging initiative adni',\n",
       " 'early childhood longitudinal study',\n",
       " 'baltimore longitudinal study of aging',\n",
       " '',\n",
       " 'baltimore longitudinal study of aging',\n",
       " 'baltimore longitudinal study of aging',\n",
       " 'adni|alzheimer s disease neuroimaging initiative adni',\n",
       " 'adni',\n",
       " 'baltimore longitudinal study of aging',\n",
       " 'adni',\n",
       " '',\n",
       " 'adni|alzheimer s disease neuroimaging initiative adni',\n",
       " 'adni',\n",
       " 'beginning postsecondary students',\n",
       " '',\n",
       " 'covid 19 death data',\n",
       " 'early childhood longitudinal study',\n",
       " '',\n",
       " 'education longitudinal study|early childhood longitudinal study',\n",
       " '',\n",
       " 'survey of earned doctorates',\n",
       " 'education longitudinal study',\n",
       " '',\n",
       " '',\n",
       " 'adni|alzheimer s disease neuroimaging initiative adni',\n",
       " 'adni|alzheimer s disease neuroimaging initiative adni',\n",
       " '',\n",
       " 'agricultural resource management survey',\n",
       " 'survey of earned doctorates|survey of doctorate recipients',\n",
       " 'adni',\n",
       " 'education longitudinal study',\n",
       " '',\n",
       " 'baltimore longitudinal study of aging',\n",
       " 'survey of earned doctorates',\n",
       " 'baltimore longitudinal study of aging',\n",
       " '',\n",
       " 'school survey on crime and safety',\n",
       " 'rural urban continuum codes',\n",
       " 'census of agriculture',\n",
       " 'slosh model|noaa tide gauge',\n",
       " 'trends in international mathematics and science study',\n",
       " 'covid 19 open research dataset',\n",
       " 'trends in international mathematics and science study',\n",
       " 'sars cov 2 genome sequences',\n",
       " 'world ocean database',\n",
       " '',\n",
       " 'adni',\n",
       " 'noaa tide gauge',\n",
       " '',\n",
       " '',\n",
       " 'adni|alzheimer s disease neuroimaging initiative adni',\n",
       " 'noaa tide gauge',\n",
       " '',\n",
       " 'trends in international mathematics and science study',\n",
       " 'baltimore longitudinal study of aging',\n",
       " 'world ocean database',\n",
       " '',\n",
       " 'alzheimer s disease neuroimaging initiative adni',\n",
       " 'national assessment of education progress',\n",
       " 'baccalaureate and beyond',\n",
       " '',\n",
       " 'baltimore longitudinal study of aging',\n",
       " '',\n",
       " 'covid 19 image data collection',\n",
       " 'adni',\n",
       " 'world ocean database',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " 'adni',\n",
       " 'alzheimer s disease neuroimaging initiative adni',\n",
       " 'covid 19 open research dataset',\n",
       " 'trends in international mathematics and science study',\n",
       " 'adni|alzheimer s disease neuroimaging initiative adni',\n",
       " '',\n",
       " 'adni',\n",
       " 'agricultural resource management survey',\n",
       " 'census of agriculture',\n",
       " 'early childhood longitudinal study',\n",
       " 'adni|alzheimer s disease neuroimaging initiative adni',\n",
       " '',\n",
       " '',\n",
       " 'education longitudinal study|early childhood longitudinal study',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " 'adni|alzheimer s disease neuroimaging initiative adni',\n",
       " 'alzheimer s disease neuroimaging initiative adni',\n",
       " 'adni',\n",
       " 'alzheimer s disease neuroimaging initiative adni',\n",
       " '',\n",
       " 'agricultural resource management survey',\n",
       " 'adni|alzheimer s disease neuroimaging initiative adni',\n",
       " 'adni|alzheimer|alzheimer s disease neuroimaging initiative adni',\n",
       " 'national education longitudinal study']"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "filtered_bert_labels[:100]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-06-13T09:25:56.887938Z",
     "iopub.status.busy": "2021-06-13T09:25:56.887306Z",
     "iopub.status.idle": "2021-06-13T09:25:56.890813Z",
     "shell.execute_reply": "2021-06-13T09:25:56.890397Z",
     "shell.execute_reply.started": "2021-06-13T09:07:16.884649Z"
    },
    "papermill": {
     "duration": 0.088864,
     "end_time": "2021-06-13T09:25:56.890927",
     "exception": false,
     "start_time": "2021-06-13T09:25:56.802063",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Adding literal matching!\n"
     ]
    }
   ],
   "source": [
    "\n",
    "if(not onlyBert):\n",
    "    print(\"Adding literal matching!\")\n",
    "    final_predictions = []\n",
    "    for literal_match, bert_pred in zip(literal_preds, filtered_bert_labels):\n",
    "        if literal_match:\n",
    "            final_predictions.append(literal_match)\n",
    "        else:\n",
    "            final_predictions.append(bert_pred)\n",
    "\n",
    "else:\n",
    "    print(\"No literal matching!\")\n",
    "    final_predictions = filtered_bert_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-06-13T09:25:57.060960Z",
     "iopub.status.busy": "2021-06-13T09:25:57.060088Z",
     "iopub.status.idle": "2021-06-13T09:25:57.063654Z",
     "shell.execute_reply": "2021-06-13T09:25:57.064102Z",
     "shell.execute_reply.started": "2021-06-13T09:07:16.898035Z"
    },
    "papermill": {
     "duration": 0.092302,
     "end_time": "2021-06-13T09:25:57.064243",
     "exception": false,
     "start_time": "2021-06-13T09:25:56.971941",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['program for the international assessment of adult competencies',\n",
       " 'trends in international mathematics and science study',\n",
       " 'agricultural resources management survey',\n",
       " 'adni|alzheimer s disease neuroimaging initiative adni',\n",
       " 'genome sequence of covid 19',\n",
       " 'adni|alzheimer s disease neuroimaging initiative adni',\n",
       " 'early childhood longitudinal study|national assessment of educational progress',\n",
       " 'baltimore longitudinal study of aging|baltimore longitudinal study of aging blsa',\n",
       " 'noaa tide gauge',\n",
       " 'baltimore longitudinal study of aging|baltimore longitudinal study of aging blsa',\n",
       " 'baltimore longitudinal study of aging|baltimore longitudinal study of aging blsa',\n",
       " 'adni|alzheimer s disease neuroimaging initiative adni',\n",
       " 'adni',\n",
       " 'baltimore longitudinal study of aging|baltimore longitudinal study of aging blsa',\n",
       " 'adni',\n",
       " 'noaa optimum interpolation sea surface temperature|optimum interpolation sea surface temperature',\n",
       " 'adni|alzheimer s disease neuroimaging initiative adni',\n",
       " 'adni|alzheimer s disease neuroimaging initiative adni',\n",
       " 'beginning postsecondary student|integrated postsecondary education data system|beginning postsecondary students',\n",
       " 'slosh model',\n",
       " 'covid 19 death data',\n",
       " 'early childhood longitudinal study',\n",
       " 'international best track archive for climate stewardship|ibtracs',\n",
       " 'early childhood longitudinal study|education longitudinal study|national education longitudinal study',\n",
       " 'survey of earned doctorates',\n",
       " 'survey of earned doctorates',\n",
       " 'education longitudinal study|national education longitudinal study',\n",
       " 'adni',\n",
       " 'adni',\n",
       " 'adni|alzheimer s disease neuroimaging initiative adni',\n",
       " 'adni|alzheimer s disease neuroimaging initiative adni',\n",
       " 'adni|alzheimer s disease neuroimaging initiative adni',\n",
       " 'agricultural resource management survey|census of agriculture',\n",
       " 'survey of doctorate recipients|survey of earned doctorates',\n",
       " 'adni',\n",
       " 'education longitudinal study|national education longitudinal study',\n",
       " 'rural urban continuum codes',\n",
       " 'baltimore longitudinal study of aging',\n",
       " 'survey of earned doctorates',\n",
       " 'baltimore longitudinal study of aging',\n",
       " 'rural urban continuum codes',\n",
       " 'school survey on crime and safety',\n",
       " 'rural urban continuum codes',\n",
       " 'census of agriculture',\n",
       " 'slosh model|noaa tide station|noaa tide gauge|sea lake and overland surges from hurricanes',\n",
       " 'trends in international mathematics and science study',\n",
       " 'covid 19 open research data|covid 19 open research dataset',\n",
       " 'trends in international mathematics and science study',\n",
       " 'sars cov 2 genome sequence|sars cov 2 genome sequences',\n",
       " 'world ocean database',\n",
       " 'rural urban continuum codes',\n",
       " 'adni',\n",
       " 'noaa tide gauge',\n",
       " 'agricultural resource management survey',\n",
       " 'national assessment of education progress|national assessment of educational progress',\n",
       " 'adni|alzheimer s disease neuroimaging initiative adni',\n",
       " 'noaa tide gauge',\n",
       " 'national assessment of education progress',\n",
       " 'trends in international mathematics and science study',\n",
       " 'baltimore longitudinal study of aging|baltimore longitudinal study of aging blsa',\n",
       " 'world ocean database',\n",
       " 'slosh model',\n",
       " 'adni|alzheimer s disease neuroimaging initiative adni',\n",
       " 'national assessment of education progress',\n",
       " 'baccalaureate and beyond|national survey of family growth',\n",
       " 'adni',\n",
       " 'baltimore longitudinal study of aging|baltimore longitudinal study of aging blsa',\n",
       " 'national science foundation survey of earned doctorates|survey of earned doctorates',\n",
       " 'covid 19 image data collection',\n",
       " 'adni',\n",
       " 'world ocean database',\n",
       " 'national assessment of education progress',\n",
       " 'rural urban continuum codes',\n",
       " 'adni|alzheimer s disease neuroimaging initiative adni',\n",
       " 'adni',\n",
       " 'adni|alzheimer s disease neuroimaging initiative adni',\n",
       " 'covid 19 open research data|covid 19 open research dataset',\n",
       " 'trends in international mathematics and science study',\n",
       " 'adni|alzheimer s disease neuroimaging initiative adni',\n",
       " 'adni|alzheimer s disease neuroimaging initiative adni',\n",
       " 'adni',\n",
       " 'agricultural resource management survey',\n",
       " 'census of agriculture',\n",
       " 'early childhood longitudinal study',\n",
       " 'adni|alzheimer s disease neuroimaging initiative adni',\n",
       " 'noaa tide gauge',\n",
       " 'sars cov 2 full genome sequences|sars cov 2 full genome sequence',\n",
       " 'head start impact study|early childhood longitudinal study|education longitudinal study|national education longitudinal study',\n",
       " 'adni|alzheimer s disease neuroimaging initiative adni',\n",
       " 'coastal change analysis program',\n",
       " 'survey of industrial research and development',\n",
       " 'adni|alzheimer s disease neuroimaging initiative adni',\n",
       " 'adni|alzheimer s disease neuroimaging initiative adni',\n",
       " 'adni',\n",
       " 'adni|alzheimer s disease neuroimaging initiative adni',\n",
       " 'adni',\n",
       " 'agricultural resource management survey',\n",
       " 'national database for autism research ndar|adni|alzheimer s disease neuroimaging initiative adni',\n",
       " 'adni|alzheimer s disease neuroimaging initiative adni',\n",
       " 'education longitudinal study|national education longitudinal study']"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_predictions[0:100]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6.6. Generate subm\n",
    "ission file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-06-13T09:25:57.232310Z",
     "iopub.status.busy": "2021-06-13T09:25:57.231334Z",
     "iopub.status.idle": "2021-06-13T09:25:57.233546Z",
     "shell.execute_reply": "2021-06-13T09:25:57.234032Z",
     "shell.execute_reply.started": "2021-06-13T09:07:16.913795Z"
    },
    "papermill": {
     "duration": 0.088327,
     "end_time": "2021-06-13T09:25:57.234167",
     "exception": false,
     "start_time": "2021-06-13T09:25:57.145840",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "#In case of submission\n",
    "if(not computeFBeta):\n",
    "    sample_submission['PredictionString'] = final_predictions\n",
    "    sample_submission.head()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-06-13T09:25:57.401046Z",
     "iopub.status.busy": "2021-06-13T09:25:57.400340Z",
     "iopub.status.idle": "2021-06-13T09:25:57.403148Z",
     "shell.execute_reply": "2021-06-13T09:25:57.402631Z",
     "shell.execute_reply.started": "2021-06-13T09:07:16.925106Z"
    },
    "papermill": {
     "duration": 0.087668,
     "end_time": "2021-06-13T09:25:57.403267",
     "exception": false,
     "start_time": "2021-06-13T09:25:57.315599",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "#In case of submission\n",
    "if(not computeFBeta):\n",
    "    sample_submission.to_csv(f'submission.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6.7. Perform validation\n",
    "In Part I of this set of notebooks, we left the first 100 papers out for validation. Here,  we perfrom evaluation on the first 100 papers by computing the Jaccard-based FBeta score."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-06-13T09:25:57.576044Z",
     "iopub.status.busy": "2021-06-13T09:25:57.575234Z",
     "iopub.status.idle": "2021-06-13T09:25:57.578000Z",
     "shell.execute_reply": "2021-06-13T09:25:57.577585Z",
     "shell.execute_reply.started": "2021-06-13T09:07:16.935889Z"
    },
    "papermill": {
     "duration": 0.093541,
     "end_time": "2021-06-13T09:25:57.578111",
     "exception": false,
     "start_time": "2021-06-13T09:25:57.484570",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def compute_fbeta(y_true: List[List[str]],\n",
    "                  y_pred: List[List[str]],\n",
    "                  beta: float = 0.5) -> float:\n",
    "    \"\"\"Compute the Jaccard-based micro FBeta score.\n",
    "\n",
    "    References\n",
    "    ----------\n",
    "    - https://www.kaggle.com/c/coleridgeinitiative-show-us-the-data/overview/evaluation\n",
    "    \"\"\"\n",
    "\n",
    "    def _jaccard_similarity(str1: str, str2: str) -> float:\n",
    "        a = set(str1.split()) \n",
    "        b = set(str2.split())\n",
    "        c = a.intersection(b)\n",
    "        return float(len(c)) / (len(a) + len(b) - len(c))\n",
    "\n",
    "    tp = 0  # true positive\n",
    "    fp = 0  # false positive\n",
    "    fn = 0  # false negative\n",
    "    for ground_truth_list, predicted_string_list in zip(y_true, y_pred):\n",
    "        \n",
    "\n",
    "        predicted_string_list_sorted = sorted(predicted_string_list)\n",
    "        for ground_truth in sorted(ground_truth_list):         \n",
    "\n",
    "            if len(predicted_string_list_sorted) == 0:\n",
    "                fn += 1\n",
    "            else:\n",
    "                similarity_scores = [\n",
    "                    _jaccard_similarity(ground_truth, predicted_string)\n",
    "                    for predicted_string in predicted_string_list_sorted\n",
    "                ]\n",
    "                matched_idx = np.argmax(similarity_scores)\n",
    "                if similarity_scores[matched_idx] >= 0.5:\n",
    "                    predicted_string_list_sorted.pop(matched_idx)\n",
    "                    tp += 1\n",
    "                else:\n",
    "                    fn += 1\n",
    "        fp += len(predicted_string_list_sorted)\n",
    "\n",
    "    tp *= (1 + beta ** 2)\n",
    "    fn *= beta ** 2\n",
    "    fbeta_score = tp / (tp+ fp + fn)\n",
    "    return fbeta_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-06-13T09:25:57.746141Z",
     "iopub.status.busy": "2021-06-13T09:25:57.745309Z",
     "iopub.status.idle": "2021-06-13T09:25:57.747887Z",
     "shell.execute_reply": "2021-06-13T09:25:57.747441Z",
     "shell.execute_reply.started": "2021-06-13T09:08:53.923252Z"
    },
    "papermill": {
     "duration": 0.08896,
     "end_time": "2021-06-13T09:25:57.748014",
     "exception": false,
     "start_time": "2021-06-13T09:25:57.659054",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def getTrainLabels(ids, final_predictions, label_df):\n",
    "    labels = []\n",
    "    for paper_id in ids:\n",
    "        \n",
    "        \n",
    "        \n",
    "        value = label_df.loc[label_df['Id'] == paper_id]\n",
    "        cleanedLabel = value[\"cleaned_label\"].values\n",
    "        labels.append(cleanedLabel)\n",
    "    return labels\n",
    "      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-06-13T09:25:57.913051Z",
     "iopub.status.busy": "2021-06-13T09:25:57.912223Z",
     "iopub.status.idle": "2021-06-13T09:25:58.134943Z",
     "shell.execute_reply": "2021-06-13T09:25:58.134132Z",
     "shell.execute_reply.started": "2021-06-13T09:08:54.119843Z"
    },
    "papermill": {
     "duration": 0.306299,
     "end_time": "2021-06-13T09:25:58.135074",
     "exception": false,
     "start_time": "2021-06-13T09:25:57.828775",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[array(['program for the international assessment of adult competencies'],\n",
      "      dtype=object), array(['trends in international mathematics and science study'],\n",
      "      dtype=object), array(['agricultural resources management survey'], dtype=object), array(['adni|alzheimer s disease neuroimaging initiative adni '],\n",
      "      dtype=object), array(['genome sequence of covid 19'], dtype=object), array(['adni|alzheimer s disease neuroimaging initiative adni '],\n",
      "      dtype=object), array(['early childhood longitudinal study'], dtype=object), array(['baltimore longitudinal study of aging blsa |baltimore longitudinal study of aging'],\n",
      "      dtype=object), array(['noaa tide gauge'], dtype=object), array(['baltimore longitudinal study of aging'], dtype=object), array(['baltimore longitudinal study of aging blsa |baltimore longitudinal study of aging'],\n",
      "      dtype=object), array(['adni|alzheimer s disease neuroimaging initiative adni '],\n",
      "      dtype=object), array(['adni'], dtype=object), array(['baltimore longitudinal study of aging blsa |baltimore longitudinal study of aging'],\n",
      "      dtype=object), array(['adni'], dtype=object), array(['optimum interpolation sea surface temperature|noaa optimum interpolation sea surface temperature'],\n",
      "      dtype=object), array(['adni|alzheimer s disease neuroimaging initiative adni '],\n",
      "      dtype=object), array(['adni|alzheimer s disease neuroimaging initiative adni '],\n",
      "      dtype=object), array(['beginning postsecondary students'], dtype=object), array(['slosh model'], dtype=object), array(['covid 19 death data'], dtype=object), array(['early childhood longitudinal study'], dtype=object), array(['international best track archive for climate stewardship'],\n",
      "      dtype=object), array(['national education longitudinal study|early childhood longitudinal study|education longitudinal study'],\n",
      "      dtype=object), array(['survey of earned doctorates'], dtype=object), array(['survey of earned doctorates'], dtype=object), array(['education longitudinal study|national education longitudinal study'],\n",
      "      dtype=object), array(['adni'], dtype=object), array(['adni'], dtype=object), array(['adni|alzheimer s disease neuroimaging initiative adni '],\n",
      "      dtype=object), array(['adni'], dtype=object), array(['adni|alzheimer s disease neuroimaging initiative adni '],\n",
      "      dtype=object), array(['agricultural resource management survey'], dtype=object), array(['survey of earned doctorates|survey of doctorate recipients'],\n",
      "      dtype=object), array(['adni'], dtype=object), array(['national education longitudinal study|education longitudinal study'],\n",
      "      dtype=object), array(['rural urban continuum codes'], dtype=object), array(['baltimore longitudinal study of aging'], dtype=object), array(['survey of earned doctorates'], dtype=object), array(['baltimore longitudinal study of aging'], dtype=object), array(['rural urban continuum codes'], dtype=object), array(['school survey on crime and safety'], dtype=object), array(['rural urban continuum codes'], dtype=object), array(['census of agriculture'], dtype=object), array(['slosh model|noaa tide station|noaa tide gauge'], dtype=object), array(['trends in international mathematics and science study'],\n",
      "      dtype=object), array(['covid 19 open research dataset'], dtype=object), array(['trends in international mathematics and science study'],\n",
      "      dtype=object), array(['sars cov 2 genome sequence|sars cov 2 genome sequences'],\n",
      "      dtype=object), array(['world ocean database'], dtype=object), array(['rural urban continuum codes'], dtype=object), array(['adni'], dtype=object), array(['noaa tide gauge'], dtype=object), array(['agricultural resource management survey'], dtype=object), array(['national assessment of education progress'], dtype=object), array(['alzheimer s disease neuroimaging initiative adni '], dtype=object), array(['noaa tide gauge'], dtype=object), array(['national assessment of education progress'], dtype=object), array(['trends in international mathematics and science study'],\n",
      "      dtype=object), array(['baltimore longitudinal study of aging blsa |baltimore longitudinal study of aging'],\n",
      "      dtype=object), array(['world ocean database'], dtype=object), array(['slosh model'], dtype=object), array(['adni|alzheimer s disease neuroimaging initiative adni '],\n",
      "      dtype=object), array(['national assessment of education progress'], dtype=object), array(['baccalaureate and beyond'], dtype=object), array(['adni'], dtype=object), array(['baltimore longitudinal study of aging blsa |baltimore longitudinal study of aging'],\n",
      "      dtype=object), array(['national science foundation survey of earned doctorates|survey of earned doctorates'],\n",
      "      dtype=object), array(['covid 19 image data collection'], dtype=object), array(['adni'], dtype=object), array(['world ocean database'], dtype=object), array(['national assessment of education progress'], dtype=object), array(['rural urban continuum codes'], dtype=object), array(['adni|alzheimer s disease neuroimaging initiative adni '],\n",
      "      dtype=object), array(['adni'], dtype=object), array(['adni|alzheimer s disease neuroimaging initiative adni '],\n",
      "      dtype=object), array(['covid 19 open research dataset'], dtype=object), array(['trends in international mathematics and science study'],\n",
      "      dtype=object), array(['alzheimer s disease neuroimaging initiative adni '], dtype=object), array(['adni'], dtype=object), array(['adni'], dtype=object), array(['agricultural resource management survey'], dtype=object), array(['census of agriculture'], dtype=object), array(['early childhood longitudinal study'], dtype=object), array(['adni|alzheimer s disease neuroimaging initiative adni '],\n",
      "      dtype=object), array(['noaa tide gauge'], dtype=object), array(['sars cov 2 full genome sequences'], dtype=object), array(['national education longitudinal study|early childhood longitudinal study|education longitudinal study'],\n",
      "      dtype=object), array(['adni|alzheimer s disease neuroimaging initiative adni '],\n",
      "      dtype=object), array(['coastal change analysis program'], dtype=object), array(['survey of industrial research and development'], dtype=object), array(['adni|alzheimer s disease neuroimaging initiative adni '],\n",
      "      dtype=object), array(['adni|alzheimer s disease neuroimaging initiative adni '],\n",
      "      dtype=object), array(['adni'], dtype=object), array(['adni|alzheimer s disease neuroimaging initiative adni '],\n",
      "      dtype=object), array(['adni'], dtype=object), array(['agricultural resource management survey'], dtype=object), array(['adni|alzheimer s disease neuroimaging initiative adni '],\n",
      "      dtype=object), array(['adni|alzheimer s disease neuroimaging initiative adni '],\n",
      "      dtype=object), array(['education longitudinal study|national education longitudinal study'],\n",
      "      dtype=object)]\n",
      "['program for the international assessment of adult competencies', 'trends in international mathematics and science study', 'agricultural resources management survey', 'adni|alzheimer s disease neuroimaging initiative adni', 'genome sequence of covid 19', 'adni|alzheimer s disease neuroimaging initiative adni', 'early childhood longitudinal study|national assessment of educational progress', 'baltimore longitudinal study of aging|baltimore longitudinal study of aging blsa', 'noaa tide gauge', 'baltimore longitudinal study of aging|baltimore longitudinal study of aging blsa', 'baltimore longitudinal study of aging|baltimore longitudinal study of aging blsa', 'adni|alzheimer s disease neuroimaging initiative adni', 'adni', 'baltimore longitudinal study of aging|baltimore longitudinal study of aging blsa', 'adni', 'noaa optimum interpolation sea surface temperature|optimum interpolation sea surface temperature', 'adni|alzheimer s disease neuroimaging initiative adni', 'adni|alzheimer s disease neuroimaging initiative adni', 'beginning postsecondary student|integrated postsecondary education data system|beginning postsecondary students', 'slosh model', 'covid 19 death data', 'early childhood longitudinal study', 'international best track archive for climate stewardship|ibtracs', 'early childhood longitudinal study|education longitudinal study|national education longitudinal study', 'survey of earned doctorates', 'survey of earned doctorates', 'education longitudinal study|national education longitudinal study', 'adni', 'adni', 'adni|alzheimer s disease neuroimaging initiative adni', 'adni|alzheimer s disease neuroimaging initiative adni', 'adni|alzheimer s disease neuroimaging initiative adni', 'agricultural resource management survey|census of agriculture', 'survey of doctorate recipients|survey of earned doctorates', 'adni', 'education longitudinal study|national education longitudinal study', 'rural urban continuum codes', 'baltimore longitudinal study of aging', 'survey of earned doctorates', 'baltimore longitudinal study of aging', 'rural urban continuum codes', 'school survey on crime and safety', 'rural urban continuum codes', 'census of agriculture', 'slosh model|noaa tide station|noaa tide gauge|sea lake and overland surges from hurricanes', 'trends in international mathematics and science study', 'covid 19 open research data|covid 19 open research dataset', 'trends in international mathematics and science study', 'sars cov 2 genome sequence|sars cov 2 genome sequences', 'world ocean database', 'rural urban continuum codes', 'adni', 'noaa tide gauge', 'agricultural resource management survey', 'national assessment of education progress|national assessment of educational progress', 'adni|alzheimer s disease neuroimaging initiative adni', 'noaa tide gauge', 'national assessment of education progress', 'trends in international mathematics and science study', 'baltimore longitudinal study of aging|baltimore longitudinal study of aging blsa', 'world ocean database', 'slosh model', 'adni|alzheimer s disease neuroimaging initiative adni', 'national assessment of education progress', 'baccalaureate and beyond|national survey of family growth', 'adni', 'baltimore longitudinal study of aging|baltimore longitudinal study of aging blsa', 'national science foundation survey of earned doctorates|survey of earned doctorates', 'covid 19 image data collection', 'adni', 'world ocean database', 'national assessment of education progress', 'rural urban continuum codes', 'adni|alzheimer s disease neuroimaging initiative adni', 'adni', 'adni|alzheimer s disease neuroimaging initiative adni', 'covid 19 open research data|covid 19 open research dataset', 'trends in international mathematics and science study', 'adni|alzheimer s disease neuroimaging initiative adni', 'adni|alzheimer s disease neuroimaging initiative adni', 'adni', 'agricultural resource management survey', 'census of agriculture', 'early childhood longitudinal study', 'adni|alzheimer s disease neuroimaging initiative adni', 'noaa tide gauge', 'sars cov 2 full genome sequences|sars cov 2 full genome sequence', 'head start impact study|early childhood longitudinal study|education longitudinal study|national education longitudinal study', 'adni|alzheimer s disease neuroimaging initiative adni', 'coastal change analysis program', 'survey of industrial research and development', 'adni|alzheimer s disease neuroimaging initiative adni', 'adni|alzheimer s disease neuroimaging initiative adni', 'adni', 'adni|alzheimer s disease neuroimaging initiative adni', 'adni', 'agricultural resource management survey', 'national database for autism research ndar|adni|alzheimer s disease neuroimaging initiative adni', 'adni|alzheimer s disease neuroimaging initiative adni', 'education longitudinal study|national education longitudinal study']\n",
      "0.6952247191011236\n"
     ]
    }
   ],
   "source": [
    "#In case of development (using the validation set)\n",
    "if(computeFBeta):\n",
    "    trainLabels = getTrainLabels(validationIds, final_predictions,  label_df)\n",
    "    print(trainLabels)\n",
    "    print(final_predictions)\n",
    "    fbeta = compute_fbeta(trainLabels, [x.split('|')for x in final_predictions])\n",
    "    print(fbeta)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.081134,
     "end_time": "2021-06-13T09:25:58.298196",
     "exception": false,
     "start_time": "2021-06-13T09:25:58.217062",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Generate submission file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-06-13T09:25:58.466789Z",
     "iopub.status.busy": "2021-06-13T09:25:58.465954Z",
     "iopub.status.idle": "2021-06-13T09:25:58.469051Z",
     "shell.execute_reply": "2021-06-13T09:25:58.468598Z",
     "shell.execute_reply.started": "2021-06-12T07:10:24.819969Z"
    },
    "papermill": {
     "duration": 0.089636,
     "end_time": "2021-06-13T09:25:58.469169",
     "exception": false,
     "start_time": "2021-06-13T09:25:58.379533",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# def generate_submission_file(test_ids, test_predictions):\n",
    "#     submission_dict = {\"Id\": test_ids, \"PredictionString\": test_predictions}\n",
    "#     submission_df = pd.DataFrame.from_dict(submission_dict)\n",
    "#     submission_df.to_csv(f'submission.csv', index=False)\n",
    "    \n",
    "    \n",
    "\n",
    "# generate_submission_file(test_df_ner[\"Id\"], final_predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.080926,
     "end_time": "2021-06-13T09:25:58.631524",
     "exception": false,
     "start_time": "2021-06-13T09:25:58.550598",
     "status": "completed"
    },
    "tags": []
   },
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "papermill": {
     "duration": 0.081894,
     "end_time": "2021-06-13T09:25:58.795082",
     "exception": false,
     "start_time": "2021-06-13T09:25:58.713188",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "papermill": {
     "duration": 0.081642,
     "end_time": "2021-06-13T09:25:58.957965",
     "exception": false,
     "start_time": "2021-06-13T09:25:58.876323",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 298.644029,
   "end_time": "2021-06-13T09:26:00.864743",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2021-06-13T09:21:02.220714",
   "version": "2.3.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
