{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.019343,
     "end_time": "2021-06-07T04:30:11.340309",
     "exception": false,
     "start_time": "2021-06-07T04:30:11.320966",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# PART 1 (Training)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.018127,
     "end_time": "2021-06-07T04:30:11.377029",
     "exception": false,
     "start_time": "2021-06-07T04:30:11.358902",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# 1. Introduction\n",
    "\n",
    "This challenge is about datasets used in scientific papers. In particular, we want to extract the datasets for scientific paper, with several NLP approaches. In this notebook, we make use of SciBERT, introduced by Beltagy, I., Lo, K., and Cohan, A. in 2019 [1]. Source code of SciBERT can be found [here](https://github.com/allenai/scibert).\n",
    "\n",
    "Furthermore, we append the existing data with a specialized Corpus for dataset tagging. TDMSci is a Corpus existing of annotated data for tasks, metrices and datasets. Here, B-DATASET and I-DATASET are the NER-labels indicating a word is (part of) a dataset [2]. Source code (and annotated data) of TDMSci can be found [here](https://github.com/IBM/science-result-extractor).\n",
    "\n",
    "We try two approaches to tackle the problem:\n",
    "-  Named Entity Recognition (NER);\n",
    "-  Masked Language Model (MLM) Classification;\n",
    "\n",
    "We have created three notebooks, one for **dataset creation** (Part 0), one for **training** (Part I) and one for **testing** (Part II). This notebook encounters the traning phase of our model(s).\n",
    "\n",
    "\n",
    "[1] Beltagy, I., Lo, K., & Cohan, A. (2019). SciBERT: A pretrained language model for scientific text. arXiv preprint arXiv:1903.10676.  \n",
    "[2] Hou, Y., Jochim, C., Gleize, M., Bonin, F., & Ganguly, D. (2021). TDMSci: A Specialized Corpus for Scientific Literature Entity Tagging of Tasks Datasets and Metrics. arXiv preprint arXiv:2101.10273.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.017927,
     "end_time": "2021-06-07T04:30:11.413182",
     "exception": false,
     "start_time": "2021-06-07T04:30:11.395255",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# 2. Preparing Notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-06-07T04:30:11.456464Z",
     "iopub.status.busy": "2021-06-07T04:30:11.454971Z",
     "iopub.status.idle": "2021-06-07T04:30:44.576065Z",
     "shell.execute_reply": "2021-06-07T04:30:44.575449Z",
     "shell.execute_reply.started": "2021-06-05T07:53:42.653554Z"
    },
    "papermill": {
     "duration": 33.14457,
     "end_time": "2021-06-07T04:30:44.576240",
     "exception": false,
     "start_time": "2021-06-07T04:30:11.431670",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in links: file:///kaggle/input/coleridge-packages/packages/datasets\r\n",
      "Processing /kaggle/input/coleridge-packages/packages/datasets/datasets-1.5.0-py3-none-any.whl\r\n",
      "Requirement already satisfied: pyarrow>=0.17.1 in /opt/conda/lib/python3.7/site-packages (from datasets) (1.0.1)\r\n",
      "Processing /kaggle/input/coleridge-packages/packages/datasets/xxhash-2.0.0-cp37-cp37m-manylinux2010_x86_64.whl\r\n",
      "Requirement already satisfied: fsspec in /opt/conda/lib/python3.7/site-packages (from datasets) (0.8.5)\r\n",
      "Requirement already satisfied: requests>=2.19.0 in /opt/conda/lib/python3.7/site-packages (from datasets) (2.25.1)\r\n",
      "Requirement already satisfied: numpy>=1.17 in /opt/conda/lib/python3.7/site-packages (from datasets) (1.19.5)\r\n",
      "Processing /kaggle/input/coleridge-packages/packages/datasets/tqdm-4.49.0-py2.py3-none-any.whl\r\n",
      "Requirement already satisfied: pandas in /opt/conda/lib/python3.7/site-packages (from datasets) (1.1.5)\r\n",
      "Requirement already satisfied: dill in /opt/conda/lib/python3.7/site-packages (from datasets) (0.3.3)\r\n",
      "Requirement already satisfied: multiprocess in /opt/conda/lib/python3.7/site-packages (from datasets) (0.70.11.1)\r\n",
      "Requirement already satisfied: importlib-metadata in /opt/conda/lib/python3.7/site-packages (from datasets) (3.4.0)\r\n",
      "Processing /kaggle/input/coleridge-packages/packages/datasets/huggingface_hub-0.0.7-py3-none-any.whl\r\n",
      "Requirement already satisfied: filelock in /opt/conda/lib/python3.7/site-packages (from huggingface-hub<0.1.0->datasets) (3.0.12)\r\n",
      "Requirement already satisfied: idna<3,>=2.5 in /opt/conda/lib/python3.7/site-packages (from requests>=2.19.0->datasets) (2.10)\r\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.7/site-packages (from requests>=2.19.0->datasets) (2020.12.5)\r\n",
      "Requirement already satisfied: chardet<5,>=3.0.2 in /opt/conda/lib/python3.7/site-packages (from requests>=2.19.0->datasets) (3.0.4)\r\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /opt/conda/lib/python3.7/site-packages (from requests>=2.19.0->datasets) (1.26.3)\r\n",
      "Requirement already satisfied: typing-extensions>=3.6.4 in /opt/conda/lib/python3.7/site-packages (from importlib-metadata->datasets) (3.7.4.3)\r\n",
      "Requirement already satisfied: zipp>=0.5 in /opt/conda/lib/python3.7/site-packages (from importlib-metadata->datasets) (3.4.0)\r\n",
      "Requirement already satisfied: python-dateutil>=2.7.3 in /opt/conda/lib/python3.7/site-packages (from pandas->datasets) (2.8.1)\r\n",
      "Requirement already satisfied: pytz>=2017.2 in /opt/conda/lib/python3.7/site-packages (from pandas->datasets) (2021.1)\r\n",
      "Requirement already satisfied: six>=1.5 in /opt/conda/lib/python3.7/site-packages (from python-dateutil>=2.7.3->pandas->datasets) (1.15.0)\r\n",
      "Installing collected packages: tqdm, xxhash, huggingface-hub, datasets\r\n",
      "  Attempting uninstall: tqdm\r\n",
      "    Found existing installation: tqdm 4.56.2\r\n",
      "    Uninstalling tqdm-4.56.2:\r\n",
      "      Successfully uninstalled tqdm-4.56.2\r\n",
      "Successfully installed datasets-1.5.0 huggingface-hub-0.0.7 tqdm-4.49.0 xxhash-2.0.0\r\n",
      "Processing /kaggle/input/coleridge-packages/seqeval-1.2.2-py3-none-any.whl\r\n",
      "Requirement already satisfied: numpy>=1.14.0 in /opt/conda/lib/python3.7/site-packages (from seqeval==1.2.2) (1.19.5)\r\n",
      "Requirement already satisfied: scikit-learn>=0.21.3 in /opt/conda/lib/python3.7/site-packages (from seqeval==1.2.2) (0.24.1)\r\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in /opt/conda/lib/python3.7/site-packages (from scikit-learn>=0.21.3->seqeval==1.2.2) (2.1.0)\r\n",
      "Requirement already satisfied: scipy>=0.19.1 in /opt/conda/lib/python3.7/site-packages (from scikit-learn>=0.21.3->seqeval==1.2.2) (1.5.4)\r\n",
      "Requirement already satisfied: joblib>=0.11 in /opt/conda/lib/python3.7/site-packages (from scikit-learn>=0.21.3->seqeval==1.2.2) (1.0.1)\r\n",
      "Installing collected packages: seqeval\r\n",
      "Successfully installed seqeval-1.2.2\r\n",
      "Processing /kaggle/input/coleridge-packages/tokenizers-0.10.1-cp37-cp37m-manylinux1_x86_64.whl\r\n",
      "tokenizers is already installed with the same version as the provided wheel. Use --force-reinstall to force an installation of the wheel.\r\n",
      "Processing /kaggle/input/coleridge-packages/transformers-4.5.0.dev0-py3-none-any.whl\r\n",
      "Requirement already satisfied: sacremoses in /opt/conda/lib/python3.7/site-packages (from transformers==4.5.0.dev0) (0.0.43)\r\n",
      "Requirement already satisfied: importlib-metadata in /opt/conda/lib/python3.7/site-packages (from transformers==4.5.0.dev0) (3.4.0)\r\n",
      "Requirement already satisfied: filelock in /opt/conda/lib/python3.7/site-packages (from transformers==4.5.0.dev0) (3.0.12)\r\n",
      "Requirement already satisfied: numpy>=1.17 in /opt/conda/lib/python3.7/site-packages (from transformers==4.5.0.dev0) (1.19.5)\r\n",
      "Requirement already satisfied: tokenizers<0.11,>=0.10.1 in /opt/conda/lib/python3.7/site-packages (from transformers==4.5.0.dev0) (0.10.1)\r\n",
      "Requirement already satisfied: regex!=2019.12.17 in /opt/conda/lib/python3.7/site-packages (from transformers==4.5.0.dev0) (2020.11.13)\r\n",
      "Requirement already satisfied: packaging in /opt/conda/lib/python3.7/site-packages (from transformers==4.5.0.dev0) (20.9)\r\n",
      "Requirement already satisfied: tqdm>=4.27 in /opt/conda/lib/python3.7/site-packages (from transformers==4.5.0.dev0) (4.49.0)\r\n",
      "Requirement already satisfied: requests in /opt/conda/lib/python3.7/site-packages (from transformers==4.5.0.dev0) (2.25.1)\r\n",
      "Requirement already satisfied: typing-extensions>=3.6.4 in /opt/conda/lib/python3.7/site-packages (from importlib-metadata->transformers==4.5.0.dev0) (3.7.4.3)\r\n",
      "Requirement already satisfied: zipp>=0.5 in /opt/conda/lib/python3.7/site-packages (from importlib-metadata->transformers==4.5.0.dev0) (3.4.0)\r\n",
      "Requirement already satisfied: pyparsing>=2.0.2 in /opt/conda/lib/python3.7/site-packages (from packaging->transformers==4.5.0.dev0) (2.4.7)\r\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /opt/conda/lib/python3.7/site-packages (from requests->transformers==4.5.0.dev0) (1.26.3)\r\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.7/site-packages (from requests->transformers==4.5.0.dev0) (2020.12.5)\r\n",
      "Requirement already satisfied: chardet<5,>=3.0.2 in /opt/conda/lib/python3.7/site-packages (from requests->transformers==4.5.0.dev0) (3.0.4)\r\n",
      "Requirement already satisfied: idna<3,>=2.5 in /opt/conda/lib/python3.7/site-packages (from requests->transformers==4.5.0.dev0) (2.10)\r\n",
      "Requirement already satisfied: six in /opt/conda/lib/python3.7/site-packages (from sacremoses->transformers==4.5.0.dev0) (1.15.0)\r\n",
      "Requirement already satisfied: click in /opt/conda/lib/python3.7/site-packages (from sacremoses->transformers==4.5.0.dev0) (7.1.2)\r\n",
      "Requirement already satisfied: joblib in /opt/conda/lib/python3.7/site-packages (from sacremoses->transformers==4.5.0.dev0) (1.0.1)\r\n",
      "Installing collected packages: transformers\r\n",
      "  Attempting uninstall: transformers\r\n",
      "    Found existing installation: transformers 4.4.2\r\n",
      "    Uninstalling transformers-4.4.2:\r\n",
      "      Successfully uninstalled transformers-4.4.2\r\n",
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\r\n",
      "allennlp 2.2.0 requires transformers<4.5,>=4.1, but you have transformers 4.5.0.dev0 which is incompatible.\u001b[0m\r\n",
      "Successfully installed transformers-4.5.0.dev0\r\n",
      "Requirement already satisfied: datasets in /opt/conda/lib/python3.7/site-packages (1.5.0)\r\n",
      "Requirement already satisfied: fsspec in /opt/conda/lib/python3.7/site-packages (from datasets) (0.8.5)\r\n",
      "Requirement already satisfied: numpy>=1.17 in /opt/conda/lib/python3.7/site-packages (from datasets) (1.19.5)\r\n",
      "Requirement already satisfied: importlib-metadata in /opt/conda/lib/python3.7/site-packages (from datasets) (3.4.0)\r\n",
      "Requirement already satisfied: xxhash in /opt/conda/lib/python3.7/site-packages (from datasets) (2.0.0)\r\n",
      "Requirement already satisfied: tqdm<4.50.0,>=4.27 in /opt/conda/lib/python3.7/site-packages (from datasets) (4.49.0)\r\n",
      "Requirement already satisfied: pyarrow>=0.17.1 in /opt/conda/lib/python3.7/site-packages (from datasets) (1.0.1)\r\n",
      "Requirement already satisfied: huggingface-hub<0.1.0 in /opt/conda/lib/python3.7/site-packages (from datasets) (0.0.7)\r\n",
      "Requirement already satisfied: multiprocess in /opt/conda/lib/python3.7/site-packages (from datasets) (0.70.11.1)\r\n",
      "Requirement already satisfied: requests>=2.19.0 in /opt/conda/lib/python3.7/site-packages (from datasets) (2.25.1)\r\n",
      "Requirement already satisfied: dill in /opt/conda/lib/python3.7/site-packages (from datasets) (0.3.3)\r\n",
      "Requirement already satisfied: pandas in /opt/conda/lib/python3.7/site-packages (from datasets) (1.1.5)\r\n",
      "Requirement already satisfied: filelock in /opt/conda/lib/python3.7/site-packages (from huggingface-hub<0.1.0->datasets) (3.0.12)\r\n",
      "Requirement already satisfied: idna<3,>=2.5 in /opt/conda/lib/python3.7/site-packages (from requests>=2.19.0->datasets) (2.10)\r\n",
      "Requirement already satisfied: chardet<5,>=3.0.2 in /opt/conda/lib/python3.7/site-packages (from requests>=2.19.0->datasets) (3.0.4)\r\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /opt/conda/lib/python3.7/site-packages (from requests>=2.19.0->datasets) (1.26.3)\r\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.7/site-packages (from requests>=2.19.0->datasets) (2020.12.5)\r\n",
      "Requirement already satisfied: zipp>=0.5 in /opt/conda/lib/python3.7/site-packages (from importlib-metadata->datasets) (3.4.0)\r\n",
      "Requirement already satisfied: typing-extensions>=3.6.4 in /opt/conda/lib/python3.7/site-packages (from importlib-metadata->datasets) (3.7.4.3)\r\n",
      "Requirement already satisfied: python-dateutil>=2.7.3 in /opt/conda/lib/python3.7/site-packages (from pandas->datasets) (2.8.1)\r\n",
      "Requirement already satisfied: pytz>=2017.2 in /opt/conda/lib/python3.7/site-packages (from pandas->datasets) (2021.1)\r\n",
      "Requirement already satisfied: six>=1.5 in /opt/conda/lib/python3.7/site-packages (from python-dateutil>=2.7.3->pandas->datasets) (1.15.0)\r\n"
     ]
    }
   ],
   "source": [
    "!pip install datasets --no-index --find-links=file:///kaggle/input/coleridge-packages/packages/datasets \n",
    "!pip install ../input/coleridge-packages/seqeval-1.2.2-py3-none-any.whl \n",
    "!pip install ../input/coleridge-packages/tokenizers-0.10.1-cp37-cp37m-manylinux1_x86_64.whl \n",
    "!pip install ../input/coleridge-packages/transformers-4.5.0.dev0-py3-none-any.whl \n",
    "!pip install datasets "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "papermill": {
     "duration": 0.025,
     "end_time": "2021-06-07T04:30:44.628114",
     "exception": false,
     "start_time": "2021-06-07T04:30:44.603114",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-06-07T04:30:44.686742Z",
     "iopub.status.busy": "2021-06-07T04:30:44.684225Z",
     "iopub.status.idle": "2021-06-07T04:30:55.906631Z",
     "shell.execute_reply": "2021-06-07T04:30:55.907326Z",
     "shell.execute_reply.started": "2021-06-05T07:54:15.592982Z"
    },
    "papermill": {
     "duration": 11.253986,
     "end_time": "2021-06-07T04:30:55.907482",
     "exception": false,
     "start_time": "2021-06-07T04:30:44.653496",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.7/site-packages/torchaudio/backend/utils.py:54: UserWarning: \"sox\" backend is being deprecated. The default backend will be changed to \"sox_io\" backend in 0.8.0 and \"sox\" backend will be removed in 0.9.0. Please migrate to \"sox_io\" backend. Please refer to https://github.com/pytorch/audio/issues/903 for the detail.\n",
      "  '\"sox\" backend is being deprecated. '\n"
     ]
    }
   ],
   "source": [
    "#Import necessary libraries\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "import nltk\n",
    "import re\n",
    "import os\n",
    "from os import listdir\n",
    "\n",
    "from os.path import isfile, join\n",
    "import re\n",
    "import json\n",
    "import time\n",
    "import datetime\n",
    "import random\n",
    "import glob\n",
    "import importlib\n",
    "import allennlp\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from transformers import *\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import torch\n",
    "#from datasets import load_dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-06-07T04:30:55.966383Z",
     "iopub.status.busy": "2021-06-07T04:30:55.964501Z",
     "iopub.status.idle": "2021-06-07T04:30:55.966999Z",
     "shell.execute_reply": "2021-06-07T04:30:55.967405Z",
     "shell.execute_reply.started": "2021-06-05T07:54:26.24489Z"
    },
    "papermill": {
     "duration": 0.034365,
     "end_time": "2021-06-07T04:30:55.967536",
     "exception": false,
     "start_time": "2021-06-07T04:30:55.933171",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "model_name = 'allenai/scibert_scivocab_cased'\n",
    "#model_name =  'bert-base-cased'\n",
    "#Initialize paths for data\n",
    "path_abs = '/kaggle/input/coleridgeinitiative-show-us-the-data/'\n",
    "path_train = os.path.join(path_abs,'train/')\n",
    "path_train_metadata = os.path.join(path_abs, 'train.csv')\n",
    "path_test = os.path.join(path_abs, 'test/')\n",
    "path_sample_submission = os.path.join(path_abs, 'sample_submission.csv')\n",
    "\n",
    "path_abs_tdmsci = '/kaggle/input/tdmsci/'\n",
    "path_test_tdmsci = os.path.join(path_abs_tdmsci, 'test_500_v2.txt')\n",
    "path_train_tdmsci = os.path.join(path_abs_tdmsci,'train_1500_v2.txt')\n",
    "#path_train_nerjson = os.path.join(path_abs_tdmsci, 'train_ner.json')\n",
    "#path_train_nerjson = '/kaggle/working/train_ner.json'\n",
    "path_train_nerjson = '../input/fork-of-mlip-group25-scibert-dataset/train_ner.json'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-06-07T04:30:56.022527Z",
     "iopub.status.busy": "2021-06-07T04:30:56.022020Z",
     "iopub.status.idle": "2021-06-07T04:30:57.407994Z",
     "shell.execute_reply": "2021-06-07T04:30:57.406993Z",
     "shell.execute_reply.started": "2021-06-05T07:54:26.256348Z"
    },
    "papermill": {
     "duration": 1.415453,
     "end_time": "2021-06-07T04:30:57.408131",
     "exception": false,
     "start_time": "2021-06-07T04:30:55.992678",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'tokens': ['ADNI', 'Mueller', 'et', 'al', '2005', 'into', 'the', 'analysis', 'base', 'such', 'that', 'these', 'datasets', 'which', 'are', 'actually', 'stored', 'in', 'their', 'entirety', 'on', 'the', 'N4U', 'Grid', 'infrastructure', 'or', 'other', 'similar', 'repositories', 'become', 'indexed', 'in', 'the', 'analysis', 'base'], 'tags': ['B-DATASET', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O'], 'id': '018e6c55-7704-4332-8084-ec53dc457b4b'}\n"
     ]
    }
   ],
   "source": [
    "acc = 0\n",
    "labels = []\n",
    "with open(path_train_nerjson) as f:\n",
    "    for row in f:\n",
    "        rowjson = json.loads(row)\n",
    "        if(acc == 0):\n",
    "            print(rowjson)\n",
    "        labels+= rowjson[\"tags\"]\n",
    "        acc += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-06-07T04:30:58.091263Z",
     "iopub.status.busy": "2021-06-07T04:30:58.090237Z",
     "iopub.status.idle": "2021-06-07T04:30:58.638306Z",
     "shell.execute_reply": "2021-06-07T04:30:58.637663Z",
     "shell.execute_reply.started": "2021-06-05T07:54:30.621362Z"
    },
    "papermill": {
     "duration": 1.204398,
     "end_time": "2021-06-07T04:30:58.638441",
     "exception": false,
     "start_time": "2021-06-07T04:30:57.434043",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "95394\n",
      "['B-DATASET' 'I-DATASET' 'O']\n"
     ]
    }
   ],
   "source": [
    "print(acc)\n",
    "print(np.unique(labels))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.026876,
     "end_time": "2021-06-07T04:30:58.692410",
     "exception": false,
     "start_time": "2021-06-07T04:30:58.665534",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# 3. Train the SciBert model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-06-07T04:30:58.751046Z",
     "iopub.status.busy": "2021-06-07T04:30:58.749295Z",
     "iopub.status.idle": "2021-06-07T04:30:58.751597Z",
     "shell.execute_reply": "2021-06-07T04:30:58.751993Z",
     "shell.execute_reply.started": "2021-06-05T07:54:34.55508Z"
    },
    "papermill": {
     "duration": 0.03312,
     "end_time": "2021-06-07T04:30:58.752122",
     "exception": false,
     "start_time": "2021-06-07T04:30:58.719002",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "#Import SciBERT models\n",
    "#SciBERT_tokenizer = AutoTokenizer.from_pretrained(model_name, sep_token='[SEP]', pad_token='[PAD]', cls_token='[CLS]') #https://huggingface.co/transformers/model_doc/bert.html  \n",
    "#SciBERT_modelTC = AutoModelForTokenClassification.from_pretrained(model_name)\n",
    "#SciBERT_modelMLM = AutoModelForMaskedLM.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-06-07T04:30:58.814762Z",
     "iopub.status.busy": "2021-06-07T04:30:58.807669Z",
     "iopub.status.idle": "2021-06-07T04:30:59.447811Z",
     "shell.execute_reply": "2021-06-07T04:30:59.448622Z",
     "shell.execute_reply.started": "2021-06-05T07:54:34.561093Z"
    },
    "papermill": {
     "duration": 0.670777,
     "end_time": "2021-06-07T04:30:59.448802",
     "exception": false,
     "start_time": "2021-06-07T04:30:58.778025",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# copy my_seqeval.py to the working directory because the input directory is non-writable\n",
    "!cp /kaggle/input/coleridge-packages/my_seqeval.py ./\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-06-07T04:30:59.505514Z",
     "iopub.status.busy": "2021-06-07T04:30:59.504983Z",
     "iopub.status.idle": "2021-06-07T04:30:59.508737Z",
     "shell.execute_reply": "2021-06-07T04:30:59.508339Z",
     "shell.execute_reply.started": "2021-06-05T07:54:35.222717Z"
    },
    "papermill": {
     "duration": 0.033874,
     "end_time": "2021-06-07T04:30:59.508846",
     "exception": false,
     "start_time": "2021-06-07T04:30:59.474972",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# def test_training_data(path):\n",
    "#     with open(path) as f:\n",
    "#         for row in f:\n",
    "#             jsonrow = json.loads(row)\n",
    "#             if(len(jsonrow[\"tokens\"]) > 512):\n",
    "#                 print(\"TOO LONG AT: {}\".format(jsonrow[\"id\"]))\n",
    "#                 print(jsonrow)\n",
    "\n",
    "\n",
    "# test_training_data(path_train_nerjson)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-06-07T04:30:59.593537Z",
     "iopub.status.busy": "2021-06-07T04:30:59.591914Z",
     "iopub.status.idle": "2021-06-07T04:31:00.364066Z",
     "shell.execute_reply": "2021-06-07T04:31:00.364916Z",
     "shell.execute_reply.started": "2021-06-05T07:54:35.23218Z"
    },
    "papermill": {
     "duration": 0.830391,
     "end_time": "2021-06-07T04:31:00.365092",
     "exception": false,
     "start_time": "2021-06-07T04:30:59.534701",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rm: cannot remove './output': No such file or directory\r\n"
     ]
    }
   ],
   "source": [
    "def train_scibert_ner(batch_size):\n",
    "    os.environ[\"MODEL_NAME\"] = f\"{model_name}\"\n",
    "    os.environ[\"TRAIN_FILE\"] = f\"{path_train_nerjson}\"\n",
    "    os.environ[\"VALIDATION_FILE\"] = f\"{path_train_nerjson}\"\n",
    "    os.environ[\"BATCH_SIZE\"] = f\"{batch_size}\"\n",
    "    \n",
    "    acc = 0\n",
    "    with open(path_train_nerjson) as f:\n",
    "        print(\"open \")\n",
    "        for row in f:\n",
    "            acc += 1\n",
    "    \n",
    "    print(\"There are {} training samples!\".format(acc))\n",
    "    \n",
    "    !python ../input/tdmsci/kaggle_run_ner.py \\\n",
    "    --model_name_or_path \"$MODEL_NAME\" \\\n",
    "    --train_file \"$TRAIN_FILE\" \\\n",
    "    --validation_file \"$VALIDATION_FILE\" \\\n",
    "    --num_train_epochs 4 \\\n",
    "    --per_device_train_batch_size \"$BATCH_SIZE\" \\\n",
    "    --per_device_eval_batch_size \"$BATCH_SIZE\" \\\n",
    "    --save_steps 15000 \\\n",
    "    --pad_to_max_length \\\n",
    "    --output_dir './output' \\\n",
    "    --report_to 'none' \\\n",
    "    --seed 123 \\\n",
    "    --do_train\n",
    "!rm -r \"./output\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-06-07T04:31:00.422855Z",
     "iopub.status.busy": "2021-06-07T04:31:00.422051Z",
     "iopub.status.idle": "2021-06-07T12:13:03.745145Z",
     "shell.execute_reply": "2021-06-07T12:13:03.744604Z",
     "shell.execute_reply.started": "2021-06-05T07:54:35.943147Z"
    },
    "papermill": {
     "duration": 27723.353728,
     "end_time": "2021-06-07T12:13:03.745291",
     "exception": false,
     "start_time": "2021-06-07T04:31:00.391563",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "open \n",
      "There are 95394 training samples!\n",
      "2021-06-07 04:31:03.133087: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudart.so.10.2\r\n",
      "Downloading and preparing dataset json/default (download: Unknown size, generated: Unknown size, post-processed: Unknown size, total: Unknown size) to /root/.cache/huggingface/datasets/json/default-0498abf660b7e026/0.0.0/83d5b3a2f62630efc6b5315f00f20209b4ad91a00ac586597caee3a4da0bef02...\r\n",
      "Dataset json downloaded and prepared to /root/.cache/huggingface/datasets/json/default-0498abf660b7e026/0.0.0/83d5b3a2f62630efc6b5315f00f20209b4ad91a00ac586597caee3a4da0bef02. Subsequent calls will reuse this data.\r\n",
      "[INFO|file_utils.py:1402] 2021-06-07 04:31:11,491 >> https://huggingface.co/allenai/scibert_scivocab_cased/resolve/main/config.json not found in cache or force_download set to True, downloading to /root/.cache/huggingface/transformers/tmp_260w4nh\r\n",
      "Downloading: 100%|██████████████████████████████| 385/385 [00:00<00:00, 349kB/s]\r\n",
      "[INFO|file_utils.py:1406] 2021-06-07 04:31:11,839 >> storing https://huggingface.co/allenai/scibert_scivocab_cased/resolve/main/config.json in cache at /root/.cache/huggingface/transformers/62ac366c3e40ed8952fcec53445ee752300aef550f61adff8f5b3485a268492b.bcde2c1ebafc440ded7d6525db15f6f30a50849b17cfb77242a97a8036b82861\r\n",
      "[INFO|file_utils.py:1409] 2021-06-07 04:31:11,839 >> creating metadata file for /root/.cache/huggingface/transformers/62ac366c3e40ed8952fcec53445ee752300aef550f61adff8f5b3485a268492b.bcde2c1ebafc440ded7d6525db15f6f30a50849b17cfb77242a97a8036b82861\r\n",
      "[INFO|configuration_utils.py:472] 2021-06-07 04:31:11,840 >> loading configuration file https://huggingface.co/allenai/scibert_scivocab_cased/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/62ac366c3e40ed8952fcec53445ee752300aef550f61adff8f5b3485a268492b.bcde2c1ebafc440ded7d6525db15f6f30a50849b17cfb77242a97a8036b82861\r\n",
      "[INFO|configuration_utils.py:508] 2021-06-07 04:31:11,840 >> Model config BertConfig {\r\n",
      "  \"attention_probs_dropout_prob\": 0.1,\r\n",
      "  \"finetuning_task\": \"ner\",\r\n",
      "  \"gradient_checkpointing\": false,\r\n",
      "  \"hidden_act\": \"gelu\",\r\n",
      "  \"hidden_dropout_prob\": 0.1,\r\n",
      "  \"hidden_size\": 768,\r\n",
      "  \"id2label\": {\r\n",
      "    \"0\": \"LABEL_0\",\r\n",
      "    \"1\": \"LABEL_1\",\r\n",
      "    \"2\": \"LABEL_2\"\r\n",
      "  },\r\n",
      "  \"initializer_range\": 0.02,\r\n",
      "  \"intermediate_size\": 3072,\r\n",
      "  \"label2id\": {\r\n",
      "    \"LABEL_0\": 0,\r\n",
      "    \"LABEL_1\": 1,\r\n",
      "    \"LABEL_2\": 2\r\n",
      "  },\r\n",
      "  \"layer_norm_eps\": 1e-12,\r\n",
      "  \"max_position_embeddings\": 512,\r\n",
      "  \"model_type\": \"bert\",\r\n",
      "  \"num_attention_heads\": 12,\r\n",
      "  \"num_hidden_layers\": 12,\r\n",
      "  \"pad_token_id\": 0,\r\n",
      "  \"position_embedding_type\": \"absolute\",\r\n",
      "  \"transformers_version\": \"4.5.0.dev0\",\r\n",
      "  \"type_vocab_size\": 2,\r\n",
      "  \"use_cache\": true,\r\n",
      "  \"vocab_size\": 31116\r\n",
      "}\r\n",
      "\r\n",
      "[INFO|configuration_utils.py:472] 2021-06-07 04:31:12,189 >> loading configuration file https://huggingface.co/allenai/scibert_scivocab_cased/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/62ac366c3e40ed8952fcec53445ee752300aef550f61adff8f5b3485a268492b.bcde2c1ebafc440ded7d6525db15f6f30a50849b17cfb77242a97a8036b82861\r\n",
      "[INFO|configuration_utils.py:508] 2021-06-07 04:31:12,190 >> Model config BertConfig {\r\n",
      "  \"attention_probs_dropout_prob\": 0.1,\r\n",
      "  \"gradient_checkpointing\": false,\r\n",
      "  \"hidden_act\": \"gelu\",\r\n",
      "  \"hidden_dropout_prob\": 0.1,\r\n",
      "  \"hidden_size\": 768,\r\n",
      "  \"initializer_range\": 0.02,\r\n",
      "  \"intermediate_size\": 3072,\r\n",
      "  \"layer_norm_eps\": 1e-12,\r\n",
      "  \"max_position_embeddings\": 512,\r\n",
      "  \"model_type\": \"bert\",\r\n",
      "  \"num_attention_heads\": 12,\r\n",
      "  \"num_hidden_layers\": 12,\r\n",
      "  \"pad_token_id\": 0,\r\n",
      "  \"position_embedding_type\": \"absolute\",\r\n",
      "  \"transformers_version\": \"4.5.0.dev0\",\r\n",
      "  \"type_vocab_size\": 2,\r\n",
      "  \"use_cache\": true,\r\n",
      "  \"vocab_size\": 31116\r\n",
      "}\r\n",
      "\r\n",
      "[INFO|file_utils.py:1402] 2021-06-07 04:31:12,541 >> https://huggingface.co/allenai/scibert_scivocab_cased/resolve/main/vocab.txt not found in cache or force_download set to True, downloading to /root/.cache/huggingface/transformers/tmp7yq9j6jo\r\n",
      "Downloading: 100%|███████████████████████████| 222k/222k [00:00<00:00, 1.73MB/s]\r\n",
      "[INFO|file_utils.py:1406] 2021-06-07 04:31:13,019 >> storing https://huggingface.co/allenai/scibert_scivocab_cased/resolve/main/vocab.txt in cache at /root/.cache/huggingface/transformers/397254f347f587a433c488ab9c20276efbe8777ee55f13f4178230f2caccdb99.72e8e5acf023d231f7689bafa0d08dd14da0d33395f00509444282fffcbb7adc\r\n",
      "[INFO|file_utils.py:1409] 2021-06-07 04:31:13,019 >> creating metadata file for /root/.cache/huggingface/transformers/397254f347f587a433c488ab9c20276efbe8777ee55f13f4178230f2caccdb99.72e8e5acf023d231f7689bafa0d08dd14da0d33395f00509444282fffcbb7adc\r\n",
      "[INFO|tokenization_utils_base.py:1702] 2021-06-07 04:31:14,413 >> loading file https://huggingface.co/allenai/scibert_scivocab_cased/resolve/main/vocab.txt from cache at /root/.cache/huggingface/transformers/397254f347f587a433c488ab9c20276efbe8777ee55f13f4178230f2caccdb99.72e8e5acf023d231f7689bafa0d08dd14da0d33395f00509444282fffcbb7adc\r\n",
      "[INFO|tokenization_utils_base.py:1702] 2021-06-07 04:31:14,413 >> loading file https://huggingface.co/allenai/scibert_scivocab_cased/resolve/main/tokenizer.json from cache at None\r\n",
      "[INFO|tokenization_utils_base.py:1702] 2021-06-07 04:31:14,413 >> loading file https://huggingface.co/allenai/scibert_scivocab_cased/resolve/main/added_tokens.json from cache at None\r\n",
      "[INFO|tokenization_utils_base.py:1702] 2021-06-07 04:31:14,413 >> loading file https://huggingface.co/allenai/scibert_scivocab_cased/resolve/main/special_tokens_map.json from cache at None\r\n",
      "[INFO|tokenization_utils_base.py:1702] 2021-06-07 04:31:14,414 >> loading file https://huggingface.co/allenai/scibert_scivocab_cased/resolve/main/tokenizer_config.json from cache at None\r\n",
      "[INFO|file_utils.py:1402] 2021-06-07 04:31:14,827 >> https://huggingface.co/allenai/scibert_scivocab_cased/resolve/main/pytorch_model.bin not found in cache or force_download set to True, downloading to /root/.cache/huggingface/transformers/tmpww7a1nso\r\n",
      "Downloading: 100%|███████████████████████████| 442M/442M [00:20<00:00, 21.8MB/s]\r\n",
      "[INFO|file_utils.py:1406] 2021-06-07 04:31:35,704 >> storing https://huggingface.co/allenai/scibert_scivocab_cased/resolve/main/pytorch_model.bin in cache at /root/.cache/huggingface/transformers/38fb438255295d0f6ad03b1cda169af73b3f1fa2b4fddc6fbf45854b508aef66.7352c55eae981bd658b28a9746a052242f6359950742ae411e486aebfa8c2456\r\n",
      "[INFO|file_utils.py:1409] 2021-06-07 04:31:35,704 >> creating metadata file for /root/.cache/huggingface/transformers/38fb438255295d0f6ad03b1cda169af73b3f1fa2b4fddc6fbf45854b508aef66.7352c55eae981bd658b28a9746a052242f6359950742ae411e486aebfa8c2456\r\n",
      "[INFO|modeling_utils.py:1051] 2021-06-07 04:31:35,705 >> loading weights file https://huggingface.co/allenai/scibert_scivocab_cased/resolve/main/pytorch_model.bin from cache at /root/.cache/huggingface/transformers/38fb438255295d0f6ad03b1cda169af73b3f1fa2b4fddc6fbf45854b508aef66.7352c55eae981bd658b28a9746a052242f6359950742ae411e486aebfa8c2456\r\n",
      "[WARNING|modeling_utils.py:1159] 2021-06-07 04:31:39,293 >> Some weights of the model checkpoint at allenai/scibert_scivocab_cased were not used when initializing BertForTokenClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight', 'cls.predictions.decoder.bias', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias']\r\n",
      "- This IS expected if you are initializing BertForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\r\n",
      "- This IS NOT expected if you are initializing BertForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\r\n",
      "[WARNING|modeling_utils.py:1170] 2021-06-07 04:31:39,293 >> Some weights of BertForTokenClassification were not initialized from the model checkpoint at allenai/scibert_scivocab_cased and are newly initialized: ['classifier.weight', 'classifier.bias']\r\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\r\n",
      "100%|███████████████████████████████████████████| 96/96 [00:58<00:00,  1.65ba/s]\r\n",
      "[INFO|trainer.py:485] 2021-06-07 04:32:44,763 >> The following columns in the training set  don't have a corresponding argument in `BertForTokenClassification.forward` and have been ignored: id, tags, tokens.\r\n",
      "[INFO|trainer.py:988] 2021-06-07 04:32:44,993 >> ***** Running training *****\r\n",
      "[INFO|trainer.py:989] 2021-06-07 04:32:44,993 >>   Num examples = 95394\r\n",
      "[INFO|trainer.py:990] 2021-06-07 04:32:44,993 >>   Num Epochs = 4\r\n",
      "[INFO|trainer.py:991] 2021-06-07 04:32:44,993 >>   Instantaneous batch size per device = 8\r\n",
      "[INFO|trainer.py:992] 2021-06-07 04:32:44,993 >>   Total train batch size (w. parallel, distributed & accumulation) = 8\r\n",
      "[INFO|trainer.py:993] 2021-06-07 04:32:44,993 >>   Gradient Accumulation steps = 1\r\n",
      "[INFO|trainer.py:994] 2021-06-07 04:32:44,993 >>   Total optimization steps = 47700\r\n",
      "{'loss': 0.0441, 'learning_rate': 4.947589098532495e-05, 'epoch': 0.04}\r\n",
      "{'loss': 0.0189, 'learning_rate': 4.8951781970649894e-05, 'epoch': 0.08}\r\n",
      "{'loss': 0.015, 'learning_rate': 4.842767295597484e-05, 'epoch': 0.13}\r\n",
      "{'loss': 0.0134, 'learning_rate': 4.79035639412998e-05, 'epoch': 0.17}\r\n",
      "{'loss': 0.0115, 'learning_rate': 4.737945492662474e-05, 'epoch': 0.21}\r\n",
      "{'loss': 0.0135, 'learning_rate': 4.685534591194969e-05, 'epoch': 0.25}\r\n",
      "{'loss': 0.0121, 'learning_rate': 4.633123689727464e-05, 'epoch': 0.29}\r\n",
      "{'loss': 0.0119, 'learning_rate': 4.580712788259958e-05, 'epoch': 0.34}\r\n",
      "{'loss': 0.011, 'learning_rate': 4.528301886792453e-05, 'epoch': 0.38}\r\n",
      "{'loss': 0.0125, 'learning_rate': 4.475890985324948e-05, 'epoch': 0.42}\r\n",
      "{'loss': 0.0104, 'learning_rate': 4.423480083857443e-05, 'epoch': 0.46}\r\n",
      "{'loss': 0.0089, 'learning_rate': 4.3710691823899376e-05, 'epoch': 0.5}\r\n",
      "{'loss': 0.0125, 'learning_rate': 4.318658280922432e-05, 'epoch': 0.55}\r\n",
      "{'loss': 0.0083, 'learning_rate': 4.266247379454927e-05, 'epoch': 0.59}\r\n",
      "{'loss': 0.0082, 'learning_rate': 4.213836477987422e-05, 'epoch': 0.63}\r\n",
      "{'loss': 0.0082, 'learning_rate': 4.161425576519916e-05, 'epoch': 0.67}\r\n",
      "{'loss': 0.0104, 'learning_rate': 4.109014675052411e-05, 'epoch': 0.71}\r\n",
      "{'loss': 0.0092, 'learning_rate': 4.0566037735849064e-05, 'epoch': 0.75}\r\n",
      "{'loss': 0.0079, 'learning_rate': 4.0041928721174006e-05, 'epoch': 0.8}\r\n",
      "{'loss': 0.0073, 'learning_rate': 3.9517819706498955e-05, 'epoch': 0.84}\r\n",
      "{'loss': 0.0078, 'learning_rate': 3.8993710691823904e-05, 'epoch': 0.88}\r\n",
      "{'loss': 0.0073, 'learning_rate': 3.8469601677148846e-05, 'epoch': 0.92}\r\n",
      "{'loss': 0.0073, 'learning_rate': 3.7945492662473795e-05, 'epoch': 0.96}\r\n",
      "{'loss': 0.0089, 'learning_rate': 3.7421383647798744e-05, 'epoch': 1.01}\r\n",
      "{'loss': 0.0066, 'learning_rate': 3.689727463312369e-05, 'epoch': 1.05}\r\n",
      "{'loss': 0.0057, 'learning_rate': 3.637316561844864e-05, 'epoch': 1.09}\r\n",
      "{'loss': 0.0065, 'learning_rate': 3.5849056603773584e-05, 'epoch': 1.13}\r\n",
      "{'loss': 0.0062, 'learning_rate': 3.532494758909853e-05, 'epoch': 1.17}\r\n",
      "{'loss': 0.0075, 'learning_rate': 3.480083857442348e-05, 'epoch': 1.22}\r\n",
      "{'loss': 0.006, 'learning_rate': 3.4276729559748424e-05, 'epoch': 1.26}\r\n",
      " 31%|██████████▍                      | 15000/47700 [2:24:52<5:10:31,  1.76it/s][INFO|trainer.py:1600] 2021-06-07 06:57:37,575 >> Saving model checkpoint to ./output/checkpoint-15000\r\n",
      "[INFO|configuration_utils.py:318] 2021-06-07 06:57:37,578 >> Configuration saved in ./output/checkpoint-15000/config.json\r\n",
      "[INFO|modeling_utils.py:837] 2021-06-07 06:57:39,125 >> Model weights saved in ./output/checkpoint-15000/pytorch_model.bin\r\n",
      "[INFO|tokenization_utils_base.py:1896] 2021-06-07 06:57:39,126 >> tokenizer config file saved in ./output/checkpoint-15000/tokenizer_config.json\r\n",
      "[INFO|tokenization_utils_base.py:1902] 2021-06-07 06:57:39,126 >> Special tokens file saved in ./output/checkpoint-15000/special_tokens_map.json\r\n",
      "{'loss': 0.0078, 'learning_rate': 3.375262054507338e-05, 'epoch': 1.3}\r\n",
      "{'loss': 0.0059, 'learning_rate': 3.322851153039833e-05, 'epoch': 1.34}\r\n",
      "{'loss': 0.0051, 'learning_rate': 3.270440251572327e-05, 'epoch': 1.38}\r\n",
      "{'loss': 0.0056, 'learning_rate': 3.218029350104822e-05, 'epoch': 1.43}\r\n",
      "{'loss': 0.0062, 'learning_rate': 3.165618448637317e-05, 'epoch': 1.47}\r\n",
      "{'loss': 0.0064, 'learning_rate': 3.113207547169811e-05, 'epoch': 1.51}\r\n",
      "{'loss': 0.0056, 'learning_rate': 3.060796645702306e-05, 'epoch': 1.55}\r\n",
      "{'loss': 0.0056, 'learning_rate': 3.0083857442348012e-05, 'epoch': 1.59}\r\n",
      "{'loss': 0.0058, 'learning_rate': 2.9559748427672958e-05, 'epoch': 1.64}\r\n",
      "{'loss': 0.0057, 'learning_rate': 2.9035639412997907e-05, 'epoch': 1.68}\r\n",
      "{'loss': 0.007, 'learning_rate': 2.851153039832285e-05, 'epoch': 1.72}\r\n",
      "{'loss': 0.0067, 'learning_rate': 2.7987421383647798e-05, 'epoch': 1.76}\r\n",
      "{'loss': 0.0046, 'learning_rate': 2.746331236897275e-05, 'epoch': 1.8}\r\n",
      "{'loss': 0.0044, 'learning_rate': 2.6939203354297693e-05, 'epoch': 1.84}\r\n",
      "{'loss': 0.0049, 'learning_rate': 2.641509433962264e-05, 'epoch': 1.89}\r\n",
      "{'loss': 0.0066, 'learning_rate': 2.589098532494759e-05, 'epoch': 1.93}\r\n",
      "{'loss': 0.005, 'learning_rate': 2.5366876310272536e-05, 'epoch': 1.97}\r\n",
      "{'loss': 0.0057, 'learning_rate': 2.4842767295597485e-05, 'epoch': 2.01}\r\n",
      "{'loss': 0.0037, 'learning_rate': 2.431865828092243e-05, 'epoch': 2.05}\r\n",
      "{'loss': 0.0047, 'learning_rate': 2.3794549266247383e-05, 'epoch': 2.1}\r\n",
      "{'loss': 0.0052, 'learning_rate': 2.327044025157233e-05, 'epoch': 2.14}\r\n",
      "{'loss': 0.0046, 'learning_rate': 2.2746331236897274e-05, 'epoch': 2.18}\r\n",
      "{'loss': 0.0043, 'learning_rate': 2.2222222222222223e-05, 'epoch': 2.22}\r\n",
      "{'loss': 0.0053, 'learning_rate': 2.1698113207547172e-05, 'epoch': 2.26}\r\n",
      "{'loss': 0.0049, 'learning_rate': 2.1174004192872118e-05, 'epoch': 2.31}\r\n",
      "{'loss': 0.0043, 'learning_rate': 2.0649895178197063e-05, 'epoch': 2.35}\r\n",
      "{'loss': 0.0031, 'learning_rate': 2.0125786163522016e-05, 'epoch': 2.39}\r\n",
      "{'loss': 0.0041, 'learning_rate': 1.960167714884696e-05, 'epoch': 2.43}\r\n",
      "{'loss': 0.0038, 'learning_rate': 1.9077568134171907e-05, 'epoch': 2.47}\r\n",
      "{'loss': 0.0034, 'learning_rate': 1.8553459119496856e-05, 'epoch': 2.52}\r\n",
      " 63%|████████████████████▊            | 30000/47700 [4:49:40<2:48:57,  1.75it/s][INFO|trainer.py:1600] 2021-06-07 09:22:25,646 >> Saving model checkpoint to ./output/checkpoint-30000\r\n",
      "[INFO|configuration_utils.py:318] 2021-06-07 09:22:25,647 >> Configuration saved in ./output/checkpoint-30000/config.json\r\n",
      "[INFO|modeling_utils.py:837] 2021-06-07 09:22:27,152 >> Model weights saved in ./output/checkpoint-30000/pytorch_model.bin\r\n",
      "[INFO|tokenization_utils_base.py:1896] 2021-06-07 09:22:27,153 >> tokenizer config file saved in ./output/checkpoint-30000/tokenizer_config.json\r\n",
      "[INFO|tokenization_utils_base.py:1902] 2021-06-07 09:22:27,154 >> Special tokens file saved in ./output/checkpoint-30000/special_tokens_map.json\r\n",
      "{'loss': 0.0043, 'learning_rate': 1.8029350104821805e-05, 'epoch': 2.56}\r\n",
      "{'loss': 0.0033, 'learning_rate': 1.750524109014675e-05, 'epoch': 2.6}\r\n",
      "{'loss': 0.0046, 'learning_rate': 1.69811320754717e-05, 'epoch': 2.64}\r\n",
      "{'loss': 0.0033, 'learning_rate': 1.645702306079665e-05, 'epoch': 2.68}\r\n",
      "{'loss': 0.0042, 'learning_rate': 1.5932914046121594e-05, 'epoch': 2.73}\r\n",
      "{'loss': 0.0045, 'learning_rate': 1.540880503144654e-05, 'epoch': 2.77}\r\n",
      "{'loss': 0.0033, 'learning_rate': 1.488469601677149e-05, 'epoch': 2.81}\r\n",
      "{'loss': 0.0033, 'learning_rate': 1.4360587002096438e-05, 'epoch': 2.85}\r\n",
      "{'loss': 0.0032, 'learning_rate': 1.3836477987421385e-05, 'epoch': 2.89}\r\n",
      "{'loss': 0.0031, 'learning_rate': 1.331236897274633e-05, 'epoch': 2.94}\r\n",
      "{'loss': 0.003, 'learning_rate': 1.2788259958071281e-05, 'epoch': 2.98}\r\n",
      "{'loss': 0.0043, 'learning_rate': 1.2264150943396227e-05, 'epoch': 3.02}\r\n",
      "{'loss': 0.0027, 'learning_rate': 1.1740041928721176e-05, 'epoch': 3.06}\r\n",
      "{'loss': 0.0021, 'learning_rate': 1.1215932914046121e-05, 'epoch': 3.1}\r\n",
      "{'loss': 0.0034, 'learning_rate': 1.069182389937107e-05, 'epoch': 3.14}\r\n",
      "{'loss': 0.003, 'learning_rate': 1.0167714884696017e-05, 'epoch': 3.19}\r\n",
      "{'loss': 0.0033, 'learning_rate': 9.643605870020965e-06, 'epoch': 3.23}\r\n",
      "{'loss': 0.0033, 'learning_rate': 9.119496855345912e-06, 'epoch': 3.27}\r\n",
      "{'loss': 0.0021, 'learning_rate': 8.59538784067086e-06, 'epoch': 3.31}\r\n",
      "{'loss': 0.0025, 'learning_rate': 8.071278825995808e-06, 'epoch': 3.35}\r\n",
      "{'loss': 0.0025, 'learning_rate': 7.547169811320755e-06, 'epoch': 3.4}\r\n",
      "{'loss': 0.0025, 'learning_rate': 7.023060796645703e-06, 'epoch': 3.44}\r\n",
      "{'loss': 0.0024, 'learning_rate': 6.49895178197065e-06, 'epoch': 3.48}\r\n",
      "{'loss': 0.0025, 'learning_rate': 5.974842767295598e-06, 'epoch': 3.52}\r\n",
      "{'loss': 0.0027, 'learning_rate': 5.4507337526205454e-06, 'epoch': 3.56}\r\n",
      "{'loss': 0.0029, 'learning_rate': 4.926624737945493e-06, 'epoch': 3.61}\r\n",
      "{'loss': 0.0031, 'learning_rate': 4.40251572327044e-06, 'epoch': 3.65}\r\n",
      "{'loss': 0.0028, 'learning_rate': 3.878406708595388e-06, 'epoch': 3.69}\r\n",
      "{'loss': 0.0022, 'learning_rate': 3.354297693920336e-06, 'epoch': 3.73}\r\n",
      "{'loss': 0.0033, 'learning_rate': 2.830188679245283e-06, 'epoch': 3.77}\r\n",
      " 94%|█████████████████████████████████  | 45000/47700 [7:14:12<25:56,  1.74it/s][INFO|trainer.py:1600] 2021-06-07 11:46:57,775 >> Saving model checkpoint to ./output/checkpoint-45000\r\n",
      "[INFO|configuration_utils.py:318] 2021-06-07 11:46:57,777 >> Configuration saved in ./output/checkpoint-45000/config.json\r\n",
      "[INFO|modeling_utils.py:837] 2021-06-07 11:46:59,382 >> Model weights saved in ./output/checkpoint-45000/pytorch_model.bin\r\n",
      "[INFO|tokenization_utils_base.py:1896] 2021-06-07 11:46:59,383 >> tokenizer config file saved in ./output/checkpoint-45000/tokenizer_config.json\r\n",
      "[INFO|tokenization_utils_base.py:1902] 2021-06-07 11:46:59,383 >> Special tokens file saved in ./output/checkpoint-45000/special_tokens_map.json\r\n",
      "{'loss': 0.0028, 'learning_rate': 2.306079664570231e-06, 'epoch': 3.82}\r\n",
      "{'loss': 0.0027, 'learning_rate': 1.781970649895178e-06, 'epoch': 3.86}\r\n",
      "{'loss': 0.003, 'learning_rate': 1.257861635220126e-06, 'epoch': 3.9}\r\n",
      "{'loss': 0.0026, 'learning_rate': 7.337526205450735e-07, 'epoch': 3.94}\r\n",
      "{'loss': 0.0032, 'learning_rate': 2.09643605870021e-07, 'epoch': 3.98}\r\n",
      "100%|███████████████████████████████████| 47700/47700 [7:40:15<00:00,  2.04it/s][INFO|trainer.py:1171] 2021-06-07 12:13:00,800 >> \r\n",
      "\r\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\r\n",
      "\r\n",
      "\r\n",
      "{'train_runtime': 27615.8069, 'train_samples_per_second': 1.727, 'epoch': 4.0}\r\n",
      "100%|███████████████████████████████████| 47700/47700 [7:40:15<00:00,  1.73it/s]\r\n",
      "[INFO|trainer.py:1600] 2021-06-07 12:13:01,171 >> Saving model checkpoint to ./output\r\n",
      "[INFO|configuration_utils.py:318] 2021-06-07 12:13:01,172 >> Configuration saved in ./output/config.json\r\n",
      "[INFO|modeling_utils.py:837] 2021-06-07 12:13:02,549 >> Model weights saved in ./output/pytorch_model.bin\r\n",
      "[INFO|tokenization_utils_base.py:1896] 2021-06-07 12:13:02,550 >> tokenizer config file saved in ./output/tokenizer_config.json\r\n",
      "[INFO|tokenization_utils_base.py:1902] 2021-06-07 12:13:02,550 >> Special tokens file saved in ./output/special_tokens_map.json\r\n",
      "[INFO|trainer_pt_utils.py:735] 2021-06-07 12:13:02,585 >> ***** train metrics *****\r\n",
      "[INFO|trainer_pt_utils.py:740] 2021-06-07 12:13:02,585 >>   epoch                      =        4.0\r\n",
      "[INFO|trainer_pt_utils.py:740] 2021-06-07 12:13:02,585 >>   init_mem_cpu_alloc_delta   =     1436MB\r\n",
      "[INFO|trainer_pt_utils.py:740] 2021-06-07 12:13:02,586 >>   init_mem_cpu_peaked_delta  =      307MB\r\n",
      "[INFO|trainer_pt_utils.py:740] 2021-06-07 12:13:02,586 >>   init_mem_gpu_alloc_delta   =      418MB\r\n",
      "[INFO|trainer_pt_utils.py:740] 2021-06-07 12:13:02,586 >>   init_mem_gpu_peaked_delta  =        0MB\r\n",
      "[INFO|trainer_pt_utils.py:740] 2021-06-07 12:13:02,586 >>   train_mem_cpu_alloc_delta  =      742MB\r\n",
      "[INFO|trainer_pt_utils.py:740] 2021-06-07 12:13:02,586 >>   train_mem_cpu_peaked_delta =      273MB\r\n",
      "[INFO|trainer_pt_utils.py:740] 2021-06-07 12:13:02,586 >>   train_mem_gpu_alloc_delta  =     1317MB\r\n",
      "[INFO|trainer_pt_utils.py:740] 2021-06-07 12:13:02,586 >>   train_mem_gpu_peaked_delta =     6202MB\r\n",
      "[INFO|trainer_pt_utils.py:740] 2021-06-07 12:13:02,586 >>   train_runtime              = 7:40:15.80\r\n",
      "[INFO|trainer_pt_utils.py:740] 2021-06-07 12:13:02,586 >>   train_samples              =      95394\r\n",
      "[INFO|trainer_pt_utils.py:740] 2021-06-07 12:13:02,586 >>   train_samples_per_second   =      1.727\r\n"
     ]
    }
   ],
   "source": [
    "#Train SciBERT model\n",
    "batch_size = 8\n",
    "train_scibert_ner(batch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 12.794884,
     "end_time": "2021-06-07T12:13:29.581010",
     "exception": false,
     "start_time": "2021-06-07T12:13:16.786126",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# 4. Test the SciBERT model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-06-07T12:13:55.285023Z",
     "iopub.status.busy": "2021-06-07T12:13:55.284272Z",
     "iopub.status.idle": "2021-06-07T12:13:55.287119Z",
     "shell.execute_reply": "2021-06-07T12:13:55.286700Z",
     "shell.execute_reply.started": "2021-06-05T07:55:06.529303Z"
    },
    "papermill": {
     "duration": 12.685189,
     "end_time": "2021-06-07T12:13:55.287243",
     "exception": false,
     "start_time": "2021-06-07T12:13:42.602054",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# max_length = 64 # max no. words for each sentence.\n",
    "# overlap = 20 # if a sentence exceeds MAX_LENGTH, we split it to multiple sentences with overlapping\n",
    "\n",
    "# pred_save_path = './pred'\n",
    "# prediction_file = 'test_predictions.txt'\n",
    "# test_input_save_path = './input_data'\n",
    "# path_pretrained_scibert = '/kaggle/working/output'\n",
    "# train_file = path_train_nerjson\n",
    "# filename_test = 'test_ner_input.json'\n",
    "\n",
    "# os.environ[\"MODEL_PATH\"] = f\"{path_pretrained_scibert}\"\n",
    "# os.environ[\"TRAIN_FILE\"] = f\"{train_file}\"\n",
    "# os.environ[\"VALIDATION_FILE\"] = f\"{train_file}\"\n",
    "# os.environ[\"TEST_FILE\"] = f\"{test_input_save_path}/{filename_test}\"\n",
    "# os.environ[\"OUTPUT_DIR\"] = f\"{pred_save_path}\"\n",
    "\n",
    "\n",
    "# os.environ[\"TEST_FILE\"] = f\"{TEST_INPUT_SAVE_PATH}/{TEST_NER_DATA_FILE}\"\n",
    "# os.environ[\"OUTPUT_DIR\"] = f\"{PREDICTION_SAVE_PATH}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-06-07T12:14:20.667460Z",
     "iopub.status.busy": "2021-06-07T12:14:20.666653Z",
     "iopub.status.idle": "2021-06-07T12:14:20.669136Z",
     "shell.execute_reply": "2021-06-07T12:14:20.669509Z",
     "shell.execute_reply.started": "2021-06-05T07:55:06.537723Z"
    },
    "papermill": {
     "duration": 12.866834,
     "end_time": "2021-06-07T12:14:20.669647",
     "exception": false,
     "start_time": "2021-06-07T12:14:07.802813",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# paper_length = [] # store the number of sentences each paper has\n",
    "# def prepare_testdata(filename):\n",
    "#     test_rows = []\n",
    "#     with open(filename) as f:\n",
    "#         for row in f:\n",
    "#             json_row = json.loads(row)\n",
    "#             test_rows.append(json_row)\n",
    "#     return test_rows\n",
    "    \n",
    "# test_rows = prepare_testdata(train_file) #test data in NER format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-06-07T12:14:45.731374Z",
     "iopub.status.busy": "2021-06-07T12:14:45.730473Z",
     "iopub.status.idle": "2021-06-07T12:14:45.732679Z",
     "shell.execute_reply": "2021-06-07T12:14:45.732050Z",
     "shell.execute_reply.started": "2021-06-05T07:55:06.550061Z"
    },
    "papermill": {
     "duration": 12.706902,
     "end_time": "2021-06-07T12:14:45.732831",
     "exception": false,
     "start_time": "2021-06-07T12:14:33.025929",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# print(len(test_rows))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-06-07T12:15:10.699020Z",
     "iopub.status.busy": "2021-06-07T12:15:10.698246Z",
     "iopub.status.idle": "2021-06-07T12:15:10.701014Z",
     "shell.execute_reply": "2021-06-07T12:15:10.700567Z",
     "shell.execute_reply.started": "2021-05-28T20:24:06.009573Z"
    },
    "papermill": {
     "duration": 12.466078,
     "end_time": "2021-06-07T12:15:10.701124",
     "exception": false,
     "start_time": "2021-06-07T12:14:58.235046",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# os.makedirs(test_input_save_path, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-06-07T12:15:36.026161Z",
     "iopub.status.busy": "2021-06-07T12:15:36.025354Z",
     "iopub.status.idle": "2021-06-07T12:15:36.028400Z",
     "shell.execute_reply": "2021-06-07T12:15:36.027896Z",
     "shell.execute_reply.started": "2021-05-28T20:24:06.295713Z"
    },
    "papermill": {
     "duration": 12.355108,
     "end_time": "2021-06-07T12:15:36.028510",
     "exception": false,
     "start_time": "2021-06-07T12:15:23.673402",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# def predict_scibert_ner():\n",
    "#     !python ../input/tdmsci/run_ner.py \\\n",
    "#     --model_name_or_path \"$MODEL_PATH\" \\\n",
    "#     --train_file \"$TRAIN_FILE\" \\\n",
    "#     --validation_file \"$VALIDATION_FILE\" \\\n",
    "#     --test_file \"$TEST_FILE\" \\\n",
    "#     --output_dir \"$OUTPUT_DIR\" \\\n",
    "#     --report_to 'none' \\\n",
    "#     --seed 123 \\\n",
    "#     --do_predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-06-07T12:16:00.980189Z",
     "iopub.status.busy": "2021-06-07T12:16:00.979343Z",
     "iopub.status.idle": "2021-06-07T12:16:00.982029Z",
     "shell.execute_reply": "2021-06-07T12:16:00.981583Z",
     "shell.execute_reply.started": "2021-05-28T20:24:06.542665Z"
    },
    "papermill": {
     "duration": 12.764014,
     "end_time": "2021-06-07T12:16:00.982141",
     "exception": false,
     "start_time": "2021-06-07T12:15:48.218127",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# bert_outputs = []\n",
    "# batch_size = 2000#64000\n",
    "\n",
    "\n",
    "# for batch_begin in range(0, len(test_rows), batch_size):#len(test_rows), batch_size):\n",
    "#     # write data rows to input file\n",
    "#     with open(f\"{test_input_save_path}/{filename_test}\", 'w') as f:\n",
    "#         for row in test_rows[batch_begin:batch_begin+batch_size]:\n",
    "#             print(row[\"id\"])\n",
    "#             json.dump(row, f)\n",
    "#             f.write('\\n')\n",
    "            \n",
    "#     with open(f\"{test_input_save_path}/{filename_test}\", 'r') as f:\n",
    "#         content = f.read()\n",
    "        \n",
    "#     # remove output dir\n",
    "#     !rm -r \"$OUTPUT_DIR\"\n",
    "    \n",
    "#     # do predict\n",
    "#     predict_scibert_ner()\n",
    "    \n",
    "#     # read predictions\n",
    "#     with open(f'{pred_save_path}/{prediction_file}') as f:\n",
    "#         this_preds = f.read().split('\\n')[:-1]\n",
    "#         bert_outputs += [pred.split() for pred in this_preds]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-06-07T12:16:26.259163Z",
     "iopub.status.busy": "2021-06-07T12:16:26.258400Z",
     "iopub.status.idle": "2021-06-07T12:16:26.261172Z",
     "shell.execute_reply": "2021-06-07T12:16:26.260754Z",
     "shell.execute_reply.started": "2021-05-28T20:24:07.156551Z"
    },
    "papermill": {
     "duration": 12.930757,
     "end_time": "2021-06-07T12:16:26.261294",
     "exception": false,
     "start_time": "2021-06-07T12:16:13.330537",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# print(bert_outputs)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 12.617769,
     "end_time": "2021-06-07T12:16:51.243236",
     "exception": false,
     "start_time": "2021-06-07T12:16:38.625467",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# 5. Restore labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-06-07T12:17:16.438560Z",
     "iopub.status.busy": "2021-06-07T12:17:16.437713Z",
     "iopub.status.idle": "2021-06-07T12:17:16.440417Z",
     "shell.execute_reply": "2021-06-07T12:17:16.439987Z",
     "shell.execute_reply.started": "2021-05-28T20:24:09.39267Z"
    },
    "papermill": {
     "duration": 12.300583,
     "end_time": "2021-06-07T12:17:16.440523",
     "exception": false,
     "start_time": "2021-06-07T12:17:04.139940",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# def jaccard_similarity(s1, s2):\n",
    "#     l1 = s1.split(\" \")\n",
    "#     l2 = s2.split(\" \")    \n",
    "#     intersection = len(list(set(l1).intersection(l2)))\n",
    "#     union = (len(l1) + len(l2)) - intersection\n",
    "#     return float(intersection) / union\n",
    "\n",
    "# def filter_bert_labels(dataset_labels, ):\n",
    "#     for labels in dataset_labels:\n",
    "#         print(labels)\n",
    "# #print(len(list(test_df_ner['Sentences'])[0]))\n",
    "# #filter_bert_labels(bert_outputs)\n",
    "# #amountSentences = [len(list(test_df['Sentences'])[0]) ]\n",
    "# #sentences = list(test_df_ner[\"Sentences\"])[0]\n",
    "# # for i in range(4,5):#(0,len(sentences)):\n",
    "# test_sentences = [row['tokens'] for row in test_rows]\n",
    "# test_ids = list(test_df_ner[\"Id\"])\n",
    "# labels = []\n",
    "# for length_i in range(len(paper_length)):\n",
    "#     paper_id = test_ids[length_i]\n",
    "#     print(paper_id)\n",
    "#     for sentence, pred in zip(test_sentences[:paper_length[length_i]], bert_outputs[:paper_length[length_i]]):\n",
    "#         dataset = \"\"\n",
    "#         for word, tag in zip(sentence, pred):\n",
    "#             #print(word)\n",
    "#             #print(tags)\n",
    "#             x = 0\n",
    "#             if(tag == 'B-DATASET'):\n",
    "#                 dataset += tag + ' '\n",
    "#             elif(tag == \"I-DATASET\" and dataset != \"\"):\n",
    "#                 dataset += tag + ' '\n",
    "#             elif(tag != \"B-DATASET\" and tag != \"I-DATASET\" and dataset != \"\"):\n",
    "#                 labels.append(dataset)\n",
    "#                 dataset = \"\" \n",
    "                                \n",
    "#     if(dataset == \"\" and len(labels) < length_i+1):\n",
    "#         labels.append(\"\")\n",
    "\n",
    "#     del test_sentences[:length_i], bert_outputs[:length_i]\n",
    "\n",
    "\n",
    "# #        print(len(tokens))\n",
    "# #         print(len(pred))\n",
    "#     #words = [tup[0] for tup in sentences[i]]\n",
    "    \n",
    "# #         print(sentence)\n",
    "# #         print(pred)\n",
    "# # for labels in bert_dataset_labels:\n",
    "# #     filtered = []\n",
    "    \n",
    "# #     for label in sorted(labels, key=len):\n",
    "# #         label = clean_text(label)\n",
    "# #         if len(filtered) == 0 or all(jaccard_similarity(label, got_label) < 0.75 for got_label in filtered):\n",
    "# #             filtered.append(label)\n",
    "    \n",
    "# #     filtered_bert_labels.append('|'.join(filtered))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 12.402612,
     "end_time": "2021-06-07T12:17:41.556555",
     "exception": false,
     "start_time": "2021-06-07T12:17:29.153943",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# 6. Generate submission file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-06-07T12:18:06.934054Z",
     "iopub.status.busy": "2021-06-07T12:18:06.933287Z",
     "iopub.status.idle": "2021-06-07T12:18:06.936200Z",
     "shell.execute_reply": "2021-06-07T12:18:06.935761Z",
     "shell.execute_reply.started": "2021-05-28T20:24:09.864036Z"
    },
    "papermill": {
     "duration": 12.963692,
     "end_time": "2021-06-07T12:18:06.936316",
     "exception": false,
     "start_time": "2021-06-07T12:17:53.972624",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# def generate_submission_file(test_ids, test_predictions):\n",
    "#     submission_dict = {\"Id\": test_ids, \"PredictionString\": test_predictions}\n",
    "#     submission_df = pd.DataFrame.from_dict(submission_dict)\n",
    "#     sample_submission.to_csv(f'submission.csv', index=False)\n",
    "\n",
    "# #Import extra training data https://github.com/IBM/science-result-extractor/tree/master/data\n",
    "# # import os\n",
    "# # import re \n",
    "\n",
    "# generate_submission_file(test_ids, labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "papermill": {
     "duration": 12.820231,
     "end_time": "2021-06-07T12:18:32.196472",
     "exception": false,
     "start_time": "2021-06-07T12:18:19.376241",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "papermill": {
     "duration": 12.320386,
     "end_time": "2021-06-07T12:18:56.863588",
     "exception": false,
     "start_time": "2021-06-07T12:18:44.543202",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "papermill": {
     "duration": 12.501139,
     "end_time": "2021-06-07T12:19:22.193394",
     "exception": false,
     "start_time": "2021-06-07T12:19:09.692255",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 28170.045377,
   "end_time": "2021-06-07T12:19:36.167415",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2021-06-07T04:30:06.122038",
   "version": "2.3.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
