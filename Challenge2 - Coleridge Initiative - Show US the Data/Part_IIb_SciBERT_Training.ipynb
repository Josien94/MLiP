{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.019343,
     "end_time": "2021-06-07T04:30:11.340309",
     "exception": false,
     "start_time": "2021-06-07T04:30:11.320966",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# PART IIb (Training SciBERT)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.018127,
     "end_time": "2021-06-07T04:30:11.377029",
     "exception": false,
     "start_time": "2021-06-07T04:30:11.358902",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# 1. Introduction\n",
    "\n",
    "This notebook contains the training procedure for SciBERT for the Kaggle challenge \"Coleridge Initative: Show US the Data\" (https://www.kaggle.com/c/coleridgeinitiative-show-us-the-data/). As a recap, this challenge is about recognizing public datasets used in scientific papers. In particular, we want to extract the datasets for scientific paper, with several NLP approaches. In this notebook, we test both BERT and SciBERT. The first model is introduced by Devlin, J., Chang, M. W., Lee, K., and Toutanova, K., in 2018 [1]. Source code of BERT can be fuond [here](https://github.com/google-research/bert). The second model is  introduced by Beltagy, I., Lo, K., and Cohan, A. in 2019 [2]. Source code of SciBERT can be found [here](https://github.com/allenai/scibert).\n",
    "\n",
    "Furthermore, we append the existing data with a specialized Corpus for dataset tagging. TDMSci is a Corpus existing of annotated data for tasks, metrices and datasets. Here, B-DATASET and I-DATASET are the NER-labels indicating a word is (part of) a dataset [3]. Source code (and annotated data) of TDMSci can be found [here](https://github.com/IBM/science-result-extractor).\n",
    "\n",
    "\n",
    "We have created three notebooks, one for **dataset creation** ([Part I](https://github.com/Josien94/MLiP/blob/main/Challenge2%20-%20Coleridge%20Initiative%20-%20Show%20US%20the%20Data/Part%20I_Creating_Dataset.ipynb)), one for **training** ([Part IIa](https://github.com/Josien94/MLiP/blob/main/Challenge2%20-%20Coleridge%20Initiative%20-%20Show%20US%20the%20Data/Part_IIa_BERT_Training.ipynb) and Part IIb) and one for **testing** ([Part III](https://github.com/Josien94/MLiP/blob/main/Challenge2%20-%20Coleridge%20Initiative%20-%20Show%20US%20the%20Data/Part%20III_With_LiteralMatching_SciBERT.ipynb)). This notebook trains a BERT model on our own created dataset.\n",
    "\n",
    "\n",
    "[1] Devlin, J., Chang, M. W., Lee, K., & Toutanova, K. (2018). Bert: Pre-training of deep bidirectional transformers for language understanding. arXiv preprint arXiv:1810.04805.\n",
    "[2] Beltagy, I., Lo, K., & Cohan, A. (2019). SciBERT: A pretrained language model for scientific text. arXiv preprint arXiv:1903.10676.  \n",
    "[3] Hou, Y., Jochim, C., Gleize, M., Bonin, F., & Ganguly, D. (2021). TDMSci: A Specialized Corpus for Scientific Literature Entity Tagging of Tasks Datasets and Metrics. arXiv preprint arXiv:2101.10273."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.017927,
     "end_time": "2021-06-07T04:30:11.413182",
     "exception": false,
     "start_time": "2021-06-07T04:30:11.395255",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# 2. Preparing Notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-06-07T04:30:11.456464Z",
     "iopub.status.busy": "2021-06-07T04:30:11.454971Z",
     "iopub.status.idle": "2021-06-07T04:30:44.576065Z",
     "shell.execute_reply": "2021-06-07T04:30:44.575449Z",
     "shell.execute_reply.started": "2021-06-05T07:53:42.653554Z"
    },
    "papermill": {
     "duration": 33.14457,
     "end_time": "2021-06-07T04:30:44.576240",
     "exception": false,
     "start_time": "2021-06-07T04:30:11.431670",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in links: file:///kaggle/input/coleridge-packages/packages/datasets\r\n",
      "Processing /kaggle/input/coleridge-packages/packages/datasets/datasets-1.5.0-py3-none-any.whl\r\n",
      "Requirement already satisfied: pyarrow>=0.17.1 in /opt/conda/lib/python3.7/site-packages (from datasets) (1.0.1)\r\n",
      "Processing /kaggle/input/coleridge-packages/packages/datasets/xxhash-2.0.0-cp37-cp37m-manylinux2010_x86_64.whl\r\n",
      "Requirement already satisfied: fsspec in /opt/conda/lib/python3.7/site-packages (from datasets) (0.8.5)\r\n",
      "Requirement already satisfied: requests>=2.19.0 in /opt/conda/lib/python3.7/site-packages (from datasets) (2.25.1)\r\n",
      "Requirement already satisfied: numpy>=1.17 in /opt/conda/lib/python3.7/site-packages (from datasets) (1.19.5)\r\n",
      "Processing /kaggle/input/coleridge-packages/packages/datasets/tqdm-4.49.0-py2.py3-none-any.whl\r\n",
      "Requirement already satisfied: pandas in /opt/conda/lib/python3.7/site-packages (from datasets) (1.1.5)\r\n",
      "Requirement already satisfied: dill in /opt/conda/lib/python3.7/site-packages (from datasets) (0.3.3)\r\n",
      "Requirement already satisfied: multiprocess in /opt/conda/lib/python3.7/site-packages (from datasets) (0.70.11.1)\r\n",
      "Requirement already satisfied: importlib-metadata in /opt/conda/lib/python3.7/site-packages (from datasets) (3.4.0)\r\n",
      "Processing /kaggle/input/coleridge-packages/packages/datasets/huggingface_hub-0.0.7-py3-none-any.whl\r\n",
      "Requirement already satisfied: filelock in /opt/conda/lib/python3.7/site-packages (from huggingface-hub<0.1.0->datasets) (3.0.12)\r\n",
      "Requirement already satisfied: idna<3,>=2.5 in /opt/conda/lib/python3.7/site-packages (from requests>=2.19.0->datasets) (2.10)\r\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.7/site-packages (from requests>=2.19.0->datasets) (2020.12.5)\r\n",
      "Requirement already satisfied: chardet<5,>=3.0.2 in /opt/conda/lib/python3.7/site-packages (from requests>=2.19.0->datasets) (3.0.4)\r\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /opt/conda/lib/python3.7/site-packages (from requests>=2.19.0->datasets) (1.26.3)\r\n",
      "Requirement already satisfied: typing-extensions>=3.6.4 in /opt/conda/lib/python3.7/site-packages (from importlib-metadata->datasets) (3.7.4.3)\r\n",
      "Requirement already satisfied: zipp>=0.5 in /opt/conda/lib/python3.7/site-packages (from importlib-metadata->datasets) (3.4.0)\r\n",
      "Requirement already satisfied: python-dateutil>=2.7.3 in /opt/conda/lib/python3.7/site-packages (from pandas->datasets) (2.8.1)\r\n",
      "Requirement already satisfied: pytz>=2017.2 in /opt/conda/lib/python3.7/site-packages (from pandas->datasets) (2021.1)\r\n",
      "Requirement already satisfied: six>=1.5 in /opt/conda/lib/python3.7/site-packages (from python-dateutil>=2.7.3->pandas->datasets) (1.15.0)\r\n",
      "Installing collected packages: tqdm, xxhash, huggingface-hub, datasets\r\n",
      "  Attempting uninstall: tqdm\r\n",
      "    Found existing installation: tqdm 4.56.2\r\n",
      "    Uninstalling tqdm-4.56.2:\r\n",
      "      Successfully uninstalled tqdm-4.56.2\r\n",
      "Successfully installed datasets-1.5.0 huggingface-hub-0.0.7 tqdm-4.49.0 xxhash-2.0.0\r\n",
      "Processing /kaggle/input/coleridge-packages/seqeval-1.2.2-py3-none-any.whl\r\n",
      "Requirement already satisfied: numpy>=1.14.0 in /opt/conda/lib/python3.7/site-packages (from seqeval==1.2.2) (1.19.5)\r\n",
      "Requirement already satisfied: scikit-learn>=0.21.3 in /opt/conda/lib/python3.7/site-packages (from seqeval==1.2.2) (0.24.1)\r\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in /opt/conda/lib/python3.7/site-packages (from scikit-learn>=0.21.3->seqeval==1.2.2) (2.1.0)\r\n",
      "Requirement already satisfied: scipy>=0.19.1 in /opt/conda/lib/python3.7/site-packages (from scikit-learn>=0.21.3->seqeval==1.2.2) (1.5.4)\r\n",
      "Requirement already satisfied: joblib>=0.11 in /opt/conda/lib/python3.7/site-packages (from scikit-learn>=0.21.3->seqeval==1.2.2) (1.0.1)\r\n",
      "Installing collected packages: seqeval\r\n",
      "Successfully installed seqeval-1.2.2\r\n",
      "Processing /kaggle/input/coleridge-packages/tokenizers-0.10.1-cp37-cp37m-manylinux1_x86_64.whl\r\n",
      "tokenizers is already installed with the same version as the provided wheel. Use --force-reinstall to force an installation of the wheel.\r\n",
      "Processing /kaggle/input/coleridge-packages/transformers-4.5.0.dev0-py3-none-any.whl\r\n",
      "Requirement already satisfied: sacremoses in /opt/conda/lib/python3.7/site-packages (from transformers==4.5.0.dev0) (0.0.43)\r\n",
      "Requirement already satisfied: importlib-metadata in /opt/conda/lib/python3.7/site-packages (from transformers==4.5.0.dev0) (3.4.0)\r\n",
      "Requirement already satisfied: filelock in /opt/conda/lib/python3.7/site-packages (from transformers==4.5.0.dev0) (3.0.12)\r\n",
      "Requirement already satisfied: numpy>=1.17 in /opt/conda/lib/python3.7/site-packages (from transformers==4.5.0.dev0) (1.19.5)\r\n",
      "Requirement already satisfied: tokenizers<0.11,>=0.10.1 in /opt/conda/lib/python3.7/site-packages (from transformers==4.5.0.dev0) (0.10.1)\r\n",
      "Requirement already satisfied: regex!=2019.12.17 in /opt/conda/lib/python3.7/site-packages (from transformers==4.5.0.dev0) (2020.11.13)\r\n",
      "Requirement already satisfied: packaging in /opt/conda/lib/python3.7/site-packages (from transformers==4.5.0.dev0) (20.9)\r\n",
      "Requirement already satisfied: tqdm>=4.27 in /opt/conda/lib/python3.7/site-packages (from transformers==4.5.0.dev0) (4.49.0)\r\n",
      "Requirement already satisfied: requests in /opt/conda/lib/python3.7/site-packages (from transformers==4.5.0.dev0) (2.25.1)\r\n",
      "Requirement already satisfied: typing-extensions>=3.6.4 in /opt/conda/lib/python3.7/site-packages (from importlib-metadata->transformers==4.5.0.dev0) (3.7.4.3)\r\n",
      "Requirement already satisfied: zipp>=0.5 in /opt/conda/lib/python3.7/site-packages (from importlib-metadata->transformers==4.5.0.dev0) (3.4.0)\r\n",
      "Requirement already satisfied: pyparsing>=2.0.2 in /opt/conda/lib/python3.7/site-packages (from packaging->transformers==4.5.0.dev0) (2.4.7)\r\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /opt/conda/lib/python3.7/site-packages (from requests->transformers==4.5.0.dev0) (1.26.3)\r\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.7/site-packages (from requests->transformers==4.5.0.dev0) (2020.12.5)\r\n",
      "Requirement already satisfied: chardet<5,>=3.0.2 in /opt/conda/lib/python3.7/site-packages (from requests->transformers==4.5.0.dev0) (3.0.4)\r\n",
      "Requirement already satisfied: idna<3,>=2.5 in /opt/conda/lib/python3.7/site-packages (from requests->transformers==4.5.0.dev0) (2.10)\r\n",
      "Requirement already satisfied: six in /opt/conda/lib/python3.7/site-packages (from sacremoses->transformers==4.5.0.dev0) (1.15.0)\r\n",
      "Requirement already satisfied: click in /opt/conda/lib/python3.7/site-packages (from sacremoses->transformers==4.5.0.dev0) (7.1.2)\r\n",
      "Requirement already satisfied: joblib in /opt/conda/lib/python3.7/site-packages (from sacremoses->transformers==4.5.0.dev0) (1.0.1)\r\n",
      "Installing collected packages: transformers\r\n",
      "  Attempting uninstall: transformers\r\n",
      "    Found existing installation: transformers 4.4.2\r\n",
      "    Uninstalling transformers-4.4.2:\r\n",
      "      Successfully uninstalled transformers-4.4.2\r\n",
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\r\n",
      "allennlp 2.2.0 requires transformers<4.5,>=4.1, but you have transformers 4.5.0.dev0 which is incompatible.\u001b[0m\r\n",
      "Successfully installed transformers-4.5.0.dev0\r\n",
      "Requirement already satisfied: datasets in /opt/conda/lib/python3.7/site-packages (1.5.0)\r\n",
      "Requirement already satisfied: fsspec in /opt/conda/lib/python3.7/site-packages (from datasets) (0.8.5)\r\n",
      "Requirement already satisfied: numpy>=1.17 in /opt/conda/lib/python3.7/site-packages (from datasets) (1.19.5)\r\n",
      "Requirement already satisfied: importlib-metadata in /opt/conda/lib/python3.7/site-packages (from datasets) (3.4.0)\r\n",
      "Requirement already satisfied: xxhash in /opt/conda/lib/python3.7/site-packages (from datasets) (2.0.0)\r\n",
      "Requirement already satisfied: tqdm<4.50.0,>=4.27 in /opt/conda/lib/python3.7/site-packages (from datasets) (4.49.0)\r\n",
      "Requirement already satisfied: pyarrow>=0.17.1 in /opt/conda/lib/python3.7/site-packages (from datasets) (1.0.1)\r\n",
      "Requirement already satisfied: huggingface-hub<0.1.0 in /opt/conda/lib/python3.7/site-packages (from datasets) (0.0.7)\r\n",
      "Requirement already satisfied: multiprocess in /opt/conda/lib/python3.7/site-packages (from datasets) (0.70.11.1)\r\n",
      "Requirement already satisfied: requests>=2.19.0 in /opt/conda/lib/python3.7/site-packages (from datasets) (2.25.1)\r\n",
      "Requirement already satisfied: dill in /opt/conda/lib/python3.7/site-packages (from datasets) (0.3.3)\r\n",
      "Requirement already satisfied: pandas in /opt/conda/lib/python3.7/site-packages (from datasets) (1.1.5)\r\n",
      "Requirement already satisfied: filelock in /opt/conda/lib/python3.7/site-packages (from huggingface-hub<0.1.0->datasets) (3.0.12)\r\n",
      "Requirement already satisfied: idna<3,>=2.5 in /opt/conda/lib/python3.7/site-packages (from requests>=2.19.0->datasets) (2.10)\r\n",
      "Requirement already satisfied: chardet<5,>=3.0.2 in /opt/conda/lib/python3.7/site-packages (from requests>=2.19.0->datasets) (3.0.4)\r\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /opt/conda/lib/python3.7/site-packages (from requests>=2.19.0->datasets) (1.26.3)\r\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.7/site-packages (from requests>=2.19.0->datasets) (2020.12.5)\r\n",
      "Requirement already satisfied: zipp>=0.5 in /opt/conda/lib/python3.7/site-packages (from importlib-metadata->datasets) (3.4.0)\r\n",
      "Requirement already satisfied: typing-extensions>=3.6.4 in /opt/conda/lib/python3.7/site-packages (from importlib-metadata->datasets) (3.7.4.3)\r\n",
      "Requirement already satisfied: python-dateutil>=2.7.3 in /opt/conda/lib/python3.7/site-packages (from pandas->datasets) (2.8.1)\r\n",
      "Requirement already satisfied: pytz>=2017.2 in /opt/conda/lib/python3.7/site-packages (from pandas->datasets) (2021.1)\r\n",
      "Requirement already satisfied: six>=1.5 in /opt/conda/lib/python3.7/site-packages (from python-dateutil>=2.7.3->pandas->datasets) (1.15.0)\r\n"
     ]
    }
   ],
   "source": [
    "!pip install datasets --no-index --find-links=file:///kaggle/input/coleridge-packages/packages/datasets \n",
    "!pip install ../input/coleridge-packages/seqeval-1.2.2-py3-none-any.whl \n",
    "!pip install ../input/coleridge-packages/tokenizers-0.10.1-cp37-cp37m-manylinux1_x86_64.whl \n",
    "!pip install ../input/coleridge-packages/transformers-4.5.0.dev0-py3-none-any.whl \n",
    "!pip install datasets "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-06-07T04:30:44.686742Z",
     "iopub.status.busy": "2021-06-07T04:30:44.684225Z",
     "iopub.status.idle": "2021-06-07T04:30:55.906631Z",
     "shell.execute_reply": "2021-06-07T04:30:55.907326Z",
     "shell.execute_reply.started": "2021-06-05T07:54:15.592982Z"
    },
    "papermill": {
     "duration": 11.253986,
     "end_time": "2021-06-07T04:30:55.907482",
     "exception": false,
     "start_time": "2021-06-07T04:30:44.653496",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.7/site-packages/torchaudio/backend/utils.py:54: UserWarning: \"sox\" backend is being deprecated. The default backend will be changed to \"sox_io\" backend in 0.8.0 and \"sox\" backend will be removed in 0.9.0. Please migrate to \"sox_io\" backend. Please refer to https://github.com/pytorch/audio/issues/903 for the detail.\n",
      "  '\"sox\" backend is being deprecated. '\n"
     ]
    }
   ],
   "source": [
    "#Import necessary libraries\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "import nltk\n",
    "import re\n",
    "import os\n",
    "from os import listdir\n",
    "\n",
    "from os.path import isfile, join\n",
    "import re\n",
    "import json\n",
    "import time\n",
    "import datetime\n",
    "import random\n",
    "import glob\n",
    "import importlib\n",
    "import allennlp\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from transformers import *\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import torch\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-06-07T04:30:55.966383Z",
     "iopub.status.busy": "2021-06-07T04:30:55.964501Z",
     "iopub.status.idle": "2021-06-07T04:30:55.966999Z",
     "shell.execute_reply": "2021-06-07T04:30:55.967405Z",
     "shell.execute_reply.started": "2021-06-05T07:54:26.24489Z"
    },
    "papermill": {
     "duration": 0.034365,
     "end_time": "2021-06-07T04:30:55.967536",
     "exception": false,
     "start_time": "2021-06-07T04:30:55.933171",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "model_name = 'allenai/scibert_scivocab_cased'\n",
    "#model_name =  'bert-base-cased'\n",
    "#Initialize paths for data\n",
    "path_abs = '/kaggle/input/coleridgeinitiative-show-us-the-data/'\n",
    "path_train = os.path.join(path_abs,'train/')\n",
    "path_train_metadata = os.path.join(path_abs, 'train.csv')\n",
    "path_test = os.path.join(path_abs, 'test/')\n",
    "path_sample_submission = os.path.join(path_abs, 'sample_submission.csv')\n",
    "\n",
    "path_abs_tdmsci = '/kaggle/input/tdmsci/'\n",
    "path_test_tdmsci = os.path.join(path_abs_tdmsci, 'test_500_v2.txt')\n",
    "path_train_tdmsci = os.path.join(path_abs_tdmsci,'train_1500_v2.txt')\n",
    "path_train_nerjson = '../input/fork-of-mlip-group25-scibert-dataset/train_ner.json'\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.026876,
     "end_time": "2021-06-07T04:30:58.692410",
     "exception": false,
     "start_time": "2021-06-07T04:30:58.665534",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# 3. Train the BERT model\n",
    "We first apply NER in combination with BERT. For this purpose, we need the pretrained [AutoModelForTokenClassification](https://huggingface.co/transformers/model_doc/auto.html#automodelfortokenclassification) for SciBERT. Source code of SciBERT can be found [here](https://github.com/allenai/scibert)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-06-07T04:30:58.814762Z",
     "iopub.status.busy": "2021-06-07T04:30:58.807669Z",
     "iopub.status.idle": "2021-06-07T04:30:59.447811Z",
     "shell.execute_reply": "2021-06-07T04:30:59.448622Z",
     "shell.execute_reply.started": "2021-06-05T07:54:34.561093Z"
    },
    "papermill": {
     "duration": 0.670777,
     "end_time": "2021-06-07T04:30:59.448802",
     "exception": false,
     "start_time": "2021-06-07T04:30:58.778025",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# copy my_seqeval.py to the working directory because the input directory is non-writable\n",
    "!cp /kaggle/input/coleridge-packages/my_seqeval.py ./\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-06-07T04:30:59.593537Z",
     "iopub.status.busy": "2021-06-07T04:30:59.591914Z",
     "iopub.status.idle": "2021-06-07T04:31:00.364066Z",
     "shell.execute_reply": "2021-06-07T04:31:00.364916Z",
     "shell.execute_reply.started": "2021-06-05T07:54:35.23218Z"
    },
    "papermill": {
     "duration": 0.830391,
     "end_time": "2021-06-07T04:31:00.365092",
     "exception": false,
     "start_time": "2021-06-07T04:30:59.534701",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rm: cannot remove './output': No such file or directory\r\n"
     ]
    }
   ],
   "source": [
    "def train_scibert_ner(batch_size):\n",
    "    os.environ[\"MODEL_NAME\"] = f\"{model_name}\"\n",
    "    os.environ[\"TRAIN_FILE\"] = f\"{path_train_nerjson}\"\n",
    "    os.environ[\"VALIDATION_FILE\"] = f\"{path_train_nerjson}\"\n",
    "    os.environ[\"BATCH_SIZE\"] = f\"{batch_size}\"\n",
    "    \n",
    "    acc = 0\n",
    "    with open(path_train_nerjson) as f:\n",
    "        print(\"open \")\n",
    "        for row in f:\n",
    "            acc += 1\n",
    "    \n",
    "    print(\"There are {} training samples!\".format(acc))\n",
    "    \n",
    "    !python ../input/tdmsci/kaggle_run_ner.py \\\n",
    "    --model_name_or_path \"$MODEL_NAME\" \\\n",
    "    --train_file \"$TRAIN_FILE\" \\\n",
    "    --validation_file \"$VALIDATION_FILE\" \\\n",
    "    --num_train_epochs 4 \\\n",
    "    --per_device_train_batch_size \"$BATCH_SIZE\" \\\n",
    "    --per_device_eval_batch_size \"$BATCH_SIZE\" \\\n",
    "    --save_steps 15000 \\\n",
    "    --pad_to_max_length \\\n",
    "    --output_dir './output' \\\n",
    "    --report_to 'none' \\\n",
    "    --seed 123 \\\n",
    "    --do_train\n",
    "!rm -r \"./output\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-06-07T04:31:00.422855Z",
     "iopub.status.busy": "2021-06-07T04:31:00.422051Z",
     "iopub.status.idle": "2021-06-07T12:13:03.745145Z",
     "shell.execute_reply": "2021-06-07T12:13:03.744604Z",
     "shell.execute_reply.started": "2021-06-05T07:54:35.943147Z"
    },
    "papermill": {
     "duration": 27723.353728,
     "end_time": "2021-06-07T12:13:03.745291",
     "exception": false,
     "start_time": "2021-06-07T04:31:00.391563",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "open \n",
      "There are 95394 training samples!\n",
      "2021-06-07 04:31:03.133087: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudart.so.10.2\r\n",
      "Downloading and preparing dataset json/default (download: Unknown size, generated: Unknown size, post-processed: Unknown size, total: Unknown size) to /root/.cache/huggingface/datasets/json/default-0498abf660b7e026/0.0.0/83d5b3a2f62630efc6b5315f00f20209b4ad91a00ac586597caee3a4da0bef02...\r\n",
      "Dataset json downloaded and prepared to /root/.cache/huggingface/datasets/json/default-0498abf660b7e026/0.0.0/83d5b3a2f62630efc6b5315f00f20209b4ad91a00ac586597caee3a4da0bef02. Subsequent calls will reuse this data.\r\n",
      "[INFO|file_utils.py:1402] 2021-06-07 04:31:11,491 >> https://huggingface.co/allenai/scibert_scivocab_cased/resolve/main/config.json not found in cache or force_download set to True, downloading to /root/.cache/huggingface/transformers/tmp_260w4nh\r\n",
      "Downloading: 100%|██████████████████████████████| 385/385 [00:00<00:00, 349kB/s]\r\n",
      "[INFO|file_utils.py:1406] 2021-06-07 04:31:11,839 >> storing https://huggingface.co/allenai/scibert_scivocab_cased/resolve/main/config.json in cache at /root/.cache/huggingface/transformers/62ac366c3e40ed8952fcec53445ee752300aef550f61adff8f5b3485a268492b.bcde2c1ebafc440ded7d6525db15f6f30a50849b17cfb77242a97a8036b82861\r\n",
      "[INFO|file_utils.py:1409] 2021-06-07 04:31:11,839 >> creating metadata file for /root/.cache/huggingface/transformers/62ac366c3e40ed8952fcec53445ee752300aef550f61adff8f5b3485a268492b.bcde2c1ebafc440ded7d6525db15f6f30a50849b17cfb77242a97a8036b82861\r\n",
      "[INFO|configuration_utils.py:472] 2021-06-07 04:31:11,840 >> loading configuration file https://huggingface.co/allenai/scibert_scivocab_cased/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/62ac366c3e40ed8952fcec53445ee752300aef550f61adff8f5b3485a268492b.bcde2c1ebafc440ded7d6525db15f6f30a50849b17cfb77242a97a8036b82861\r\n",
      "[INFO|configuration_utils.py:508] 2021-06-07 04:31:11,840 >> Model config BertConfig {\r\n",
      "  \"attention_probs_dropout_prob\": 0.1,\r\n",
      "  \"finetuning_task\": \"ner\",\r\n",
      "  \"gradient_checkpointing\": false,\r\n",
      "  \"hidden_act\": \"gelu\",\r\n",
      "  \"hidden_dropout_prob\": 0.1,\r\n",
      "  \"hidden_size\": 768,\r\n",
      "  \"id2label\": {\r\n",
      "    \"0\": \"LABEL_0\",\r\n",
      "    \"1\": \"LABEL_1\",\r\n",
      "    \"2\": \"LABEL_2\"\r\n",
      "  },\r\n",
      "  \"initializer_range\": 0.02,\r\n",
      "  \"intermediate_size\": 3072,\r\n",
      "  \"label2id\": {\r\n",
      "    \"LABEL_0\": 0,\r\n",
      "    \"LABEL_1\": 1,\r\n",
      "    \"LABEL_2\": 2\r\n",
      "  },\r\n",
      "  \"layer_norm_eps\": 1e-12,\r\n",
      "  \"max_position_embeddings\": 512,\r\n",
      "  \"model_type\": \"bert\",\r\n",
      "  \"num_attention_heads\": 12,\r\n",
      "  \"num_hidden_layers\": 12,\r\n",
      "  \"pad_token_id\": 0,\r\n",
      "  \"position_embedding_type\": \"absolute\",\r\n",
      "  \"transformers_version\": \"4.5.0.dev0\",\r\n",
      "  \"type_vocab_size\": 2,\r\n",
      "  \"use_cache\": true,\r\n",
      "  \"vocab_size\": 31116\r\n",
      "}\r\n",
      "\r\n",
      "[INFO|configuration_utils.py:472] 2021-06-07 04:31:12,189 >> loading configuration file https://huggingface.co/allenai/scibert_scivocab_cased/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/62ac366c3e40ed8952fcec53445ee752300aef550f61adff8f5b3485a268492b.bcde2c1ebafc440ded7d6525db15f6f30a50849b17cfb77242a97a8036b82861\r\n",
      "[INFO|configuration_utils.py:508] 2021-06-07 04:31:12,190 >> Model config BertConfig {\r\n",
      "  \"attention_probs_dropout_prob\": 0.1,\r\n",
      "  \"gradient_checkpointing\": false,\r\n",
      "  \"hidden_act\": \"gelu\",\r\n",
      "  \"hidden_dropout_prob\": 0.1,\r\n",
      "  \"hidden_size\": 768,\r\n",
      "  \"initializer_range\": 0.02,\r\n",
      "  \"intermediate_size\": 3072,\r\n",
      "  \"layer_norm_eps\": 1e-12,\r\n",
      "  \"max_position_embeddings\": 512,\r\n",
      "  \"model_type\": \"bert\",\r\n",
      "  \"num_attention_heads\": 12,\r\n",
      "  \"num_hidden_layers\": 12,\r\n",
      "  \"pad_token_id\": 0,\r\n",
      "  \"position_embedding_type\": \"absolute\",\r\n",
      "  \"transformers_version\": \"4.5.0.dev0\",\r\n",
      "  \"type_vocab_size\": 2,\r\n",
      "  \"use_cache\": true,\r\n",
      "  \"vocab_size\": 31116\r\n",
      "}\r\n",
      "\r\n",
      "[INFO|file_utils.py:1402] 2021-06-07 04:31:12,541 >> https://huggingface.co/allenai/scibert_scivocab_cased/resolve/main/vocab.txt not found in cache or force_download set to True, downloading to /root/.cache/huggingface/transformers/tmp7yq9j6jo\r\n",
      "Downloading: 100%|███████████████████████████| 222k/222k [00:00<00:00, 1.73MB/s]\r\n",
      "[INFO|file_utils.py:1406] 2021-06-07 04:31:13,019 >> storing https://huggingface.co/allenai/scibert_scivocab_cased/resolve/main/vocab.txt in cache at /root/.cache/huggingface/transformers/397254f347f587a433c488ab9c20276efbe8777ee55f13f4178230f2caccdb99.72e8e5acf023d231f7689bafa0d08dd14da0d33395f00509444282fffcbb7adc\r\n",
      "[INFO|file_utils.py:1409] 2021-06-07 04:31:13,019 >> creating metadata file for /root/.cache/huggingface/transformers/397254f347f587a433c488ab9c20276efbe8777ee55f13f4178230f2caccdb99.72e8e5acf023d231f7689bafa0d08dd14da0d33395f00509444282fffcbb7adc\r\n",
      "[INFO|tokenization_utils_base.py:1702] 2021-06-07 04:31:14,413 >> loading file https://huggingface.co/allenai/scibert_scivocab_cased/resolve/main/vocab.txt from cache at /root/.cache/huggingface/transformers/397254f347f587a433c488ab9c20276efbe8777ee55f13f4178230f2caccdb99.72e8e5acf023d231f7689bafa0d08dd14da0d33395f00509444282fffcbb7adc\r\n",
      "[INFO|tokenization_utils_base.py:1702] 2021-06-07 04:31:14,413 >> loading file https://huggingface.co/allenai/scibert_scivocab_cased/resolve/main/tokenizer.json from cache at None\r\n",
      "[INFO|tokenization_utils_base.py:1702] 2021-06-07 04:31:14,413 >> loading file https://huggingface.co/allenai/scibert_scivocab_cased/resolve/main/added_tokens.json from cache at None\r\n",
      "[INFO|tokenization_utils_base.py:1702] 2021-06-07 04:31:14,413 >> loading file https://huggingface.co/allenai/scibert_scivocab_cased/resolve/main/special_tokens_map.json from cache at None\r\n",
      "[INFO|tokenization_utils_base.py:1702] 2021-06-07 04:31:14,414 >> loading file https://huggingface.co/allenai/scibert_scivocab_cased/resolve/main/tokenizer_config.json from cache at None\r\n",
      "[INFO|file_utils.py:1402] 2021-06-07 04:31:14,827 >> https://huggingface.co/allenai/scibert_scivocab_cased/resolve/main/pytorch_model.bin not found in cache or force_download set to True, downloading to /root/.cache/huggingface/transformers/tmpww7a1nso\r\n",
      "Downloading: 100%|███████████████████████████| 442M/442M [00:20<00:00, 21.8MB/s]\r\n",
      "[INFO|file_utils.py:1406] 2021-06-07 04:31:35,704 >> storing https://huggingface.co/allenai/scibert_scivocab_cased/resolve/main/pytorch_model.bin in cache at /root/.cache/huggingface/transformers/38fb438255295d0f6ad03b1cda169af73b3f1fa2b4fddc6fbf45854b508aef66.7352c55eae981bd658b28a9746a052242f6359950742ae411e486aebfa8c2456\r\n",
      "[INFO|file_utils.py:1409] 2021-06-07 04:31:35,704 >> creating metadata file for /root/.cache/huggingface/transformers/38fb438255295d0f6ad03b1cda169af73b3f1fa2b4fddc6fbf45854b508aef66.7352c55eae981bd658b28a9746a052242f6359950742ae411e486aebfa8c2456\r\n",
      "[INFO|modeling_utils.py:1051] 2021-06-07 04:31:35,705 >> loading weights file https://huggingface.co/allenai/scibert_scivocab_cased/resolve/main/pytorch_model.bin from cache at /root/.cache/huggingface/transformers/38fb438255295d0f6ad03b1cda169af73b3f1fa2b4fddc6fbf45854b508aef66.7352c55eae981bd658b28a9746a052242f6359950742ae411e486aebfa8c2456\r\n",
      "[WARNING|modeling_utils.py:1159] 2021-06-07 04:31:39,293 >> Some weights of the model checkpoint at allenai/scibert_scivocab_cased were not used when initializing BertForTokenClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight', 'cls.predictions.decoder.bias', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias']\r\n",
      "- This IS expected if you are initializing BertForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\r\n",
      "- This IS NOT expected if you are initializing BertForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\r\n",
      "[WARNING|modeling_utils.py:1170] 2021-06-07 04:31:39,293 >> Some weights of BertForTokenClassification were not initialized from the model checkpoint at allenai/scibert_scivocab_cased and are newly initialized: ['classifier.weight', 'classifier.bias']\r\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\r\n",
      "100%|███████████████████████████████████████████| 96/96 [00:58<00:00,  1.65ba/s]\r\n",
      "[INFO|trainer.py:485] 2021-06-07 04:32:44,763 >> The following columns in the training set  don't have a corresponding argument in `BertForTokenClassification.forward` and have been ignored: id, tags, tokens.\r\n",
      "[INFO|trainer.py:988] 2021-06-07 04:32:44,993 >> ***** Running training *****\r\n",
      "[INFO|trainer.py:989] 2021-06-07 04:32:44,993 >>   Num examples = 95394\r\n",
      "[INFO|trainer.py:990] 2021-06-07 04:32:44,993 >>   Num Epochs = 4\r\n",
      "[INFO|trainer.py:991] 2021-06-07 04:32:44,993 >>   Instantaneous batch size per device = 8\r\n",
      "[INFO|trainer.py:992] 2021-06-07 04:32:44,993 >>   Total train batch size (w. parallel, distributed & accumulation) = 8\r\n",
      "[INFO|trainer.py:993] 2021-06-07 04:32:44,993 >>   Gradient Accumulation steps = 1\r\n",
      "[INFO|trainer.py:994] 2021-06-07 04:32:44,993 >>   Total optimization steps = 47700\r\n",
      "{'loss': 0.0441, 'learning_rate': 4.947589098532495e-05, 'epoch': 0.04}\r\n",
      "{'loss': 0.0189, 'learning_rate': 4.8951781970649894e-05, 'epoch': 0.08}\r\n",
      "{'loss': 0.015, 'learning_rate': 4.842767295597484e-05, 'epoch': 0.13}\r\n",
      "{'loss': 0.0134, 'learning_rate': 4.79035639412998e-05, 'epoch': 0.17}\r\n",
      "{'loss': 0.0115, 'learning_rate': 4.737945492662474e-05, 'epoch': 0.21}\r\n",
      "{'loss': 0.0135, 'learning_rate': 4.685534591194969e-05, 'epoch': 0.25}\r\n",
      "{'loss': 0.0121, 'learning_rate': 4.633123689727464e-05, 'epoch': 0.29}\r\n",
      "{'loss': 0.0119, 'learning_rate': 4.580712788259958e-05, 'epoch': 0.34}\r\n",
      "{'loss': 0.011, 'learning_rate': 4.528301886792453e-05, 'epoch': 0.38}\r\n",
      "{'loss': 0.0125, 'learning_rate': 4.475890985324948e-05, 'epoch': 0.42}\r\n",
      "{'loss': 0.0104, 'learning_rate': 4.423480083857443e-05, 'epoch': 0.46}\r\n",
      "{'loss': 0.0089, 'learning_rate': 4.3710691823899376e-05, 'epoch': 0.5}\r\n",
      "{'loss': 0.0125, 'learning_rate': 4.318658280922432e-05, 'epoch': 0.55}\r\n",
      "{'loss': 0.0083, 'learning_rate': 4.266247379454927e-05, 'epoch': 0.59}\r\n",
      "{'loss': 0.0082, 'learning_rate': 4.213836477987422e-05, 'epoch': 0.63}\r\n",
      "{'loss': 0.0082, 'learning_rate': 4.161425576519916e-05, 'epoch': 0.67}\r\n",
      "{'loss': 0.0104, 'learning_rate': 4.109014675052411e-05, 'epoch': 0.71}\r\n",
      "{'loss': 0.0092, 'learning_rate': 4.0566037735849064e-05, 'epoch': 0.75}\r\n",
      "{'loss': 0.0079, 'learning_rate': 4.0041928721174006e-05, 'epoch': 0.8}\r\n",
      "{'loss': 0.0073, 'learning_rate': 3.9517819706498955e-05, 'epoch': 0.84}\r\n",
      "{'loss': 0.0078, 'learning_rate': 3.8993710691823904e-05, 'epoch': 0.88}\r\n",
      "{'loss': 0.0073, 'learning_rate': 3.8469601677148846e-05, 'epoch': 0.92}\r\n",
      "{'loss': 0.0073, 'learning_rate': 3.7945492662473795e-05, 'epoch': 0.96}\r\n",
      "{'loss': 0.0089, 'learning_rate': 3.7421383647798744e-05, 'epoch': 1.01}\r\n",
      "{'loss': 0.0066, 'learning_rate': 3.689727463312369e-05, 'epoch': 1.05}\r\n",
      "{'loss': 0.0057, 'learning_rate': 3.637316561844864e-05, 'epoch': 1.09}\r\n",
      "{'loss': 0.0065, 'learning_rate': 3.5849056603773584e-05, 'epoch': 1.13}\r\n",
      "{'loss': 0.0062, 'learning_rate': 3.532494758909853e-05, 'epoch': 1.17}\r\n",
      "{'loss': 0.0075, 'learning_rate': 3.480083857442348e-05, 'epoch': 1.22}\r\n",
      "{'loss': 0.006, 'learning_rate': 3.4276729559748424e-05, 'epoch': 1.26}\r\n",
      " 31%|██████████▍                      | 15000/47700 [2:24:52<5:10:31,  1.76it/s][INFO|trainer.py:1600] 2021-06-07 06:57:37,575 >> Saving model checkpoint to ./output/checkpoint-15000\r\n",
      "[INFO|configuration_utils.py:318] 2021-06-07 06:57:37,578 >> Configuration saved in ./output/checkpoint-15000/config.json\r\n",
      "[INFO|modeling_utils.py:837] 2021-06-07 06:57:39,125 >> Model weights saved in ./output/checkpoint-15000/pytorch_model.bin\r\n",
      "[INFO|tokenization_utils_base.py:1896] 2021-06-07 06:57:39,126 >> tokenizer config file saved in ./output/checkpoint-15000/tokenizer_config.json\r\n",
      "[INFO|tokenization_utils_base.py:1902] 2021-06-07 06:57:39,126 >> Special tokens file saved in ./output/checkpoint-15000/special_tokens_map.json\r\n",
      "{'loss': 0.0078, 'learning_rate': 3.375262054507338e-05, 'epoch': 1.3}\r\n",
      "{'loss': 0.0059, 'learning_rate': 3.322851153039833e-05, 'epoch': 1.34}\r\n",
      "{'loss': 0.0051, 'learning_rate': 3.270440251572327e-05, 'epoch': 1.38}\r\n",
      "{'loss': 0.0056, 'learning_rate': 3.218029350104822e-05, 'epoch': 1.43}\r\n",
      "{'loss': 0.0062, 'learning_rate': 3.165618448637317e-05, 'epoch': 1.47}\r\n",
      "{'loss': 0.0064, 'learning_rate': 3.113207547169811e-05, 'epoch': 1.51}\r\n",
      "{'loss': 0.0056, 'learning_rate': 3.060796645702306e-05, 'epoch': 1.55}\r\n",
      "{'loss': 0.0056, 'learning_rate': 3.0083857442348012e-05, 'epoch': 1.59}\r\n",
      "{'loss': 0.0058, 'learning_rate': 2.9559748427672958e-05, 'epoch': 1.64}\r\n",
      "{'loss': 0.0057, 'learning_rate': 2.9035639412997907e-05, 'epoch': 1.68}\r\n",
      "{'loss': 0.007, 'learning_rate': 2.851153039832285e-05, 'epoch': 1.72}\r\n",
      "{'loss': 0.0067, 'learning_rate': 2.7987421383647798e-05, 'epoch': 1.76}\r\n",
      "{'loss': 0.0046, 'learning_rate': 2.746331236897275e-05, 'epoch': 1.8}\r\n",
      "{'loss': 0.0044, 'learning_rate': 2.6939203354297693e-05, 'epoch': 1.84}\r\n",
      "{'loss': 0.0049, 'learning_rate': 2.641509433962264e-05, 'epoch': 1.89}\r\n",
      "{'loss': 0.0066, 'learning_rate': 2.589098532494759e-05, 'epoch': 1.93}\r\n",
      "{'loss': 0.005, 'learning_rate': 2.5366876310272536e-05, 'epoch': 1.97}\r\n",
      "{'loss': 0.0057, 'learning_rate': 2.4842767295597485e-05, 'epoch': 2.01}\r\n",
      "{'loss': 0.0037, 'learning_rate': 2.431865828092243e-05, 'epoch': 2.05}\r\n",
      "{'loss': 0.0047, 'learning_rate': 2.3794549266247383e-05, 'epoch': 2.1}\r\n",
      "{'loss': 0.0052, 'learning_rate': 2.327044025157233e-05, 'epoch': 2.14}\r\n",
      "{'loss': 0.0046, 'learning_rate': 2.2746331236897274e-05, 'epoch': 2.18}\r\n",
      "{'loss': 0.0043, 'learning_rate': 2.2222222222222223e-05, 'epoch': 2.22}\r\n",
      "{'loss': 0.0053, 'learning_rate': 2.1698113207547172e-05, 'epoch': 2.26}\r\n",
      "{'loss': 0.0049, 'learning_rate': 2.1174004192872118e-05, 'epoch': 2.31}\r\n",
      "{'loss': 0.0043, 'learning_rate': 2.0649895178197063e-05, 'epoch': 2.35}\r\n",
      "{'loss': 0.0031, 'learning_rate': 2.0125786163522016e-05, 'epoch': 2.39}\r\n",
      "{'loss': 0.0041, 'learning_rate': 1.960167714884696e-05, 'epoch': 2.43}\r\n",
      "{'loss': 0.0038, 'learning_rate': 1.9077568134171907e-05, 'epoch': 2.47}\r\n",
      "{'loss': 0.0034, 'learning_rate': 1.8553459119496856e-05, 'epoch': 2.52}\r\n",
      " 63%|████████████████████▊            | 30000/47700 [4:49:40<2:48:57,  1.75it/s][INFO|trainer.py:1600] 2021-06-07 09:22:25,646 >> Saving model checkpoint to ./output/checkpoint-30000\r\n",
      "[INFO|configuration_utils.py:318] 2021-06-07 09:22:25,647 >> Configuration saved in ./output/checkpoint-30000/config.json\r\n",
      "[INFO|modeling_utils.py:837] 2021-06-07 09:22:27,152 >> Model weights saved in ./output/checkpoint-30000/pytorch_model.bin\r\n",
      "[INFO|tokenization_utils_base.py:1896] 2021-06-07 09:22:27,153 >> tokenizer config file saved in ./output/checkpoint-30000/tokenizer_config.json\r\n",
      "[INFO|tokenization_utils_base.py:1902] 2021-06-07 09:22:27,154 >> Special tokens file saved in ./output/checkpoint-30000/special_tokens_map.json\r\n",
      "{'loss': 0.0043, 'learning_rate': 1.8029350104821805e-05, 'epoch': 2.56}\r\n",
      "{'loss': 0.0033, 'learning_rate': 1.750524109014675e-05, 'epoch': 2.6}\r\n",
      "{'loss': 0.0046, 'learning_rate': 1.69811320754717e-05, 'epoch': 2.64}\r\n",
      "{'loss': 0.0033, 'learning_rate': 1.645702306079665e-05, 'epoch': 2.68}\r\n",
      "{'loss': 0.0042, 'learning_rate': 1.5932914046121594e-05, 'epoch': 2.73}\r\n",
      "{'loss': 0.0045, 'learning_rate': 1.540880503144654e-05, 'epoch': 2.77}\r\n",
      "{'loss': 0.0033, 'learning_rate': 1.488469601677149e-05, 'epoch': 2.81}\r\n",
      "{'loss': 0.0033, 'learning_rate': 1.4360587002096438e-05, 'epoch': 2.85}\r\n",
      "{'loss': 0.0032, 'learning_rate': 1.3836477987421385e-05, 'epoch': 2.89}\r\n",
      "{'loss': 0.0031, 'learning_rate': 1.331236897274633e-05, 'epoch': 2.94}\r\n",
      "{'loss': 0.003, 'learning_rate': 1.2788259958071281e-05, 'epoch': 2.98}\r\n",
      "{'loss': 0.0043, 'learning_rate': 1.2264150943396227e-05, 'epoch': 3.02}\r\n",
      "{'loss': 0.0027, 'learning_rate': 1.1740041928721176e-05, 'epoch': 3.06}\r\n",
      "{'loss': 0.0021, 'learning_rate': 1.1215932914046121e-05, 'epoch': 3.1}\r\n",
      "{'loss': 0.0034, 'learning_rate': 1.069182389937107e-05, 'epoch': 3.14}\r\n",
      "{'loss': 0.003, 'learning_rate': 1.0167714884696017e-05, 'epoch': 3.19}\r\n",
      "{'loss': 0.0033, 'learning_rate': 9.643605870020965e-06, 'epoch': 3.23}\r\n",
      "{'loss': 0.0033, 'learning_rate': 9.119496855345912e-06, 'epoch': 3.27}\r\n",
      "{'loss': 0.0021, 'learning_rate': 8.59538784067086e-06, 'epoch': 3.31}\r\n",
      "{'loss': 0.0025, 'learning_rate': 8.071278825995808e-06, 'epoch': 3.35}\r\n",
      "{'loss': 0.0025, 'learning_rate': 7.547169811320755e-06, 'epoch': 3.4}\r\n",
      "{'loss': 0.0025, 'learning_rate': 7.023060796645703e-06, 'epoch': 3.44}\r\n",
      "{'loss': 0.0024, 'learning_rate': 6.49895178197065e-06, 'epoch': 3.48}\r\n",
      "{'loss': 0.0025, 'learning_rate': 5.974842767295598e-06, 'epoch': 3.52}\r\n",
      "{'loss': 0.0027, 'learning_rate': 5.4507337526205454e-06, 'epoch': 3.56}\r\n",
      "{'loss': 0.0029, 'learning_rate': 4.926624737945493e-06, 'epoch': 3.61}\r\n",
      "{'loss': 0.0031, 'learning_rate': 4.40251572327044e-06, 'epoch': 3.65}\r\n",
      "{'loss': 0.0028, 'learning_rate': 3.878406708595388e-06, 'epoch': 3.69}\r\n",
      "{'loss': 0.0022, 'learning_rate': 3.354297693920336e-06, 'epoch': 3.73}\r\n",
      "{'loss': 0.0033, 'learning_rate': 2.830188679245283e-06, 'epoch': 3.77}\r\n",
      " 94%|█████████████████████████████████  | 45000/47700 [7:14:12<25:56,  1.74it/s][INFO|trainer.py:1600] 2021-06-07 11:46:57,775 >> Saving model checkpoint to ./output/checkpoint-45000\r\n",
      "[INFO|configuration_utils.py:318] 2021-06-07 11:46:57,777 >> Configuration saved in ./output/checkpoint-45000/config.json\r\n",
      "[INFO|modeling_utils.py:837] 2021-06-07 11:46:59,382 >> Model weights saved in ./output/checkpoint-45000/pytorch_model.bin\r\n",
      "[INFO|tokenization_utils_base.py:1896] 2021-06-07 11:46:59,383 >> tokenizer config file saved in ./output/checkpoint-45000/tokenizer_config.json\r\n",
      "[INFO|tokenization_utils_base.py:1902] 2021-06-07 11:46:59,383 >> Special tokens file saved in ./output/checkpoint-45000/special_tokens_map.json\r\n",
      "{'loss': 0.0028, 'learning_rate': 2.306079664570231e-06, 'epoch': 3.82}\r\n",
      "{'loss': 0.0027, 'learning_rate': 1.781970649895178e-06, 'epoch': 3.86}\r\n",
      "{'loss': 0.003, 'learning_rate': 1.257861635220126e-06, 'epoch': 3.9}\r\n",
      "{'loss': 0.0026, 'learning_rate': 7.337526205450735e-07, 'epoch': 3.94}\r\n",
      "{'loss': 0.0032, 'learning_rate': 2.09643605870021e-07, 'epoch': 3.98}\r\n",
      "100%|███████████████████████████████████| 47700/47700 [7:40:15<00:00,  2.04it/s][INFO|trainer.py:1171] 2021-06-07 12:13:00,800 >> \r\n",
      "\r\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\r\n",
      "\r\n",
      "\r\n",
      "{'train_runtime': 27615.8069, 'train_samples_per_second': 1.727, 'epoch': 4.0}\r\n",
      "100%|███████████████████████████████████| 47700/47700 [7:40:15<00:00,  1.73it/s]\r\n",
      "[INFO|trainer.py:1600] 2021-06-07 12:13:01,171 >> Saving model checkpoint to ./output\r\n",
      "[INFO|configuration_utils.py:318] 2021-06-07 12:13:01,172 >> Configuration saved in ./output/config.json\r\n",
      "[INFO|modeling_utils.py:837] 2021-06-07 12:13:02,549 >> Model weights saved in ./output/pytorch_model.bin\r\n",
      "[INFO|tokenization_utils_base.py:1896] 2021-06-07 12:13:02,550 >> tokenizer config file saved in ./output/tokenizer_config.json\r\n",
      "[INFO|tokenization_utils_base.py:1902] 2021-06-07 12:13:02,550 >> Special tokens file saved in ./output/special_tokens_map.json\r\n",
      "[INFO|trainer_pt_utils.py:735] 2021-06-07 12:13:02,585 >> ***** train metrics *****\r\n",
      "[INFO|trainer_pt_utils.py:740] 2021-06-07 12:13:02,585 >>   epoch                      =        4.0\r\n",
      "[INFO|trainer_pt_utils.py:740] 2021-06-07 12:13:02,585 >>   init_mem_cpu_alloc_delta   =     1436MB\r\n",
      "[INFO|trainer_pt_utils.py:740] 2021-06-07 12:13:02,586 >>   init_mem_cpu_peaked_delta  =      307MB\r\n",
      "[INFO|trainer_pt_utils.py:740] 2021-06-07 12:13:02,586 >>   init_mem_gpu_alloc_delta   =      418MB\r\n",
      "[INFO|trainer_pt_utils.py:740] 2021-06-07 12:13:02,586 >>   init_mem_gpu_peaked_delta  =        0MB\r\n",
      "[INFO|trainer_pt_utils.py:740] 2021-06-07 12:13:02,586 >>   train_mem_cpu_alloc_delta  =      742MB\r\n",
      "[INFO|trainer_pt_utils.py:740] 2021-06-07 12:13:02,586 >>   train_mem_cpu_peaked_delta =      273MB\r\n",
      "[INFO|trainer_pt_utils.py:740] 2021-06-07 12:13:02,586 >>   train_mem_gpu_alloc_delta  =     1317MB\r\n",
      "[INFO|trainer_pt_utils.py:740] 2021-06-07 12:13:02,586 >>   train_mem_gpu_peaked_delta =     6202MB\r\n",
      "[INFO|trainer_pt_utils.py:740] 2021-06-07 12:13:02,586 >>   train_runtime              = 7:40:15.80\r\n",
      "[INFO|trainer_pt_utils.py:740] 2021-06-07 12:13:02,586 >>   train_samples              =      95394\r\n",
      "[INFO|trainer_pt_utils.py:740] 2021-06-07 12:13:02,586 >>   train_samples_per_second   =      1.727\r\n"
     ]
    }
   ],
   "source": [
    "#Train SciBERT model\n",
    "batch_size = 8\n",
    "train_scibert_ner(batch_size)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 28170.045377,
   "end_time": "2021-06-07T12:19:36.167415",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2021-06-07T04:30:06.122038",
   "version": "2.3.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
